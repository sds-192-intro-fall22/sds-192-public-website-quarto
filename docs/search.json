[
  {
    "objectID": "schedule.html#september-07-2022",
    "href": "schedule.html#september-07-2022",
    "title": "Schedule",
    "section": "September 07, 2022",
    "text": "September 07, 2022\n\nWhat is Data Science?\n\nDue TodayFurther Resources\n\n\n\n\n\n Day 1 Slides\n Spinelli Center\n Getting Started with Slack"
  },
  {
    "objectID": "schedule.html#september-09-2022",
    "href": "schedule.html#september-09-2022",
    "title": "Schedule",
    "section": "September 09, 2022",
    "text": "September 09, 2022\n\nInfrastructure Set-up\n\nDue TodayFurther Resources\n\n\n Fill out First Day of Class Questionnaire in Moodle\n Complete Syllabus Quiz in Moodle\n Contact me if you will be using a Chromebook\n\n\n Configuring Git Set-up Instructions"
  },
  {
    "objectID": "schedule.html#september-12-2022",
    "href": "schedule.html#september-12-2022",
    "title": "Schedule",
    "section": "September 12, 2022",
    "text": "September 12, 2022\n\nData Fundamentals\nUNDERSTANDING DATASETS\n\nDue TodayFurther Resources\n\n\n\n\n\n Day 2 Slides\n Day 2 Activity"
  },
  {
    "objectID": "schedule.html#september-14-2022",
    "href": "schedule.html#september-14-2022",
    "title": "Schedule",
    "section": "September 14, 2022",
    "text": "September 14, 2022\n\nIntroduction to R\nUNDERSTANDING DATASETS\n\nDue TodayFurther Resources\n\n\n (Read in Perusall; linked in Moodle) 2. R Basics , Irizarry, Rafael A. (2022). Introduction to Data Science. Data Analysis and Prediction Algorithms with R. URL: https://rafalab.github.io/dsbook/ (visited on Jan. 14, 2022).\n\n\n Day 3 Slides\n Day 3 Activity"
  },
  {
    "objectID": "schedule.html#september-16-2022",
    "href": "schedule.html#september-16-2022",
    "title": "Schedule",
    "section": "September 16, 2022",
    "text": "September 16, 2022\n\nLab: Understanding Datasets\nUNDERSTANDING DATASETS\n\nDue TodayFurther Resources\n\n\n\n\n\n Lab Templates\n Lab 1 Instructions\n Problem-Solving Lab Instructions"
  },
  {
    "objectID": "schedule.html#september-19-2022",
    "href": "schedule.html#september-19-2022",
    "title": "Schedule",
    "section": "September 19, 2022",
    "text": "September 19, 2022\n\nGrammar of Graphics\nVISUALIZATION AESTHETICS\n\nDue TodayFurther Resources\n\n\n (Read in Perusall; linked in Moodle) 2. Data Visualization , Baumer, Benjamin S., Daniel T. Kaplan, and Nicholas J. Horton (2021). Modern Data Science with R. 2nd. CRC Press. URL: https://mdsr-book.github.io/mdsr2e/ (visited on Jan. 14, 2022).\n\n\n Day 4 Slides\n Day 4 Activity"
  },
  {
    "objectID": "schedule.html#september-21-2022",
    "href": "schedule.html#september-21-2022",
    "title": "Schedule",
    "section": "September 21, 2022",
    "text": "September 21, 2022\n\nVisualization Conventions\n\nDue TodayFurther Resources\n\n\n Problem-Solving Lab Due\n Lab 1 Due\n\n\n Day 5 Slides\n ggplot2 cheatsheet\n Navigate to RStudio and switch the project called class-activities. Then click the blue downward arrow in the Git tab to pull the new activity into your environment."
  },
  {
    "objectID": "schedule.html#september-23-2022",
    "href": "schedule.html#september-23-2022",
    "title": "Schedule",
    "section": "September 23, 2022",
    "text": "September 23, 2022\n\nLab: Designing Effective Data Visualizations\nVISUALIZATION AESTHETICS\n\nDue TodayFurther Resources\n\n\n Study Plan Due\n\n\n Lab 2 Templates\n Lab 2 Instructions"
  },
  {
    "objectID": "schedule.html#september-26-2022",
    "href": "schedule.html#september-26-2022",
    "title": "Schedule",
    "section": "September 26, 2022",
    "text": "September 26, 2022\n\nFrequency Plots\nPLOTTING FREQUENCIES AND DISTRIBUTIONS\n\nDue TodayFurther Resources\n\n\n (Read in Perusall; linked in Moodle) 2. Data Visualization , Ismay, Chester and Albert Y. Kim (2021). Modern Dive: Statistical Inference via Data Science. CRC Press. URL: https://moderndive.com/ (visited on Jan. 14, 2022).\n Complete CATME Survey\n\n\n Day 6 Slides\n ggplot2 cheatsheet\n Navigate to RStudio and switch the project called class-activities. Then click the blue downward arrow in the Git tab to pull the new activity into your environment."
  },
  {
    "objectID": "schedule.html#september-28-2022",
    "href": "schedule.html#september-28-2022",
    "title": "Schedule",
    "section": "September 28, 2022",
    "text": "September 28, 2022\n\nMountain Day!\nPLOTTING FREQUENCIES AND DISTRIBUTIONS\n\nDue TodayFurther Resources\n\n\n Lab 2 Due"
  },
  {
    "objectID": "schedule.html#september-30-2022",
    "href": "schedule.html#september-30-2022",
    "title": "Schedule",
    "section": "September 30, 2022",
    "text": "September 30, 2022\n\nWork on Group Projects in Class\nPLOTTING FREQUENCIES AND DISTRIBUTIONS\n\nDue TodayFurther Resources\n\n\n Project 1 Assigned\n\n\n Check team number at CATME.org\n Project 1"
  },
  {
    "objectID": "schedule.html#october-03-2022",
    "href": "schedule.html#october-03-2022",
    "title": "Schedule",
    "section": "October 03, 2022",
    "text": "October 03, 2022\n\nLab: Collaborating via GitHub\nGITHUB\n\nDue TodayFurther Resources\n\n\n Read Lab 4 in Perusall\n Watch Lab 2 Recap in Perusall"
  },
  {
    "objectID": "schedule.html#october-05-2022",
    "href": "schedule.html#october-05-2022",
    "title": "Schedule",
    "section": "October 05, 2022",
    "text": "October 05, 2022\n\nBoxplots\nGITHUB\n\nDue TodayFurther Resources\n\n\n Group Contract Due\n\n\n Day 7 Slides"
  },
  {
    "objectID": "schedule.html#october-07-2022",
    "href": "schedule.html#october-07-2022",
    "title": "Schedule",
    "section": "October 07, 2022",
    "text": "October 07, 2022\n\nLab: Visualizing Data\n\nDue TodayFurther Resources\n\n\n Lab 4 Due\n\n\n Lab 3 Template\n Lab 3 Instructions"
  },
  {
    "objectID": "schedule.html#october-10-2022",
    "href": "schedule.html#october-10-2022",
    "title": "Schedule",
    "section": "October 10, 2022",
    "text": "October 10, 2022\n\nNo Class\n\nDue TodayFurther Resources"
  },
  {
    "objectID": "schedule.html#october-12-2022",
    "href": "schedule.html#october-12-2022",
    "title": "Schedule",
    "section": "October 12, 2022",
    "text": "October 12, 2022\n\nSubsetting, Aggregating, and Summarizing Data\nTRANSFORMING DATA\n\nDue TodayFurther Resources\n\n\n (Read in Perusall; linked in Moodle) 3. Data Wrangling , Ismay, Chester and Albert Y. Kim (2021). Modern Dive: Statistical Inference via Data Science. CRC Press. URL: https://moderndive.com/ (visited on Jan. 14, 2022).\n Lab 3 Due\n\n\n Day 8 Slides\n dplyr Cheatsheet"
  },
  {
    "objectID": "schedule.html#october-14-2022",
    "href": "schedule.html#october-14-2022",
    "title": "Schedule",
    "section": "October 14, 2022",
    "text": "October 14, 2022\n\nLab: Aggregating and Summarizing Data\nTRANSFORMING DATA\n\nDue TodayFurther Resources\n\n\n\n\n\n Lab 5 Template\n Lab 5 Instructions"
  },
  {
    "objectID": "schedule.html#october-17-2022",
    "href": "schedule.html#october-17-2022",
    "title": "Schedule",
    "section": "October 17, 2022",
    "text": "October 17, 2022\n\nJoining Datasets\nTRANSFORMING DATA\n\nDue TodayFurther Resources\n\n\n (Read in Perusall; linked in Moodle) 5. Data wrangling on multiple tables , Baumer, Benjamin S., Daniel T. Kaplan, and Nicholas J. Horton (2021). Modern Data Science with R. 2nd. CRC Press. URL: https://mdsr-book.github.io/mdsr2e/ (visited on Jan. 14, 2022).\n Watch Lab 3 Recap in Perusall\n\n\n Day 9 Slides\n Link to Survey 1\n Link to Survey 2"
  },
  {
    "objectID": "schedule.html#october-19-2022",
    "href": "schedule.html#october-19-2022",
    "title": "Schedule",
    "section": "October 19, 2022",
    "text": "October 19, 2022\n\nEthics of Data Joining\nTRANSFORMING DATA\n\nDue TodayFurther Resources\n\n\n Project 1 Due\n Quiz 1 Recommended Deadline\n Lab 5 Due\n\n\n Day 10 Slides"
  },
  {
    "objectID": "schedule.html#october-21-2022",
    "href": "schedule.html#october-21-2022",
    "title": "Schedule",
    "section": "October 21, 2022",
    "text": "October 21, 2022\n\nClass will not meet in-person: Meet with project groups on Zoom\n\nDue TodayFurther Resources\n\n\n Project 2 Assigned\n Watch Project Intro Video in Perusall"
  },
  {
    "objectID": "schedule.html#october-24-2022",
    "href": "schedule.html#october-24-2022",
    "title": "Schedule",
    "section": "October 24, 2022",
    "text": "October 24, 2022\n\nProject 1 Shares\n\nDue TodayFurther Resources\n\n\n Zoom link for today’s class has been sent via email and is posted on Moodle.\n Watch Lab 5 Recap in Perusall"
  },
  {
    "objectID": "schedule.html#october-26-2022",
    "href": "schedule.html#october-26-2022",
    "title": "Schedule",
    "section": "October 26, 2022",
    "text": "October 26, 2022\n\nData Wrangling Problem-Solving\n\nDue TodayFurther Resources\n\n\n Group Contract Due\n\n\n Problem Solving Tips"
  },
  {
    "objectID": "schedule.html#october-28-2022",
    "href": "schedule.html#october-28-2022",
    "title": "Schedule",
    "section": "October 28, 2022",
    "text": "October 28, 2022\n\nLab: Joining Datasets\nJOINING DATASETS\n\nDue TodayFurther Resources\n\n\n Quiz 1 Due\n\n\n Lab 6 Template\n Lab 6 Instructions"
  },
  {
    "objectID": "schedule.html#october-31-2022",
    "href": "schedule.html#october-31-2022",
    "title": "Schedule",
    "section": "October 31, 2022",
    "text": "October 31, 2022\n\nTidying Datasets\nTIDYING DATA\n\nDue TodayFurther Resources\n\n\n (Read in Perusall; linked in Moodle) 6. Tidy Data , Baumer, Benjamin S., Daniel T. Kaplan, and Nicholas J. Horton (2021). Modern Data Science with R. 2nd. CRC Press. URL: https://mdsr-book.github.io/mdsr2e/ (visited on Jan. 14, 2022)."
  },
  {
    "objectID": "schedule.html#november-02-2022",
    "href": "schedule.html#november-02-2022",
    "title": "Schedule",
    "section": "November 02, 2022",
    "text": "November 02, 2022\n\nPivoting Datasets\nTIDYING DATA\n\nDue TodayFurther Resources\n\n\n (Read in Perusall; linked in Moodle) 26. Parsing dates and times , Irizarry, Rafael A. (2022). Introduction to Data Science. Data Analysis and Prediction Algorithms with R. URL: https://rafalab.github.io/dsbook/ (visited on Jan. 14, 2022).\n Lab 6 Due\n\n\n Day 12 Slides: Tidying Datasets\n tidyr Cheatsheet"
  },
  {
    "objectID": "schedule.html#november-04-2022",
    "href": "schedule.html#november-04-2022",
    "title": "Schedule",
    "section": "November 04, 2022",
    "text": "November 04, 2022\n\nLab: Pivoting Datasets\nTIDYING DATA\n\nDue TodayFurther Resources\n\n\n\n\n\n Lab 7 Template\n Lab 7 Instructions"
  },
  {
    "objectID": "schedule.html#november-07-2022",
    "href": "schedule.html#november-07-2022",
    "title": "Schedule",
    "section": "November 07, 2022",
    "text": "November 07, 2022\n\nWriting Functions\nPROGRAMMING IN R\n\nDue TodayFurther Resources\n\n\n (Read in Perusall; linked in Moodle) 7. Iteration , Baumer, Benjamin S., Daniel T. Kaplan, and Nicholas J. Horton (2021). Modern Data Science with R. 2nd. CRC Press. URL: https://mdsr-book.github.io/mdsr2e/ (visited on Jan. 14, 2022).\n Study Plan Assessment Due\n Watch Lab 6 Recap in Perusall\n\n\n Day 13 Slides: Functions"
  },
  {
    "objectID": "schedule.html#november-09-2022",
    "href": "schedule.html#november-09-2022",
    "title": "Schedule",
    "section": "November 09, 2022",
    "text": "November 09, 2022\n\nIteration\nPROGRAMMING IN R\n\nDue TodayFurther Resources\n\n\n Project 2 Due\n Lab 7 Due\n\n\n Day 14 Slides: Iteration\n purrr cheatsheet"
  },
  {
    "objectID": "schedule.html#november-11-2022",
    "href": "schedule.html#november-11-2022",
    "title": "Schedule",
    "section": "November 11, 2022",
    "text": "November 11, 2022\n\nLab: Programming in R\nPROGRAMMING IN R\n\nDue TodayFurther Resources\n\n\n Project 3 Assigned\n\n\n Lab 8 Template\n Lab 8 Instructions"
  },
  {
    "objectID": "schedule.html#november-14-2022",
    "href": "schedule.html#november-14-2022",
    "title": "Schedule",
    "section": "November 14, 2022",
    "text": "November 14, 2022\n\nMap Projections and Spatial Thinking\nMAPPING\n\nDue TodayFurther Resources\n\n\n (Read in Perusall; linked in Moodle) 17. Working with geospatial data (17.1-17.3) , Baumer, Benjamin S., Daniel T. Kaplan, and Nicholas J. Horton (2021). Modern Data Science with R. 2nd. CRC Press. URL: https://mdsr-book.github.io/mdsr2e/ (visited on Jan. 14, 2022).\n Watch Lab 7 Recap in Perusall\n\n\n Day 15 Slides: Point Mapping\n Leaflet Guide"
  },
  {
    "objectID": "schedule.html#november-16-2022",
    "href": "schedule.html#november-16-2022",
    "title": "Schedule",
    "section": "November 16, 2022",
    "text": "November 16, 2022\n\nLab: Mapping Point Data in Leaflet\nMAPPING\n\nDue TodayFurther Resources\n\n\n Lab 8 Due\n\n\n Lab 9 Template\n Lab 9 Instructions"
  },
  {
    "objectID": "schedule.html#november-18-2022",
    "href": "schedule.html#november-18-2022",
    "title": "Schedule",
    "section": "November 18, 2022",
    "text": "November 18, 2022\n\nPolygon Mapping in Leaflet\nMAPPING\n\nDue TodayFurther Resources\n\n\n Quiz 2 Recommended Deadline\n\n\n Day 16 Slides: Polygon Mapping\n MP3 Template Your team name should be a string with your team members’ last names."
  },
  {
    "objectID": "schedule.html#november-21-2022",
    "href": "schedule.html#november-21-2022",
    "title": "Schedule",
    "section": "November 21, 2022",
    "text": "November 21, 2022\n\nHow to Lie with Maps\nMAPPING\n\nDue TodayFurther Resources\n\n\n (Read in Perusall; linked in Moodle) 17. Working with geospatial data (17.4-17.8) , Baumer, Benjamin S., Daniel T. Kaplan, and Nicholas J. Horton (2021). Modern Data Science with R. 2nd. CRC Press. URL: https://mdsr-book.github.io/mdsr2e/ (visited on Jan. 14, 2022).\n Group Contract Due\n Watch Lab 8 Recap in Perusall\n\n\n Day 17 Slides: How to Lie with Maps"
  },
  {
    "objectID": "schedule.html#november-23-2022",
    "href": "schedule.html#november-23-2022",
    "title": "Schedule",
    "section": "November 23, 2022",
    "text": "November 23, 2022\n\nNo Class\n\nDue TodayFurther Resources"
  },
  {
    "objectID": "schedule.html#november-25-2022",
    "href": "schedule.html#november-25-2022",
    "title": "Schedule",
    "section": "November 25, 2022",
    "text": "November 25, 2022\n\nNo Class\n\nDue TodayFurther Resources"
  },
  {
    "objectID": "schedule.html#november-28-2022",
    "href": "schedule.html#november-28-2022",
    "title": "Schedule",
    "section": "November 28, 2022",
    "text": "November 28, 2022\n\nWorking with APIs\nDATA RETRIEVAL\n\nDue TodayFurther Resources\n\n\n Quiz 2 Due\n Lab 9 Due\n\n\n Day 18 Slides: APIs\n SoQL Documentation"
  },
  {
    "objectID": "schedule.html#november-30-2022",
    "href": "schedule.html#november-30-2022",
    "title": "Schedule",
    "section": "November 30, 2022",
    "text": "November 30, 2022\n\nAdvanced APIs\nDATA RETRIEVAL\n\nDue TodayFurther Resources\n\n\n\n\n\n Lab 10 Template\n Lab 10 Instructions"
  },
  {
    "objectID": "schedule.html#december-02-2022",
    "href": "schedule.html#december-02-2022",
    "title": "Schedule",
    "section": "December 02, 2022",
    "text": "December 02, 2022\n\nLab: Working with APIs\nDATA RETRIEVAL\n\nDue TodayFurther Resources\n\n\n Watch Lab 9 Recap in Perusall"
  },
  {
    "objectID": "schedule.html#december-05-2022",
    "href": "schedule.html#december-05-2022",
    "title": "Schedule",
    "section": "December 05, 2022",
    "text": "December 05, 2022\n\nWork on Group Projects in Class\n\nDue TodayFurther Resources"
  },
  {
    "objectID": "schedule.html#december-07-2022",
    "href": "schedule.html#december-07-2022",
    "title": "Schedule",
    "section": "December 07, 2022",
    "text": "December 07, 2022\n\nTBD\n\nDue TodayFurther Resources"
  },
  {
    "objectID": "schedule.html#december-09-2022",
    "href": "schedule.html#december-09-2022",
    "title": "Schedule",
    "section": "December 09, 2022",
    "text": "December 09, 2022\n\nProject 3 Shares\n\nDue TodayFurther Resources\n\n\n Quiz 3 Recommended Deadline"
  },
  {
    "objectID": "schedule.html#december-12-2022",
    "href": "schedule.html#december-12-2022",
    "title": "Schedule",
    "section": "December 12, 2022",
    "text": "December 12, 2022\n\nWrap-up\n\nDue TodayFurther Resources\n\n\n Project 3 Due\n Quiz 3 Due\n Lab 10 Due"
  },
  {
    "objectID": "lab1.html",
    "href": "lab1.html",
    "title": "Lab 1: Understanding Datasets",
    "section": "",
    "text": "This lab is all about learning to understand the context and parts of a dataset by referencing and interpreting data dictionaries and technical data documentation.\n\n\n\nRead a data dictionary\nReference data documentation\nIdentify unique observations in a dataset\nUnderstand different variable types\nLook up value codes and recode a variable\nDetermine the number of missing values in a variable and why they are missing"
  },
  {
    "objectID": "lab1.html#review-of-key-terms",
    "href": "lab1.html#review-of-key-terms",
    "title": "Lab 1: Understanding Datasets",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\nRectangular Datasets\n\ndatasets in which all rows are the same length, and all columns are the same length\n\nObservations\n\nrows in a dataset; represent discrete entities we observe in the world\n\nVariables\n\ncolumns in a dataset; describe something about an observation\n\nVector\n\none-dimensional set of values that are all of the same type\n\nData Frame\n\na list of vectors of equal lengths; typically organizes data into a two-dimensional table composed of columns (the vectors) and rows\n\nUnique Key\n\nvariable (column) in the dataset that can be used to uniquely identify each row\n\nNominal categorical variables\n\nvariables that identify something else; sometimes, numbers are considered nominal categorical variables (e.g. zip code)\n\nOrdinal categorical variables\n\ncategorical variables that can be ranked or placed in a particular order (e.g. High, Medium, Low)\n\nDiscrete numeric variables\n\nnumeric variables that represent something that is countable (e.g. the number of students in a classroom, the number pages in a book)\n\nContinuous numeric variables are variables\n\nvariables in which it is always possible to measure the value more precisely (e.g. time can be measured with infinite amount of specificity - hours > minutes > seconds > milliseconds > microseconds > nanoseconds …)"
  },
  {
    "objectID": "lab1.html#scorecard-dataset",
    "href": "lab1.html#scorecard-dataset",
    "title": "Lab 1: Understanding Datasets",
    "section": "Scorecard Dataset",
    "text": "Scorecard Dataset\nIn his 2013 State of the Union Address, President Barack Obama announced his plans to create a “college scorecard” that would allow prospective students and parents to compare schools in terms of cost, offerings, diversity, completion rates, and post-graduate earnings. This data was first published in 2015 and since has undergone several improvements and revisions.\nThe College Scorecard dataset is massive. In fact, I thought long and hard about whether this was really the first dataset I wanted to introduce to you in a lab. It includes information about over 6500 institutions in the U.S., and has more than 3000 columns documenting information about those institutions. I chose this dataset for this lab because, if you can learn to read this data dictionary, you will be leaps and bounds ahead of the game in learning to read other data dictionaries. (It’s also just a super cool dataset, and hint, hint: you will get a chance to dive into it in much more detail in a few weeks). While the full data is available online, we are only going to work with a small subset of the data today."
  },
  {
    "objectID": "lab1.html#setting-up-your-environment",
    "href": "lab1.html#setting-up-your-environment",
    "title": "Lab 1: Understanding Datasets",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nInstall the RScorecard package by entering the following into your Console: install.packages(\"rscorecard\")\nCreate a Scorecard API Key at this link. Shortly after you fill out the form, you will be emailed a key. Copy that key into code chunk below, replacing all of the following text in sc_key(): Sys.getenv(“SCORECARD_KEY”). Be sure to wrap the key in quotation marks.\nDownload the Scorecard Data Dictionary and Technical Documentation for Institution-Level Data Files here.\nRun the code below to the import 2018 Scorecard data for Massachusetts into R. Call me or one of the data assistants over if you get an error.\n\n\nlibrary(tidyverse)\nlibrary(rscorecard)\nsc_key(Sys.getenv(\"SCORECARD_KEY\")) # Replace Sys.getenv(\"SCORECARD_KEY\") here with your API Key in quotation marks\n\nscorecard <- sc_init() %>%\n  sc_year(2018) %>%                 #Note how we are looking at only 2021 data here!\n  sc_filter(stabbr == \"MA\") %>%     #Note how we are looking at only Massachusetts data here!\n  sc_select(unitid, instnm, city, highdeg, control, ugds, adm_rate, costt4_a, costt4_p, pcip27, pctfloan, admcon7, wdraw_orig_yr2_rt, cdr3) %>%\n  sc_get()"
  },
  {
    "objectID": "lab1.html#glimpsing-the-data",
    "href": "lab1.html#glimpsing-the-data",
    "title": "Lab 1: Understanding Datasets",
    "section": "Glimpsing the Data",
    "text": "Glimpsing the Data\nWhen working with very large datasets, we need tools to help us get a sense of the dataset without having to load the entire data frame. For instance, we can view the first 6 rows of the dataset by calling head().\n\nscorecard %>% head()\n\n# A tibble: 6 × 15\n  unitid instnm       city  highdeg control  ugds adm_r…¹ costt…² costt…³ pcip27\n   <int> <chr>        <chr>   <int>   <int> <int>   <dbl>   <int>   <int>  <dbl>\n1 164368 Hult Intern… Camb…       4       2   574   0.571   60090      NA 0     \n2 164447 American In… Spri…       4       2  1307   0.684   47742      NA 0     \n3 164465 Amherst Col… Amhe…       3       2  1855   0.128   71300      NA 0.110 \n4 164492 Anna Maria … Paxt…       4       2  1131   0.736   51109      NA 0     \n5 164535 Assabet Val… Marl…       1       1    50   0.452      NA   19703 0     \n6 164562 Assumption … Worc…       4       2  2014   0.811   53303      NA 0.0162\n# … with 5 more variables: pctfloan <dbl>, admcon7 <int>,\n#   wdraw_orig_yr2_rt <dbl>, cdr3 <dbl>, year <dbl>, and abbreviated variable\n#   names ¹​adm_rate, ²​costt4_a, ³​costt4_p\n\n\nstr() provides a great deal of information about the observations in the data frame, including the number of variables, the number of observations, the column names, their data types, and a list of observations.\n\nscorecard %>% str()\n\ntibble [153 × 15] (S3: tbl_df/tbl/data.frame)\n $ unitid           : int [1:153] 164368 164447 164465 164492 164535 164562 164580 164599 164614 164632 ...\n $ instnm           : chr [1:153] \"Hult International Business School\" \"American International College\" \"Amherst College\" \"Anna Maria College\" ...\n $ city             : chr [1:153] \"Cambridge\" \"Springfield\" \"Amherst\" \"Paxton\" ...\n $ highdeg          : int [1:153] 4 4 3 4 1 4 4 1 3 4 ...\n $ control          : int [1:153] 2 2 2 2 1 2 2 3 2 2 ...\n $ ugds             : int [1:153] 574 1307 1855 1131 50 2014 2361 45 56 1921 ...\n $ adm_rate         : num [1:153] 0.571 0.684 0.128 0.736 0.452 ...\n $ costt4_a         : int [1:153] 60090 47742 71300 51109 NA 53303 68482 NA 26691 46315 ...\n $ costt4_p         : int [1:153] NA NA NA NA 19703 NA NA NA NA NA ...\n $ pcip27           : num [1:153] 0 0 0.11 0 0 ...\n $ pctfloan         : num [1:153] 0.0566 0.8333 0.1661 0.7464 0.4286 ...\n $ admcon7          : int [1:153] 3 5 1 3 3 3 1 NA 1 5 ...\n $ wdraw_orig_yr2_rt: num [1:153] NA 0.181 0.124 0.129 NA ...\n $ cdr3             : num [1:153] 0.043 0.073 0.014 0.073 0.065 0.044 0.019 0.06 0.024 0.048 ...\n $ year             : num [1:153] 2018 2018 2018 2018 2018 ...\n\n\nYou can also click on the name of your data frame in your Environment panel in RStudio, and it will open a new tab in RStudio that displays the data in a tabular format. Try clicking on scorecard in your Environment panel.\n\n\n\n\n\n\nTip\n\n\n\nThis is the same as calling df %>% View() in your Console.\n\n\n\n\n\n\n\n\nNote\n\n\n\nI use the letters df as a placeholder to refer to an arbitrary data frame in R. Any time that I refer to df in a lab, know that you can swap out df with any data frame in your environment."
  },
  {
    "objectID": "lab1.html#getting-to-know-this-dataset",
    "href": "lab1.html#getting-to-know-this-dataset",
    "title": "Lab 1: Understanding Datasets",
    "section": "Getting to Know this Dataset",
    "text": "Getting to Know this Dataset\n\nObservations (Rows)\nIn starting our data analysis, we need to have a good sense of what each observation in our dataset refers to - or its observational unit. Think of it this way. If you were to count the number rows in your dataset, what would that number refer to? A unique key is a variable (or set of variables) that uniquely identifies an observation in the dataset. Think of a unique key as a unique way to identify a row and all of the values in it. There should never be more than one row in the dataset with the same unique key. A unique key tells us what each row in the dataset refers to.\n\nQuestion\n\nSee if you can identify a unique key for this dataset. Write some lines of code to determine whether the column you’ve identified can act as unique key for the data. Hint: You need to check whether the values in the column ever repeat.\n\n\n\n\n\n\n\n\n\n\n\n\nWhy not use name as a unique key?\n\n\n\nNote that NAME is typically not an appropriate variable to use as a unique key. Let me provide an example to demonstrate this. When I worked for BetaNYC, I was trying to build a map of vacant storefronts in NYC by mapping all commercially zoned properties in the city, and then filtering out those properties where a business was licensed or permitted. This way the map would only include properties where there wasn’t a business operating. One set of businesses I was filtering out was restaurants. The only dataset that the city had made publicly available for restaurant permits was broken. It was operating on an automated process to update whenever there was a change in the permit; however, whenever a permit was updated, rather than updating the appropriate fields in the existing dataset, it was creating a new row in the dataset that only included the permit holder (the restaurant name), the permit type, and the updated fields. Notably the unique permit ID was not being included in this new row. We pointed this issue out to city officials, but fixing something like this can be slow and time-consuming, so in the meantime, we looked into whether we could clean the data ourselves by aggregating the rows that referred to the same restaurant. However, without the permit ID it was impossible to uniquely identify the restaurants in the dataset. Sure, we had the restaurant name, but do you know how many Wendy’s there are in NYC?\n\n\nAnytime we count something in the world, we are not only engaging in a process of tabulation; we are also engaging in a process of defining. If I count the number of students in a class, I first have to define what counts as a student. If someone is auditing the class, do they count? If I, as the instructor, am learning from my students, do I count myself as a student? As I make decisions about how I’m going to define “student,” those decisions impact the numbers that I produce. When I change my definition of “student,” how I go about tabulating students also changes. Thus, as we prepare to count observations in a dataset, it is important to know how those observations are defined.\n\n\n\nQuestion\n\nAt this point, you’ve probably figured out that each row in this dataset is a higher education institution. …but there are many different ways that we can define higher education institutions, and that will impact what gets included and excluded in our data. Referencing the Technical Documentation, locate a definition for the unit of observation in this dataset. What institutions are included, and what institutions are excluded? Summarize a definition below.\n\n\n\n\nVariables (Columns)\nNote the column names for this dataframe, and the kinds of values that appear in those columns. Some of them (like city and year) might make sense to you immediately. Others (like pcip27 and highdeg) might be much more confusing. To figure out what we are looking out, we are going to need to refer to the dataset’s data dictionary.\nOpen the data dictionary you downloaded in an earlier step. It will open as an Excel file. Click on the tab labeled “Institution_Data_Dictionary”. There are thousands of variables in this dataset, falling into the broader categories of school, completion, admissions, cost, etc. Note how the file is organized, and specifically draw your attention to:\n\nColumn 1 (NAME OF DATA ELEMENT): This is a long description of the variable and gives you clues as to what is represented in it.\nColumn 6 (VARIABLE NAME): This is the column name for the variable. This is how you will reference the variable in R.\nColumn 7 (VALUE): These are the possible values for that variable. Note that for many categorical variables, the values are numbers. We are going to have to associate the numbers with their corresponding labels.\nColumn 8 (LABEL): These are the labels associated with the values recorded for the variable.\nColumn 11 (NOTES): This provides notes about the variable, including whether it is currently in use and what missing values indicate.\n\n\nQuestion\n\nFor each of the variable names in the scorecard data frame, look up the associated name in the data dictionary. You will need to search for the variable name in the sixth column of the data dictionary (I recommend using Ctrl-F to quickly locate the variable name in the spreadsheet.) Once you’ve found the variable name, reference column 1 to determine what this variable means, and reference columns 7 and 8 to see what possible values will appear in that column.\nIdentify one nominal variable, one ordinal variable, one discrete variable, and one continuous variable in scorecard and list their variable names below. Then uncomment the lines below and use the typeof() function to see how R determined their data types. Did any surprise you?\n\n\n#typeof(scorecard$_____)\n#typeof(scorecard$_____)\n#typeof(scorecard$_____)\n#typeof(scorecard$_____)\n\n\n\n\n\n\n\nValues (Cells)\nYou may have noticed that several categorical variables are coded as numbers in the imported dataset. For instance, look at the column control which designates the institution’s ownership. Running the code below, we see that the distinct values in that column are 1, 2, and 3.\n\nscorecard %>% distinct(control)\n\n# A tibble: 3 × 1\n  control\n    <int>\n1       2\n2       1\n3       3\n\n\nWhen we reference that column in the data dictionary (row 27), we see that a 1 in that column designates that the institution is Public, a 2 that the institution is Private nonprofit, and a 3 that the institution is Private for-profit. While I can always look that up, sometimes it is helpful to have that information in our dataset. For instance, let’s say I create a bar plot that’s supposed to show how many higher education institutions have each type of ownership in MA (which you will learn how to do soon!). The plot can be confusing when control is a series of numbers.\n\nggplot(scorecard, aes(x = control)) +\n  geom_bar()\n\n\n\n\nWith this in mind, sometimes it can be helpful to recode the values in a column. Recoding data involves replacing the values in a vector according to criteria that we provide. Remember how all columns in a data frame are technically vectors? We can use the recode() function to recode all of the values in the control vector. We are going to store the recoded values in a new column in our dataset called control_text. Check out the code below to see how we do this. Reference the help pages for recode (i.e. ?recode) to help you interpret the code.\n\nscorecard$control_text <-\n  recode(\n    scorecard$control, \n    \"1\" = \"Public\", \n    \"2\" = \"Private nonprofit\", \n    \"3\" = \"Private for-profit\",\n    .default = NA_character_\n  )\n\nCheck out our barplot now!\n\nggplot(scorecard, aes(x = control_text)) +\n  geom_bar()\n\n\n\n\n\nQuestion\n\nWrite code below to recode the admcon7 variable and store the results in a new variable in scorecard called admcon7_text. You’ll need to look up the values in the data dictionary. If you’ve done this correctly, running this code should produce a barplot that displays multiple bars.\n\n\nscorecard$admcon7_text <-\n  recode(\n    scorecard$admcon7, \n    #Fill replacements here\n    .default = NA_character_\n  )\n\nggplot(scorecard, aes(x = admcon7_text)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\n\n\n\nMissing Values\nWhen we have missing values in a rectangular dataset, we have to provide a placeholder for the missing value in order for the dataset to remain rectangular. If we just skipped the value, then our dataset wouldn’t necessarily have rows of all equal lengths and columns of all equal lengths. In R, NA serves as that placeholder. Before we start analyzing data, it can be important to note how many NA values we have in a column so that we can determine if the data is representative.\nThe function is.na() checks whether a value is an NA value and returns TRUE if it is and FALSE if it isn’t. Providing a vector to is.na() will check this for every value in the vector and return a logical vector indicating TRUE/FALSE for every original value in the vector.\n\nis.na(scorecard$wdraw_orig_yr2_rt)\n\n  [1]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE\n [25] FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE\n [37] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n [49] FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE\n [73]  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE\n [85] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE\n [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE\n[109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE\n[121]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE\n[133]  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n[145] FALSE  TRUE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE\n\n\nWhen we sum() across a logical vector, R will calculate the number of TRUE values in the vector.\n\nsum(is.na(scorecard$wdraw_orig_yr2_rt))\n\n[1] 48\n\n\n\nQuestion\n\nIn a code chunk below, calculate the number of missing values in both the costt4_a and the costt4_p columns. Reference the NOTES column in the data dictionary to determine why there are so many NA values in these columns. Add a comment to the code chunk below, explaining the reason for missing values in these columns.\n\n\n\n\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nReferencing the College Scorecard data documentation, see if you can determine which students are included in calculations of earnings and debt. How might the data’s coverage bias the values that get reported? What might be the social consequences of these biases? Share your ideas on our sds-192-discussions Slack channel."
  },
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "Lab 6: Joining Data",
    "section": "",
    "text": "In this lab, you will …\n\n\n\n…"
  },
  {
    "objectID": "lab6.html#review-of-key-terms",
    "href": "lab6.html#review-of-key-terms",
    "title": "Lab 6: Joining Data",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\n\n\n\n\n\nTip\n\n\n\nYou may wish to reference this Data Wrangling Cheatsheet when completing this lab.\n\n\nleft\nright\ninner\nouter\nanti"
  },
  {
    "objectID": "lab6.html#twitter-dataset",
    "href": "lab6.html#twitter-dataset",
    "title": "Lab 6: Joining Data",
    "section": "Twitter Dataset",
    "text": "Twitter Dataset"
  },
  {
    "objectID": "lab6.html#setting-up-your-environment",
    "href": "lab6.html#setting-up-your-environment",
    "title": "Lab 6: Joining Data",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nInstall the TwitteR package by entering the following into your Console: install.packages(\"twitteR\")\nAPI Key and Secret\nConfigure App\nhttp://127.0.0.1:1410\nhttps://github.com/sds-192-intro-fall22/sds-192-public-website-quarto\n\n\nlibrary(rtweet)\n\nWarning: package 'rtweet' was built under R version 4.1.2\n\nauth_setup_default()\n\nUsing default authentication available.\nReading auth from '/Users/lpoirier_1/Library/Preferences/org.R-project.R/R/rtweet/default.rds'\n\nrt <- search_tweets('\"Smith College\"', n = 1000, include_rts = FALSE)\nusers <- users_data(rt)\nin_reply <- lookup_tweets(rt$in_reply_to_status_id)\nusers_reply <- users_data(in_reply)\n\nuseRs <- search_users(\"#smithies\", n = 200)\n\nuseRs_twt <- tweets_data(useRs)\n\nR_foundation_flw <- get_followers(\"lindsaypoirier\", n = 30000, \n                                  retryonratelimit = TRUE)\nR_foundation_flw_data <- lookup_users(head(R_foundation_flw$from_id, 100), verbose = FALSE)\nR_foundation_tweets <- get_timelines(head(R_foundation_flw$from_id, 10))\n\nWarning: `get_timelines()` was deprecated in rtweet 1.0.0.\nPlease use `get_timeline()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\n…\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\n… If you finish early, I encourage you to attempt to plot some of this data below using ggplot()!"
  },
  {
    "objectID": "lab5.html",
    "href": "lab5.html",
    "title": "Lab 5: Exploratory Data Analysis",
    "section": "",
    "text": "In this lab, you will apply the 5 data wrangling verbs we learned this week in order to analyze data regarding NYPD stop, question, and frisk. Specifically, we will replicate data analysis performed by the NYCLU in 2011 to demonstrate how the practice was being carried out unconstitutionally in New York.\n\n\n\nAnswer questions with data using data wrangling verbs\nConsider the implications of racial categorization"
  },
  {
    "objectID": "lab5.html#review-of-key-terms",
    "href": "lab5.html#review-of-key-terms",
    "title": "Lab 5: Exploratory Data Analysis",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\n\n\n\n\n\nTip\n\n\n\nYou may wish to reference this Data Wrangling Cheatsheet when completing this lab.\n\n\n\nfilter\n\nfilters to rows (observations) that meet a certain criteria\n\nselect\n\nkeeps only selected variables (columns)\n\narrange\n\nsorts values in a variable (column)\n\nsummarize\n\ncalculates a single value by performing an operation across a variable (column); summarizing by n() calculates the number of observations in the column\n\ngroup_by\n\ngroups rows (observations) by shared values in a variable (column); when paired with summarize(), performs an operation in each group\n\nmutate\n\ncreates a new variable (column) and assigns values according to criteria we provide"
  },
  {
    "objectID": "lab5.html#stop-question-and-frisk-dataset",
    "href": "lab5.html#stop-question-and-frisk-dataset",
    "title": "Lab 5: Exploratory Data Analysis",
    "section": "Stop, Question, and Frisk Dataset",
    "text": "Stop, Question, and Frisk Dataset\nIn 1968, the Supreme Court case Terry v. Ohio ruled that a police officer could stop an individual without “probable cause” but with “reasonable suspicion” the suspect had committed a crime. Further, an officer could frisk an individual without “probable cause” but with “reasonable suspicion” the suspect was carrying a weapon.\nIn the early 1990s, stop, question, and frisk became a widely prevalent tactic in policing in NYC. Every time an officer stops a civilian in NYC, the officer is supposed to complete a UF-250 form (see image below) outlining the details of the stop.\n\n\n\nUF250 Side 2, Dan Nguyen’s Blog\n\n\n\n\n\nUF250 Side 2, Dan Nguyen’s Blog\n\n\nAs a result of some high profile shootings in the late 1990s and mid-2000s, both the New York State Attorney General’s Office and the New York Civil Liberties Union began to examine NYPD stop and frisk activity for racial profiling. With pressure from these organizations, in the mid-2000s, information recorded on UF-250 forms began getting reported in public databases. In the early 2010s, when the NYPD’s use of stop and frisk went before the US District Court, this data was integral in proving that stop and frisk was being carried out in NYC in an unconstitutional way. The ruling mandated that the NYPD create a policy outlining when stops were authorized, and since the practice has declined considerably. See\n(Smith 2018) and (Southall and Gold 2019) for more information.\n\nData Dictionary\nWe will analyze data for all stops in 2011. This data can be found here. However, we’re only going to be considering a subset of the columns. Here is a data dictionary for the columns we’re using in this analysis.\n\n\n\n\n\n\n\n\nVARIABLE\nDEFINITION\nPOSSIBLE VALUES\n\n\n\n\npct\nPrecinct of the stop\n1:123\n\n\narrestsum\nWas an arrest made or summons issued?\n1 = Yes\n0 = No\n\n\nfrisked\nWas suspect frisked?\n1 = Yes\n0 = No\n\n\nwpnfound\nWas a weapon found on suspect?\n1 = Yes\n0 = No\n\n\nrace_cat\nSuspect’s race\nAMERICAN INDIAN/ALASKAN NATIVE\nASIAN/PACIFIC ISLANDER\nBLACK\nBLACK-HISPANIC\nOTHER\nWHITE\nWHITE-HISPANIC\n\n\nage\nSuspect’s age\n999 indicates a missing value"
  },
  {
    "objectID": "lab5.html#setting-up-your-environment",
    "href": "lab5.html#setting-up-your-environment",
    "title": "Lab 5: Exploratory Data Analysis",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nlibrary(tidyverse)\n\nsqf_url <- \"https://www1.nyc.gov/assets/nypd/downloads/zip/analysis_and_planning/stop-question-frisk/sqf-2011-csv.zip\"\ntemp <- tempfile()\ndownload.file(sqf_url, temp)\nsqf_zip <- unzip(temp, \"2011.csv\")\nsqf_2011 <- read.csv(sqf_zip, stringsAsFactors = FALSE) \nsqf_2011_race_cat <- read.csv(\"https://raw.githubusercontent.com/lindsaypoirier/STS-101/master/Data/SQF/sqf_race_categories.csv\", stringsAsFactors = FALSE) \nrm(sqf_url)\nrm(temp)\nrm(sqf_zip)\n\n\nPrepare Dataset for Analysis\nThe original dataset codes each race with a single letter. This adds a column called race_cat that writes out each racial category in accordance with the data documentation. It also replaces every instance of “Y” in the dataset with 1 and every instance of “N” with 0. This will allow us to sum the Yes’s in the dataset. You’ll learn more about how this code works in coming weeks.\n\nsqf_2011 <- \n  sqf_2011 %>% \n  select(pct, race, age, frisked, pistol, riflshot, asltweap, knifcuti, machgun, othrweap, sumissue, arstmade) %>%\n  left_join(sqf_2011_race_cat, by = \"race\") %>%\n  mutate(across(frisked:arstmade, \n         ~ case_when(. == \"Y\" ~ 1, . == \"N\" ~ 0)))\nrm(sqf_2011_race_cat)\n\nThe original dataset had separate variables for indicating whether a pistol, rifle, assault weapon, knife, machine gun, or other weapon was found on a suspect. We will create a variable equal to 1 if any of these weapons were found on the suspect. Further, the original dataset had separate variables for indicating whether a stop resulted in an arrest made or summons issued. We will create a variable equal to 1 if either occurred.\n\n\n\n\n\n\nExercise 1\n\n\n\nAdd new columns indicating 1) whether a weapon was found or 2) an arrest/summons was made.\n\n\n\nsqf_2011 <- \n  sqf_2011 %>%\n  #Add a variable for weapon found\n  _____(wpnfound = case_when(pistol == 1 |\n                               riflshot == 1 | \n                               asltweap == 1 |\n                               knifcuti == 1 | \n                               machgun == 1 | \n                               othrweap == 1 ~ 1,\n                             TRUE ~ 0))\nsqf_2011 <- \n  sqf_2011 %>%\n  #Add a variable for arrest made or summons issued\n  _____(arrestsumm = case_when(sumissue == 1 | \n                                arstmade == 1 ~ 1,\n                               TRUE ~ 0))\n\n\n\n\n\n\n\nExercise 2\n\n\n\nSubset the dataset to the six variables listed in the data dictionary above.\n\n\n\nsqf_2011 <-\n  sqf_2011 %>%\n  _____(pct, arrestsumm, _____, wpnfound, race_cat, _____)"
  },
  {
    "objectID": "lab5.html#analysis",
    "href": "lab5.html#analysis",
    "title": "Lab 5: Exploratory Data Analysis",
    "section": "Analysis",
    "text": "Analysis\n\n\n\n\n\n\nExercise 3\n\n\n\nCalculate the number of stops in 2011. Hint: Keep in mind that every row in the dataset represents one stop.\n\n\n\n\n\n\n\n\nTip\n\n\n\nsummarize() will return a data frame with the summarized value. Remember a data frame is a two-dimensional table with rows and columns (even if it’s a 1x1 table). If we want just the value, we can call pull() to extract the value from that two–dimensional table.\n\n\n\ntotal_stops <-\n  sqf_2011 %>%\n  summarize(Count = _____) %>%\n  pull()\n\ntotal_stops\n\n\n\n\n\n\n\nExercise 4\n\n\n\nHow many stops did not result in an arrest or summons in 2011? What percentage of stops did not result in an arrest or summons?\n\n\n\nsqf_2011 %>%\n  #Subset to rows where suspect innocent\n  _____(arrestsumm _____ 0) %>% \n  #Calculate number of observations\n  _____(total_innocent = n(), \n            percent_innocent = _____ / total_stops * 100)\n\n\n\n\n\n\n\nExercise 5\n\n\n\nIn how many stops were the individuals aged 14-24? In what percentage of stops were the individuals aged 14-24?\n\n\n\nsqf_2011 %>%\n  #Subset to rows where suspect age 14-24\n  _____(age _____ 14 & age _____ 24) %>% \n  #Calculate number of observations and percentage of observations\n  _____(total_14_24 = _____, \n            percent_14_24 = n() / total_stops * 100)\n\nWhy doesn’t this match the values we see on the NYCLU website?\nNote the following from the NYCLU’s 2011 report on Stop, Question, and Frisk data:\n\n“In a negligible number of cases, race and age information is not recorded in the database. Throughout this report, percentages of race and age are percentages of those cases where race and age are recorded, not of all stops.”\n\n\ntotal_stops_age_recorded <-\n  sqf_2011 %>%\n  #Subset to rows where age is not 999\n  _____(age _____ 999) %>% \n  summarize(Count = n()) %>%\n  pull()\n\nsqf_2011_sub %>%\n  filter(age >= 14 & age <= 24) %>%\n  summarize(total_14_24 = n(), \n            percent_14_24 = n() / total_stops_age_recorded * 100)\n\nThis still doesn’t match the values we see on the website, but it does match the values we see in the NYCLU’s 2011 report on Stop, Question, and Frisk data. This is typically when I would reach out to a representative at the NYCLU to inquire about the discrepancy.\n\n\n\n\n\n\nExercise 6\n\n\n\nHow many stops were there per race in 2011? What percentage of stops per race in 2011? Arrange by number of stops in descending order.\n\n\n\ntotal_stops_race_recorded <-\n  sqf_2011 %>%\n  #Subset to rows where race_cat is not NA or \"OTHER\"\n  filter(_____(race_cat) & race_cat _____ \"OTHER\") %>% \n  summarize(Count = n()) %>%\n  pull()\n\nsqf_2011 %>%\n  #Subset to rows where race_cat is not NA or \"OTHER\"\n  _____(_____(race_cat) & race_cat _____ \"OTHER\") %>% \n  #Group by race\n  _____(race_cat) %>% \n  #Calculate number of observations\n  _____(stops = n(), \n            percent_stops = n() / total_stops_race_recorded * 100) %>%\n  #Sort by stops in descending order\n  _____(_____(stops)) \n\nNote how this dataset categorizes race. Many different government datasets categorize race in many different ways. This, for instance, is not how the US Census categorizes race. How we categorize race matters for how we can talk about discrimination and racial profiling. Imagine if WHITE was not categorized separately from WHITE-HISPANIC. The values in this dataset would appear very differently! The NYCLU chose to aggregate two racial categories in this dataset into the one category - Latino - in order to advance certain claims regarding discrimination. What we should remember is that these racial categories are not reported by those stopped; they are recorded by officers stopping individuals. They may not reflect how individuals identify themselves.\n\nsqf_2011 %>%\n  #Subset to rows where race_cat is \"WHITE-HISPANIC\" or \"BLACK-HISPANIC\"\n  _____(race_cat _____ c(\"WHITE-HISPANIC\", \"BLACK-HISPANIC\")) %>% \n  #Calculate number of observations\n  _____(stops_Latinx = _____, \n            percent_Latinx = n() / total_stops_race_recorded * 100)\n\n\n\n\n\n\n\nExercise 7\n\n\n\nWhat percentage of stops in 2011 resulted in a frisk per race? What percentage of stops in 2011 resulted in a weapon found per race? What percentage of stops in 2011 resulted in an arrest or summons per race? In your resulting data table, each row should be a race, and there should be columns for stops, percent_stops, percent_frisked, percent_wpnfound , and percent_arrestsum.\n\n\n\n# Write code here. \n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nIn her ruling in the 2011 District Court case Floyd vs. the City of New York, Honorable Shira Sheidlen remarked the following regarding the database of stops we analyzed today:\n\nBecause it is impossible to individually analyze each of those stops, plaintiffs’ case was based on the imperfect information contained in the NYPD’s database of forms (“UF-250s”) that officers are required to prepare after each stop. The central flaws in this database all skew toward underestimating the number of unconstitutional stops that occur: the database is incomplete, in that officers do not prepare a UF-250 for every stop they make; it is one-sided, in that the UF250 only records the officer’s version of the story; the UF-250 permits the officer to merely check a series of boxes, rather than requiring the officer to explain the basis for her suspicion; and many of the boxes on the form are inherently subjective and vague (such as “furtive movements”). Nonetheless, the analysis of the UF-250 database reveals that at least 200,000 stops were made without reasonable suspicion.\n\nWhat are some of the consequences of the incompleteness and/or one-sided nature of this dataset? How should we think about and communicate its flaws vs its value? How might this data collection program be redesigned so as to represent more diverse perspectives? Share your ideas on our sds-192-discussions Slack channel.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you finish early, I encourage you to attempt to plot some of this data below using ggplot()!"
  },
  {
    "objectID": "problem-solving.html",
    "href": "problem-solving.html",
    "title": "Problem Solving with Data",
    "section": "",
    "text": "Read problem carefully.\nTake a deep breath!\nRead the problem again.\nDetermine the unit of observation in your available data and its relevance to the question.\nDetermine which variables in the available data are relevant to your question.\nDraw what your final data frame might look like.\nGet your data frame down to size! Compare the scope of what is requested in the question to what is requested in your data and subset accordingly.\nWrangle! (This is the most variable step, and you’ll need to have a handle on our data wrangling verbs.)\nSelect the most relevant variables for final presentation."
  },
  {
    "objectID": "problem-solving.html#interpreting-error-messages",
    "href": "problem-solving.html#interpreting-error-messages",
    "title": "Problem Solving",
    "section": "Interpreting Error Messages",
    "text": "Interpreting Error Messages\nThroughout this week, we have taken a look at different error messages that R presents when it can’t evaluate our code. Today, we will consider these in more detail. First, it’s important to make some distinctions between the kinds of messages that R presents to us when attempting to run code:\n\nErrors\n\nTerminate a process that we are trying to run in R. They arise when it is not possible for R to continue evaluating a function.\n\nWarnings\n\nDon’t terminate a process but are meant to warn us that there may be an issue with our code and its output. They arise when R recognizes potential problems with the code we’ve supplied.\n\nMessages\n\nAlso don’t terminate a process and don’t necessarily indicate a problem but simply provide us with more potentially helpful information about the code we’ve supplied.\n\n\nCheck out the differences between an error and a warning in R by reviewing the output in the Console when you run the following code chunks.\n\nError in R\n\nsum(\"3\", \"4\")\n\nError in sum(\"3\", \"4\"): invalid 'type' (character) of argument\n\n\n\n\nWarning in R\n\nvector1 <- 1:5\nvector2 <- 3:6\nvector1 + vector2\n\nWarning in vector1 + vector2: longer object length is not a multiple of shorter\nobject length\n\n\n[1]  4  6  8 10  8\n\n\nSo what should you do when you get an error message? How should you interpret it? Luckily, there are some clues and standardized components of the message the indicate why R can’t execute the code. Consider the following error message that you received when running the code above:\nError in sum(“3”, “4”) : invalid ‘type’ (character) of argument\nThere are three things we should pay attention to in this message:\n\nThe word “Error” indicates that this code did not run.\nThe text immediately after the word “in” tells us which specific function did not run.\nThe text after the colon gives us clues as why the code did not run.\n\nReviewing the error above, I can guess that there was a problem with the argument that I supplied to the sum() function, and specifically that I supplied a function of the wrong type.\n\nQuestion\n\nRun the codes below and check out the error messages. Review the code to fix each of the errors. Note that each subsequent code chunk relies on the previous code chunk, so you will need to fix the errors in order and run the chunks in order.\n\n\n# Create three vectors\na <- 1, 2, 3, 4, 5\nb <- \"a\", \"b\", \"c\", \"d\", \"e\"\nc <- TRUE, FALSE, TRUE, TRUE, FALSE\n\nError: <text>:2:7: unexpected ','\n1: # Create three vectors\n2: a <- 1,\n         ^\n\n\n\n\n\n\n# Add the values in the vector a\na_added <- add(a)\n\nError in add(a): could not find function \"add\"\n\n\n\n\n\n\n# Multiply the previous output by 3\nthree_times_a_added <- added_a * 3\n\nError in eval(expr, envir, enclos): object 'added_a' not found\n\n\n\n\n\n\n# Create a dataframe with col1 and col2\ndf <- data.frame(\n  col1 = c(1, 2, 3)\n  col2 = c(\"a\", \"b\", \"c\")\n\nError: <text>:4:3: unexpected symbol\n3:   col1 = c(1, 2, 3)\n4:   col2\n     ^\n\n\n\n\n\n\n# Add a new column to df\ndf$col3 <- c(TRUE, FALSE)\n\nError in df$col3 <- c(TRUE, FALSE): object of type 'closure' is not subsettable"
  },
  {
    "objectID": "problem-solving.html#preparing-to-get-help",
    "href": "problem-solving.html#preparing-to-get-help",
    "title": "Problem Solving",
    "section": "Preparing to Get Help",
    "text": "Preparing to Get Help\nWhen we do get errors in our code and need to ask for help in interpreting them, it’s important to provide collaborators with the information they need to help us. Sometimes when teaching R I will hear things like: “My code doesn’t work!” or “I’m stuck and don’t know what to do,” and it can be challenging to suss out the root of the issue without more information. Here are some strategies for describing issues you are having with your code:\n\nReference line numbers. Notice the left side of this document has a series of numbers listed vertically next to each line? These are known as line numbers. Oftentimes, if you are having an issue with your code and ask me to review it, I will say something like: “Check out line 53.” By this I mean that you should scroll the document to the 53rd line. You can similarly tell me or your peers which line of your code you are struggling with.\nCompose good reproducible examples. A good reproducible example includes all of the lines of code that we need to reproduce an output on our own machines. This means that if you create a vector in a previous code snippet and then supply it as an argument in another code snippet, you are going to want to make sure that both of these lines of code appear in your reproducible example. Further, if the functions that you are using are from certain packages, you will want to make sure the library() call to load that package is in your reproducible example.\nUse the code and code block buttons in Slack to share example code. First, when we copy and paste code from RStudio into programs like Slack and email, we can’t see the output. Second, certain characters like quotation marks and apostrophes are treated differently across these programs. For example, run the code chunk below and check out the output in your Console. The first line of code I typed directly into RStudio. The second I copied over from Slack.\n\n\n# typed directly into RStudio\ntoupper(\"apple\")\n# copied from Slack\ntoupper(“apple”)\n\nError: <text>:4:9: unexpected input\n3: # copied from Slack\n4: toupper(“\n           ^\n\n\nNotice the slight differences in the quotation marks? R recognizes the first but doesn’t recognize the second, even though I used the same keyboard key to create both. This is due to the fact that these two systems use different character encodings.\nThe Code button (for a single line of code) and Code Block button (for multiple lines of code) in Slack are useful tools for composing code and avoiding character encoding issues. If you click these buttons when typing a Slack message, you can enter code in the red outlined box that appears, and this will easily copy to RStudio. I will ask you to always use these features when copying code this semester.\n\n\n\n\n\n\nTip\n\n\n\nIn Slack, you can also wrap text backticks (` `) to have it output in a single-line code block, and three backticks (``` ```) to have it output as a multi-line code block.\n\n\n\nQuestion\n\nCopy and paste one line of code from this lab as a threaded comment on the Slack message I posted on September 7, 2022. Be sure the text is formatted as code."
  },
  {
    "objectID": "problem-solving.html#help-pages",
    "href": "problem-solving.html#help-pages",
    "title": "Problem Solving",
    "section": "Help pages",
    "text": "Help pages\nOne resource we’ve already discussed are the R help pages. I tend to use the help pages when I know the function I need to use, but can’t remember how to apply it or what its parameters are. Help pages typically include a description of the function, its arguments, details about the function, the values it produces, a list of related functions, and examples of its use. We can access the help pages for a function by typing the name of the function with a question mark in front of it into our Console (e.g. ?log or ?sum). Some help pages are well-written and include helpful examples, while others are spotty and don’t include many examples.\n\nQuestion\n\nAccess the help pages for the function sort(). Write code below to sort the vector a (which you created in an earlier step) in decreasing order."
  },
  {
    "objectID": "problem-solving.html#cheatsheets",
    "href": "problem-solving.html#cheatsheets",
    "title": "Problem Solving",
    "section": "Cheatsheets",
    "text": "Cheatsheets\nThe R community has developed a series of cheatsheets that list the functions made available through a particular package and their arguments. I tend to use cheatsheets when I know what I need to do to a dataset in R, but I can’t recall the function that enables me to do it.\n\nQuestion\n\nNavigate to this cheatsheet for base R. Imagine we collected the temperature of our home each day for the past ten days (see code below).\n\n#Create a vector of temperatures\ntemps_to_factor <- c(68, 70, 78, 75, 69, 80, 66, 66, 79)\n\nLet’s say we wanted to find how each day ranked from coolest to hottest. For instance, I wanted to know the ranking for day 1 vs. day 2 vs. day 3, and so on. Using the cheatsheet, find the function that will allow you to generate a ranking of each day’s temperature. Search the help pages for this function to determine how to randomly rank two days with the same temperature. Write a comment to yourself describing how this function is different than sorting the data."
  },
  {
    "objectID": "grading_contract.html",
    "href": "grading_contract.html",
    "title": "Assessment",
    "section": "",
    "text": "This course will be graded using a standards-based assessment system. In a more traditional grading system, your scores on a series of assignments are averaged over the course of the semester. In this course, rather than assessing and averaging your achievement on particular assignments, I will instead be assessing the development of your fluency in a set of pre-defined standards. You will have multiple opportunities over the course of the semester to showcase the depth of your understanding regarding these standards. A standards-based grading system carries the following benefits:\nThis is my second iteration of teaching a standards-based course, and I’ve made several revisions from the first iteration based on what I’ve learned from experience and student feedback. I’m excited to refine this system this semester as I believe it aligns with my overarching goals for the course. In this course, I find it far more important that you come away with an understanding of the concepts behind core data science strategies (along with an ability to find and interpret reference materials) than it is to demonstrate memorization of R syntax. Developing this understanding will empower you to learn and apply new data science languages on your own."
  },
  {
    "objectID": "grading_contract.html#what-are-the-standards-i-will-be-assessed-on-in-this-course",
    "href": "grading_contract.html#what-are-the-standards-i-will-be-assessed-on-in-this-course",
    "title": "Assessment",
    "section": "What are the standards I will be assessed on in this course?",
    "text": "What are the standards I will be assessed on in this course?\n\n\n\n\n\n\nData Visualization\n\n\n\n\nThis dimension refers to the development of your ability to produce multiple types of compelling and well-designed visualizations from data.\n\nVISUALIZATION AESTHETICS PLOTTING FREQUENCIES AND DISTRIBUTIONS MAPPING\n\n\n\n\n\n\n\n\nData Wrangling\n\n\n\n\nThis dimension refers to the development of your ability to transform datasets into new formats in order to prepare them for further analysis or visualization.\n\nTRANSFORMING DATA JOINING DATASETS TIDYING DATA\n\n\n\n\n\n\n\n\nData Science Workflow\n\n\n\n\nThis dimension refers to the development of your ability to apply data science best practices in your work.\n\nUNDERSTANDING DATASETS DATA RETRIEVAL GITHUB PROGRAMMING IN R\n\n\n\n\n\n\n\n\nData Ethics\n\n\n\n\nThis dimension refers to your ability to recognize and navigate ethical dilemmas that emerge in data science work. There is not a formal standard for data ethics because issues related to data ethics will be discussed in every unit of the course, and you will be assessed on data ethics issues related to other standards in quizzes and projects."
  },
  {
    "objectID": "grading_contract.html#how-will-i-be-assessed-on-the-course-standards",
    "href": "grading_contract.html#how-will-i-be-assessed-on-the-course-standards",
    "title": "Assessment",
    "section": "How will I be assessed on the course standards?",
    "text": "How will I be assessed on the course standards?\n\nInformal Assessments\n\nReadingsLab Recaps\n\n\nEach week, you will be assigned a section of the course texts to read prior to class. I expect that you will come having read this section in order to prepare for in-class exercise and labs. You do not need to complete the exercises in the course texts but may choose to do so if you wish. Please note however that we won’t have time to go over the solutions in class, and I don’t have a solutions manual for these texts (though I’m happy to go over them in office hours). All course readings will be available in Perusall, and you can post questions and comments in the reading for myself or your classmates to answer.\n\n\nLab solutions will be posted as video lectures in Perusall. I expect that you will check your lab answers by reviewing these videos. You may leave comments in Perusall at certain timestamps as questions come up.\n\n\n\n\n\nFormal Assessments\n\n\n\n\n\n\nTip\n\n\n\n5PM (close of business day) will be the cut-off time for all assignments in this course. The reason I’ve set the deadlines to 5PM is that I’d like to discourage students from staying up late into the night to complete assignments. Note that there is a 24-hour grace period for submitting labs and project assignments. This means that you will still get full credit for these assignments as long as they are submitted by 5PM the day following the assignment due date. However, quizzes and course advancement assignments must be submitted by 5PM on the due date for credit.\n\n\n\nLabsProjectsQuizzesCourse Advancement\n\n\nIn most weeks, you will be assigned a lab, which you will start in class and complete at home. There will be one lab per standard. Labs will be designed to help you practice applying the course standards towards the analysis of a dataset. You may work on labs in groups, but all group members should submit their own lab. Labs will be graded for completion, and you can earn 3 points per standard by completing the lab associated with that standard. All sections of the lab must be completed in good faith to earn these points.\n\n\nThere will be 3 projects, to be completed in groups of 3-4, assigned over the course of the semester. In each you will have an opportunity to demonstrate fluency in standards we have covered up to that point in the semester. I will provide prompts for each project, but you will have a lot of flexibility to demonstrate your own creativity and explore your own interests in designing a project around the prompt. You can earn up to 3 points towards 9 of the ten standards based on your project submission. If you don’t earn full credit on a standard for a project submission, you may improve your score on that standard in the next project. The only standard that won’t be covered in projects is Data Retrieval, which we will cover too late in the semester to work into projects. Projects will be graded for fluency.\n\n\nThere will be 3 quizzes administered throughout the semester - each assessing 3-4 course standards. There will be 3 questions per standard on each quiz, and you can earn up to 3 points towards each standard based on your quiz attempt: 1 point per question. In this sense, quizzes will be graded for fluency.\nQuizzes will be taken at home, administered in Moodle, and are open book/open Internet. You may start a quiz at any time before its due date, but it must be completed by its due date in order to earn credit. Please note that extensions will not be granted for quizzes.\n\n\nThere are a series of very short assignments on the syllabus that are designed to ensure that you are prepared for individual and collaborative work. This includes things like reviewing the syllabus, developing a course study plan, developing group collaboration plans, and completing peer evaluations. In total, there are 13 course advancement assignments, and you can earn 1 point towards your final grade per assignment.\n\n\n\nAssignment\nPoints\n\n\n\n\nSyllabus Quiz\n1\n\n\nCATME Survey\n1\n\n\nProblem Solving Lab\n2\n\n\nStudy Plan\n2\n\n\nStudy Plan Evaluation\n2\n\n\nGroup Contract (x3)\n3\n\n\n\nThese assignments will be graded for completion. For the most part, you will get out of them, what you put into them. Because these assignments are designed to keep our course running smoothly, please note that extensions will not be granted for course advancement assignments.\n\n\n\n\n\n\n\n\n\nReassessment\n\n\n\nThis course will have an optional final exam. The final exam is an opportunity to reassess standards that you have not received full quiz credit on. If you choose to take the final exam, you only need to complete the sections of the exam associated with standards you wish to reassess, and the score you ultimately receive for a standard will be based on whichever is higher of your quiz score or your final exam score for that standard."
  },
  {
    "objectID": "grading_contract.html#how-will-this-system-work",
    "href": "grading_contract.html#how-will-this-system-work",
    "title": "Assessment",
    "section": "How will this system work?",
    "text": "How will this system work?\nIf you’ve been adding, you may have figured out by this point that we have:\n\\((3 * 10) + (3 * 10) + (3 * 9) = 87\\)\nYou can earn the remaining 13 points by completing the course advancement assignments, for a grand total of 100 points. At the end of the semester, I will sum your scores on all standards and other assignments and assign final grades accordingly:\n\n\n\nLetter Grade\nNumeric Grade\n\n\n\n\nA\n≥ 92.5\n\n\nA-\n≥ 90.0\n\n\nB+\n≥ 87.5\n\n\nB\n≥ 82.5\n\n\nB-\n≥ 80.0\n\n\nC+\n≥ 77.5\n\n\nC\n≥ 72.5\n\n\nC-\n≥ 70.0\n\n\nD+\n≥ 67.5\n\n\nD\n≥ 62.5\n\n\nD-\n≥ 60.0\n\n\nE\n< 60.0"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SDS 192: Introduction to Data Science",
    "section": "",
    "text": "Data science involves applying a set of strategies to transform a recorded set of values into something from which we can glean knowledge and insight. This course will introduce you to concepts and methods from the field of data science, along with how to apply them in R. You will learn how to acquire, clean, wrangle, and visualize data. You will also learn best practices in data science workflows, such as code documentation and version control. Issues in data ethics will be addressed throughout the course.\nClasses will be held on Mondays, Wednesdays, and Fridays from 9:25 AM to 10:40 AM in McConnell 404.\nSDS 100: Reproducible Scientific Computing with Data is a co-requisite for this course and designed to help support you in coding in R. Please note that I walk into this course with the assumption that most students have never coded before. Coding for the first time can be intimidating, but I intend do everything in my power to support you through the learning curve and to make things both fun and relevant in the process. I personally picked up most of my data science skills through a lot of trial-and-error, practice, and curiosity. My hope is that, in this course, you will learn through experimentation, along with independent and collaborative problem-solving. Honing these competencies will serve you as you move on to other courses in the SDS program and/or at Smith.\n\n\n\n\n\n\n\n\nLindsay Poirier, she/her/hers.\n\n\n\n\n\nWhile you’re welcome to refer to me as Professor Poirier, I would prefer it if you called me Lindsay. I am a cultural anthropologist that studies how public interest datasets get produced, how communities think about and interface with data, and how data infrastructure can be designed more equitably. My Ph.D. is in an interdisciplinary discipline called Science and Technology Studies - a field that studies the intricate ways science, technology, culture, and politics all co-constitute each other. I work on a number of collaborative research projects that leverage public data to deepen understanding of social and environmental inequities in the US, while also qualitatively studying the politics behind data gaps and inconsistencies. As an instructor, I prioritize active learning and often structure courses as flipped classrooms. You can expect in-class time to predominantly involve a mix of lectures and live problem-solving exercises.\n\n\n\nSlackOffice Hours\n\n\nI can best support students in this course when I can readily keep tabs on our course-related communication. Because of this, I ask that you please don’t email me regarding course-related questions or issues. The best way to get in touch with me is via our course Slack. If you have course-related questions, I encourage you to ask them in the #sds-192-questions channel. When discretion is needed, feel free to DM. Please reserve more formal concerns like grades or accommodation requests for an in-person (or in-person virtual) conversation.\nDuring the week, I will try my best to answer all Slack messages within 24 hours of receiving them. Please note that to maintain my own work-life balance, I often don’t answer Slack messages late in the evenings or on the weekends. It’s important that you plan when you start your assignments accordingly.\n\n\nOffice hours are a great opportunity for us to chat about what you’re learning in the course, clarify expectations on assignments, and review work in progress. I also love when students drop in to office hours to request book recommendations, discuss career or research paths, or just to say hi! I encourage each student in the course to join office hours at least once this semester. If you’re unable to attend my office hours at the regularly scheduled time, there is link on Moodle to book a meeting with me.\n\nMonday, 3-4, McConnell 212-213\nWednesday, 3-5, McConnell 212-213\n\n\n\n\n\n\n\n\nA number of excellent textbooks introducing data science concepts and methods have been written in the past few years, including a few from faculty in the Smith SDS department. To accompany the topics we will cover each week, I will be selecting my favorite chapters from these books and posting them to Perusall. However, all three books we will engage in this course cover almost every topic we will address, so feel free to supplement your reading with corresponding chapters in the other books: especially if you find yourself drawn to the teaching and writing style in a certain book. All books are available for free online.\n\nBaumer, Benjamin S., Daniel T. Kaplan, and Nicholas J. Horton. 2021. Modern Data Science with R. 2nd ed. CRC Press. https://mdsr-book.github.io/mdsr2e/.\nIrizarry, Rafael A. 2022. Introduction to Data Science. Data Analysis and Prediction Algorithms with R. https://rafalab.github.io/dsbook/.\nIsmay, Chester, and Albert Y. Kim. 2021. Modern Dive: Statistical Inference via Data Science. CRC Press. https://moderndive.com/.\n\nEach week I will also list optional reading and resources in our course schedule that you may reference if you are struggling with a topic or if you wish to explore that topic further. I will update this list often throughout the semester.\n\n\n\nThis course will be graded via a standards-based assessment system.\n\n\n\n\n\n\nSpinelli Center\n\n\n\nSmith’s Spinelli Center offers a number of resources to support SDS students. Spinelli Center Data Assistants will visit our classroom regularly to support you through lab work. The Center also offers drop-in tutoring hours Sunday through Thursday 7-9 PM. Finally, you can drop-in to Seelye 207D or schedule an appointment with the Data Research and Statistics Counselor (Kenneth Jeong). To schedule an appointment, email qlctutor@smith.edu.\n\n\n\n\n\n\nPreparationAttendanceExtensionsAcademic Honesty\n\n\nThis is a 4-credit course with 4.5 hours per week of in-classroom instructions. Smith expects students to devote 7.5 out-of-class hours per week to 4-credit classes. I have designed the course assignments and selected the course readings with this target in mind.\n\n\nAttending class is not only important for your learning but also an act of community. Attendance is expected in this course. Many course assignments will be completed in-class. That said, you do not need to inform me when you will be absent. If you are sick, please stay home. If you must miss a class entirely, you should contact a peer to discuss what was missed. Please note that the SDS Program has adopted a shared policy regarding in-person attendance this semester:\n\nIn keeping with Smith’s core identity and mission as an in-person, residential college, SDS affirms College policy (as per the Provost and Dean of the College) that students will attend class in person. SDS courses will not provide options for remote attendance. Students who have been determined to require a remote attendance accommodation by the Office of Disability Services will be the only exceptions to this policy. As with any other kind of ADA accommodations, please notify your instructor during the first week of classes to discuss how we can meet your accommodations.\n\n\n\nThere is an automatic 24-hour grace period on all lab and project assignments. There will be no penalties for submitting the project within this 24-hour period, and you do not need to inform me that you intend to take the extra time. You can also request up to a 72-hour extension on any project or lab assignment, as long as you make that request at least 48 hours before the original assignment due date. You can request an extension by filling out the Extension Request form on Moodle, and I will confirm your extension on Slack. Beyond this, late assignments will not be accepted.\n\n\n\nSmith College expects all students to be honest and committed to the principles of academic and intellectual integrity in their preparation and submission of course work and examinations. Students and faculty at Smith are part of an academic community defined by its commitment to scholarship, which depends on scrupulous and attentive acknowledgement of all sources of information, and honest and respectful use of college resources.\n\nAny cases of dishonesty or plagiarism will be reported to the Academic Honor Board. Examples of dishonesty or plagiarism include:\n\nSubmitting work completed by another student as your own.\nCopying and pasting words from sources without quoting and citing the author.\nParaphrasing material from another source without citing the author.\nFailing to cite your sources correctly.\nFalsifying or misrepresenting information in submitted work.\nPaying another student or service to complete assignments for you.\n\n\n\n\n\n\n\n\n\n\nDeadlines for Quizzess and Course Advancement Assignments\n\n\n\nThe standards you will be practicing in this course all build off of each other, and it’s important that I know how students are doing on each standard in order to direct my teaching going forward. Because of this, the deadlines for quizzes are a bit more firm than for other assignments. On the course schedule and in Moodle you will see a suggested deadline for quizzes and a final deadline. I won’t be able to accept quiz submissions after the final deadline, so you’ll want to be sure to stay on top of these dates. Similarly, course advancements assignments are assignments that need to be completed to keep our course moving. Because of this, they need to be completed by the due date. I will give you an opportunity to work on many of these assignments in class.\n\n\n\n\n\n\nCode of ConductPrinciples of CommunityPronouns\n\n\nAs the instructor for this course, I am committed to making participation in this course a harassment-free experience for everyone, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, or religion. Examples of unacceptable behavior by participants in this course include the use of sexual language or imagery, derogatory comments or personal attacks, trolling, public or private harassment, insults, or other unprofessional conduct.\nAs the instructor I have the right and responsibility to point out and stop behavior that is not aligned to this Code of Conduct. Participants who do not follow the Code of Conduct may be reprimanded for such behavior. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the instructor.\nAll students and the instructor are expected to adhere to this Code of Conduct in all settings for this course: seminars, office hours, and over Slack.\nThis Code of Conduct is adapted from the Contributor Covenant, version 1.0.0, available here.\n\n\nI hope that we can foster a collaborative and caring environment in this classroom: one that celebrates successes, respects individual strengths and weaknesses, demonstrates compassion for each other’s struggles, and affirms diverse identities. Here are some ideas that I have for creating this environment in our course:\n\nCheck-in with colleagues before starting collaborative work. “What three words describe how you’re feeling?” “Name one challenge and one success from this week.” “What are you doing for self-care right now?” Thank each other for sharing where they’re at.\nConsider when to step up and when to step back in class discussions, creating space for others to contribute. Listening is just as important to community-building as speaking.\nAcknowledge that there is much we don’t know about how our colleagues experience the world. …but don’t ask colleagues to speak on behalf of a social group you perceive them to be a part of.\nCheer on colleagues as they give presentations or try something out for the first time.\nAsk questions often in our #sds-192-questions channel. Help each other out by answering questions when you can.\nMistakes happen. I will certainly make mistakes in class. Admit mistakes, and then move on.\n\n\n\nUsing the proper pronouns for our students is foundational to a safe, respectful classroom environment that creates a culture of trust. For information on pronouns and usage, please see the Office of Equity and Inclusion link here: Pronouns\n\n\n\n\n\n\n\nAccommodationsStudent Well-beingTrigger Warnings\n\n\nIt is my goal for everyone to succeed in this course. If you have personal circumstances that may impact your experience of our classroom, I encourage you to contact Office of Disability Services in College Hall 104 or at ods@smith.edu. The Office will generate a letter that indicates to me what kind of support you need and how I can make your classroom experience more accommodating. Once you have this letter, you are welcome to visit my office hours or email me to discuss ideas about how we can tailor the course accordingly. While you can request accommodations at any time, the sooner we start this conversation, the better. If you have concerns about the course that are not addressed through ODS, please contact me. At no point will I ask you to divulge details about your personal circumstances to me.\n\n\nCollege life is stressful, and life outside of college can be overwhelming. It is my position that attending to your physical and mental health and well-being should be a top priority. I will remind you of this often throughout the semester. I encourage you to schedule a time to talk with me if you are struggling with this course. If you, or anyone you know, is experiencing distress, there are numerous campus resources that can provide support via the Schacht Center. I can point you to these resources at any time throughout the semester.\n\n\nA trigger is a topic or image that can precipitate an intense emotional response. When common triggering topics are to be covered in this course, I will do my best to provide a trigger warning in advance of the discussion. However, I can’t always anticipate triggers. With this in mind I’ve set up an anonymous form, available on Moodle, where you can indicate topics for which you would like me to provide a warning.\n\n\n\n\n\n\n\nMoodlePerusallSlackGitHub\n\n\nGrades, forms, handouts, and quizzes will be available on the course Moodle.\n\n\nAll course readings and recorded lectures will be available on Perusall. You can access Perusall via our course Moodle page.\n\n\n\n#general: Course announcements (only I can post)\n#sds-192-discussions: Share news articles and relevant opportunities\n#sds-192-questions: Ask and answer questions about our course\nYou can also create private Slack channels with your project group members.\n\n\n\nI will be using GitHub Classroom to distribute several course assignments, and you will submit assignments by pushing changes to template documents to a private GitHub repository. I will provide guidance on how to do this early in the semester.\n\n\n\n\n\n\n\n\n\nRStudio/RStudio Server\n\n\n\nThis class will use the R statistical software package. In the first week of the course, I will help you install and configure R and RStudio. If you are using a laptop, you will install both on that computer. If you are using a Chromebook or Tablet, an account will be created for you on the Smith College RStudio Server so that you can access a cloud-based version of RStudio. You should let me know in the first week of the course if you are using a Chromebook or tablet."
  },
  {
    "objectID": "lab3.html",
    "href": "lab3.html",
    "title": "Lab 3: Plotting Frequencies and Distributions",
    "section": "",
    "text": "The goal of this lab is to provide you with practice in producing data visualizations that help to answer a research question.\n\n\n\nProduce and interpret univariate plots\nProduce and interpret multivariate plots\nContextualize plots with descriptive labels and titles"
  },
  {
    "objectID": "lab3.html#review-of-key-terms",
    "href": "lab3.html#review-of-key-terms",
    "title": "Lab 3: Plotting Frequencies and Distributions",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\nMultivariate Plots\n\nPlots that summarize and visualize the distribution and relationship between multiple variables\n\nUnivariate Plots\n\nPlots that summarize and visualize the distribution of a single variable"
  },
  {
    "objectID": "lab3.html#spotify-dataset",
    "href": "lab3.html#spotify-dataset",
    "title": "Lab 3: Plotting Frequencies and Distributions",
    "section": "Spotify Dataset",
    "text": "Spotify Dataset\nToday, we are prioritizing joy! Our research question will be: How joyful are popular Spotify playlists in my favorite music genre?\nThe music feature from Spotify’s data that serves as a measure of joy is called valence. This is the description from their API documentation for valence:\n\nA measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n\n(Pretty vague if you ask me, but today we’ll go with it.)"
  },
  {
    "objectID": "lab3.html#setting-up-your-environment",
    "href": "lab3.html#setting-up-your-environment",
    "title": "Lab 3: Plotting Frequencies and Distributions",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\n\n\n\n\n\nTip\n\n\n\nToday’s set-up is a little more complicated than usual, so be sure to take it slow and ask questions as they come up!\n\n\n\nInstall the Spotify R package by entering the following into your Console: install.packages(\"spotifyr\")\nLog-in to Spotify’s Developer Dashboard here. If you have a Spotify account, you can log-in with that account. Otherwise, you should create one.\nClick the ‘Create an App’ button to create an app named “SDS 192 Class Project”. You can indicate that this is a “Project for SDS 192 class”\nClick Edit Settings. Under the heading Redirect URIs copy and paste this URL: http://localhost:1410/, and click Add. Scroll to the bottom of the window and click Save. This is going to allow us to authenticate our Spotify accounts through our local computers.\nClick the Users and Access button. Scroll down to Add New User, and add your name and the email address associated with your Spotify account. Click Add.\nClick “Show Client Secret”. Copy client id and secret below, and then run the code chunk.\n\n\nlibrary(spotifyr)\n# id <- 'FILL CLIENT ID HERE'\n# secret <- 'FILL CLIENT SECRET HERE'\n# Sys.setenv(SPOTIFY_CLIENT_ID = id)\n# Sys.setenv(SPOTIFY_CLIENT_SECRET = secret)\n\n\nBelow replace poiril with your Spotify username. This is the ID that appears in the upper right hand corner when you log into your Spotify account (not your developer account.)\nSearch Spotify for your favorite music genre and select three playlists from the search. Note that this code will only work for playlists, and playlists may be a ways down in the search results.\nWhen you click on a playlist, notice the URL in the navigation bar of your web browser. It should look something like spotify.com/playlist/LONG_STRING_OF_CHARACTERS. Copy the long string of characters at the end of the URL, and paste it into the function below. Your characters should replace the example playlist I’ve added: 7ryj1GwAWYUY36VQd4uXoq.\nRepeat this for the other two playlists, replacing my other examples. Then run the code.\n\n\nlibrary(tidyverse)\nspotify_playlists <- get_playlist_audio_features(\n  username = \"poiril\",\n  playlist_uris = c(\"7ryj1GwAWYUY36VQd4uXoq\",\n                    \"3DYUw0nHB9o8tLZKQup4zp\",\n                    \"37i9dQZF1DXdOEFt9ZX0dh\"),\n  authorization = get_spotify_access_token()\n) %>%\n  select(-c(track.artists, \n            track.available_markets, \n            track.album.artists, \n            track.album.available_markets, \n            track.album.images))"
  },
  {
    "objectID": "lab3.html#data-analysis",
    "href": "lab3.html#data-analysis",
    "title": "Lab 3: Plotting Frequencies and Distributions",
    "section": "Data Analysis",
    "text": "Data Analysis\nIn the exercises below, you will create a series of plots that will enable us to compare the joyfulness of the three playlists you’ve selected.\nI encourage you to take a look at the spotify_playlists data frame in your environment. You’ll note that each row in the dataset is a song/track from one of the the three playlists you selected (unique ID would be track.id), and columns provide information about that song (such as the track.name, the playlist it is a part of, its key, loudness, danceability, and acousticness). We can produce some pretty cool visualizations from this data. For instance check out how we might compare the relationship between the energy and acousticness of songs across the three selected playlists.\n\nspotify_playlists %>%\n  ggplot(aes(x = acousticness, y = energy)) +\n  geom_point(alpha = 0.5, size = 0.5) +\n  coord_flip() +\n  facet_wrap(vars(playlist_name)) +\n  labs(title = \"Acousticness and Energy of Songs in Classic Rock Spotify Playlists, 2022\",\n       x = \"Acousticness\", \n       y = \"Energy\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou might use this plot as a starting template for the plots you are going to create below!\n\n\nWe’re going to start our analysis with univariate plotting. Specifically, we are going to produce data visualizations that count the number of observations in a dataset that fall into specific groupings. When grouping observations by a categorical variable, we will produce a bar plot. When grouping observations into intervals of a numeric variable, we will produce a histogram. Remember when labeling that these plots visualize frequency.\n\n\n\n\n\n\nWarning\n\n\n\nKeep in mind that all plots should have 5 contextual details represented in titles or labels:\n\nThe data’s unit of observation\nThe variables represented\nAny filters applied\nThe geographic scope of the data\nThe temporal scope of the data\n\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\nHow many songs are in each playlist? Create a plot to visualize this, and order the results by the number of songs. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nExercise 2\n\n\n\nWhat is the distribution of valence across all of the songs (in intervals of 0.1 valence)? Create a histogram to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n\n# Create plot here\n\nLet’s move on to some multivariate plotting. Remember that we can add further variables to a plot via a number of different aesthetics (e.g. color: fill= or col=; size: size=; position: x= or y=, small multiples: + facet_wrap(vars(...)) ). Whenever we add further data to a plot, we should be on the lookout for overplotting.\n\n\n\n\n\n\nExercise 3\n\n\n\nWhat is the distribution of valence across all of the songs (in intervals of 0.1 valence) in each playlist? Create a histogram to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nExercise 4\n\n\n\nWhat are differences in the summary statistics (max, min, median, etc.) of the valence of songs in each playlist? Create grouped boxplot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nExercise 5\n\n\n\nDo happier songs tend to be more danceable in each playlist? Create a plot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context. Also be sure to adjust your plot to address overplotting.\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nExercise 6\n\n\n\nDo songs composed in the minor or major mode tend to be happier in each playlist? Create a plot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nExercise 7\n\n\n\nDo happier songs tend to have a higher tempo across all playlists? What role might the song’s mode play? Create a plot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context. Also be sure to adjust your plot to address overplotting.\n\n\n\n#Create boxplot here\n\n\n\n\n\n\n\nExercise 8\n\n\n\nBased on all of your plots, summarize what you learned about the joyfulness of these three playlists below.\n\n\n\n\n\n\n\n\nYour Response\n\n\n\nFill response here.\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nCheck out this article documenting ethical concerns regarding Spotify’s data collection practices. Should we be concerned about the assumptions that Spotify makes about us based on our music streaming habits? What about the way they curate music for us? What are some of the social consequences to this form of user surveillance? Share your ideas on our `sds-192-discussions` Slack channel."
  },
  {
    "objectID": "lab2.html",
    "href": "lab2.html",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "",
    "text": "Designing effective data visualizations involves reviewing available data and then determining how best to map variables in the data onto a variety of visual cues. When we refer to visual cues, we are referring to those visual components of the plot that help us discern differences across data points. For instance, a plot might use different shapes or colors to represent different categories of data. A plot might also place points at different positions or bars at different heights to represent different numeric values. This week we will practice mapping variables in a dataset onto different plot aesthetics in order to tell different stories with the data. We will only be creating one type of plot today - a scatterplot. However, we are going to show how we can use different visual cues to plot a number of different variables onto a scatterplot.\n\n\n\nRead the ggplot cheatsheets\nMap variables onto plot aeshetics\nAdjust the attributes of a plot\nAdjust the scales of aeshetics on plots\nDeal with overplotting\nFacet plots into small multiples"
  },
  {
    "objectID": "lab2.html#review-of-key-terms",
    "href": "lab2.html#review-of-key-terms",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\nAesthetics\n\nVisual cues that we map variables in a dataset onto\n\nCartesian grid\n\nA 2-dimensional grid with an intersecting x and y axis\n\nSequential color\n\nA uni-directional ordering of shades\n\nQualitative color\n\nA discrete set of colors\n\nDivergent color\n\nA diverging ordering of color shades\n\nOverplotting\n\nInstances when visual representations of individual data points overlap on a plot making aspects of the plot illegible"
  },
  {
    "objectID": "lab2.html#county-health-ranking-dataset",
    "href": "lab2.html#county-health-ranking-dataset",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "County Health Ranking Dataset",
    "text": "County Health Ranking Dataset\nToday we will be working with an incredible data resource from the University of Wisconsin Population Health Institute. It aggregates data from a number of government sources to produce county health indicators for every county in the US. Something to keep in mind when reviewing this data:\nNote that, while this is a powerful data source for visualizing health disparities, particularly in smaller counties, there can be large degrees of uncertainty in the reporting of certain health measures. We won’t be working with this dataset’s confidence intervals today, so we do want to consider that the visualizations that we see are not a perfect reflection of county health. Run the code below to load in the datasets we will be working with today."
  },
  {
    "objectID": "lab2.html#setting-up-your-environment",
    "href": "lab2.html#setting-up-your-environment",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nInstall the RColorBrewer package by entering the following into your R Console: install.packages(\"RColorBrewer\")\nRun the code below to the import the bridge inventory for Massachusetts and for Hampshire County into R. Call me or one of the data assistants over if you get an error.\n\n\nlibrary(tidyverse)\nlibrary(RColorBrewer)\ncounties <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_counties.csv\")\nroute_prefixes <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_route_pre.csv\")\nmaintenance <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_maintenance.csv\")\nkinds <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_kind.csv\")\n\nnbi_ma <- read.delim(\"https://www.fhwa.dot.gov/bridge/nbi/2022/delimited/MA22.txt\", sep = \",\") %>%\n  left_join(counties) %>%\n  left_join(route_prefixes) %>%\n  left_join(maintenance) %>%\n  left_join(kinds) %>%\n  filter(SERVICE_ON_042A == 1) %>%\n  select(STRUCTURE_NUMBER_008, COUNTY_CODE_003_L, ROUTE_PREFIX_005B_L, MAINTENANCE_021_L, YEAR_BUILT_027, ADT_029, STRUCTURE_KIND_043A_L, STRUCTURAL_EVAL_067, BRIDGE_IMP_COST_094) %>%\n  mutate(STRUCTURE_KIND_043A_L = \n           case_when(\n             STRUCTURE_KIND_043A_L == \"Concrete continuous\" ~ \"Concrete\",\n             STRUCTURE_KIND_043A_L == \"Steel continuous\" ~ \"Steel\",\n             STRUCTURE_KIND_043A_L == \"Prestressed concrete continuous\" ~ \"Prestressed concrete\",\n             TRUE ~ STRUCTURE_KIND_043A_L)) %>%\n  mutate(BRIDGE_IMP_COST_094 = BRIDGE_IMP_COST_094 * 1000)\n\nnbi_hampshire <- nbi_ma %>% filter(COUNTY_CODE_003_L == \"Hampshire\")\n\nrm(counties, kinds, maintenance, route_prefixes)"
  },
  {
    "objectID": "lab2.html#visualization-principles",
    "href": "lab2.html#visualization-principles",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Visualization Principles",
    "text": "Visualization Principles\n\n\n\n\n\n\nTip\n\n\n\nYou may wish to reference this ggplot cheatsheet when completing this lab. Note how this cheatsheet is organized. There are headings for things like:\n\nBasics\nGeom functions\nScales\nCoordinate systems\n\nThe first tip to find what you’re looking for is to consider what heading the graphic element will likely fall under. Also note that, for many functions on this cheatsheet:\n\nan image is provided for how the plot will transform,\nthe function and its arguments are referenced in bold, and\na text description of what will happen when you apply the function is provided\n\nFinally note that most of these functions listed on this cheatsheet are appended to the ggplot() function call with a + sign."
  },
  {
    "objectID": "lab2.html#exercise-1-reference-the-ggplot-cheatsheet-to-add-a-subtitle-to-this-plot-and-to-set-the-theme-to-minimal-to-balance-the-data-to-ink-ratio.",
    "href": "lab2.html#exercise-1-reference-the-ggplot-cheatsheet-to-add-a-subtitle-to-this-plot-and-to-set-the-theme-to-minimal-to-balance-the-data-to-ink-ratio.",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Exercise 1: Reference the ggplot() cheatsheet to add a subtitle to this plot and to set the theme to minimal to balance the data-to-ink ratio.",
    "text": "Exercise 1: Reference the ggplot() cheatsheet to add a subtitle to this plot and to set the theme to minimal to balance the data-to-ink ratio.\nYour subtitle should be the data’s source: County Health Rankings, University of Wisconsin Population Health Institute\n\nggplot(ma_county_health_2021, \n       aes(x = Name, y = HIV_prevalence_raw_value)) +\n  geom_col() +\n  coord_flip() +\n  labs(title = \"Health Measures by County, Massachusetts, 2021\",\n       x = \"County Name\", \n       y = \"HIV Prevalence Measure\",\n       subtitle = \"County Health Rankings, University of Wisconsin Population Health Institute\") +\n  theme_minimal()\n\n\n\n\n\nWhat kinds of disparities in HIV prevalence are indicated here? What social and historical conditions might explain these disparities?"
  },
  {
    "objectID": "lab2.html#exercise-2-reference-the-ggplot-cheatsheet-to-add-a-subtitle-to-this-plot-same-as-above-to-set-the-theme-to-minimal-and-add-a-trend-line.",
    "href": "lab2.html#exercise-2-reference-the-ggplot-cheatsheet-to-add-a-subtitle-to-this-plot-same-as-above-to-set-the-theme-to-minimal-and-add-a-trend-line.",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Exercise 2: Reference the ggplot() cheatsheet to add a subtitle to this plot (same as above), to set the theme to minimal, and add a trend line.",
    "text": "Exercise 2: Reference the ggplot() cheatsheet to add a subtitle to this plot (same as above), to set the theme to minimal, and add a trend line.\n\n\n\n\n\n\nHint\n\n\n\nThis line adds smoothed conditional means, or a regression line, to the plot.\n\n\n\nggplot(ma_county_health_2021, \n       aes(x = Food_insecurity_raw_value, y = Child_mortality_raw_value)) +\n  geom_point() +\n  labs(title = \"Health Measures by County, Massachusetts, 2021\",\n       x = \"Food Insecurity Measure\", \n       y = \"Child Mortality Measure\",\n       subtitle = \"County Health Rankings, University of Wisconsin Population Health Institute\") +\n  theme_minimal() +\n  geom_smooth()\n\nWarning: Removed 2 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 2 rows containing missing values (geom_point).\n\n\n\n\n\n\nReflection: What kind of relationship between food security and child mortality do we see here?"
  },
  {
    "objectID": "lab2.html#exercise-3-divide-this-plot-by-state-using-facet_wrap.",
    "href": "lab2.html#exercise-3-divide-this-plot-by-state-using-facet_wrap.",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Exercise 3: Divide this plot by state using facet_wrap().",
    "text": "Exercise 3: Divide this plot by state using facet_wrap().\n\nggplot(ne_county_health_2021, \n       aes(x = Unemployment_raw_value, y = Uninsured_raw_value)) +\n  geom_point() +\n  geom_smooth() +\n  labs(title = \"Health Measures by County, New England, 2021\",\n       x = \"Unemployment Measure\", \n       y = \"Uninsured Measure\",\n       subtitle = \"County Health Rankings, University of Wisconsin Population Health Institute\") +\n  theme_minimal() +\n  facet_wrap(vars(State_Abbreviation))\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\n\n\nReflection: In which states do counties have the highest uninsured measures? In which states do counties have the lowest?"
  },
  {
    "objectID": "lab2.html#exercise-4-adding-a-size-aesthetic-to-plot",
    "href": "lab2.html#exercise-4-adding-a-size-aesthetic-to-plot",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Exercise 4: Adding a Size Aesthetic to Plot",
    "text": "Exercise 4: Adding a Size Aesthetic to Plot\nReference the ggplot() cheatsheet to adjust the following plot in these ways:\n\nSet the size of the point to the median income value\nChange the label for the legend (hint: you will supply the name of the aesthetic argument in the labs() function)\nSet the legend’s position to the bottom of the plot\n\n\nggplot(ne_county_health_2021, \n       aes(x = Poor_physical_health_days_raw_value, \n           y = Life_expectancy_raw_value, \n           size = Median_household_income_raw_value)) +\n  geom_point(alpha = 0.5) +\n  labs(title = \"Health Measures by County, New England, 2021\",\n       x = \"Average number of physically unhealthy days reported in past 30 days (age-adjusted)\", \n       y = \"Life Expectancy\", \n       size = \"Median Household Income\",\n       subtitle = \"County Health Rankings, University of Wisconsin Population Health Institute\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_size_continuous(labels = scales::comma)\n\n\n\n\n\nBonus!: See if you can figure out how to convert the income values in the legend from scientific to comma notation. (Hint: You might reference how we converted decimals to percentages along our axes in the last lab.) Reflection: What does this plot tell us that we couldn’t see if we only considered the variables on the x and y axis? # Adding Color to Plots\n\nWe’re going to use the RBrewer package to add color to these plots. To start, enter the following in your Console to check out this package’s color palettes:\ndisplay.brewer.all(colorblindFriendly = TRUE)\nThe first set is sequential, the second is qualitative, and the third is divergent."
  },
  {
    "objectID": "lab2.html#exercise-5-add-a-sequential-color-aesthetic-to-plot",
    "href": "lab2.html#exercise-5-add-a-sequential-color-aesthetic-to-plot",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Exercise 5: Add a Sequential Color Aesthetic to Plot",
    "text": "Exercise 5: Add a Sequential Color Aesthetic to Plot\nCopy and paste the plot you created in Exercise 4 into the code snippet below. Change the size argument in your aes() function to col (short for color). Be sure to also change this argument in your labs() function to update your legend label. Finally, append the following argument to the end of your plot:\nscale_color_distiller(palette = \"Greens\", labels = scales::comma)\n\nggplot(ne_county_health_2021, \n       aes(x = Poor_physical_health_days_raw_value, \n           y = Life_expectancy_raw_value, \n           col = Median_household_income_raw_value)) +\n  geom_point() +\n  labs(title = \"Health Measures by County, New England, 2021\",\n       x = \"Average number of physically unhealthy days reported in past 30 days (age-adjusted)\", \n       y = \"Life Expectancy\", \n       col = \"Median Household Income\",\n       subtitle = \"County Health Rankings, University of Wisconsin Population Health Institute\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_color_distiller(palette = \"Greens\", labels = scales::comma)\n\n\n\n\n\nWhy do we use a sequential pattern here instead of a divergent pattern? Because Median Household Income is a continuous variable that starts at 0 and increases in one direction. There’s not a middle or neutral value that we are comparing incomes around. We should only use divergent color palettes in cases when we are comparing distances in two directions from some middle point. For instance, maybe I would use a divergent palette to visualize temperatures above or below freezing. ## Exercise 6: Add a Qualitative Color Aesthetic to Plot\n\nCopy and paste the plot we created above into the code snippet below. Instead of coloring the points by Median Household Income, color them by state. Be sure to update your legend title. Finally, convert the sequential palette into a categorical palette. To determine how to do this, direct your attention to the Color and Fill Scales section of the ggplot() cheatsheet. How should the word “distiller” be edited in this function call in order create a palette with a discrete set of colors? Once you determine this, update the function call and the palette to “Set2.”\nYou don’t need to change the word “color” to “fill” when adjusting the scale function name. We apply the color argument to points and lines, and the fill argument to bars and shapes. This is a point plot, so we will stick with color!\n\nggplot(ne_county_health_2021, \n       aes(x = Poor_physical_health_days_raw_value, \n           y = Life_expectancy_raw_value, \n           col = State_Abbreviation)) +\n  geom_point() +\n  labs(title = \"Health Measures by County, New England, 2021\",\n       x = \"Average number of physically unhealthy days reported in past 30 days (age-adjusted)\", \n       y = \"Life Expectancy\", \n       col = \"State\",\n       subtitle = \"County Health Rankings, University of Wisconsin Population Health Institute\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\nReflection: To what extent can you identify disparities in life expectancies and unhealthy days across states? Which states tend to be higher and which lower? ## Exercise 7: Facet the Plot by State\n\nCopy and paste the plot you created above into the code snippet below. Remove the col aesthetic and all labels/legends and then facet the plot by state.\n\nggplot(ne_county_health_2021, \n       aes(x = Poor_physical_health_days_raw_value, \n           y = Life_expectancy_raw_value)) +\n  geom_point() +\n  labs(title = \"Health Measures by County, New England, 2021\",\n       x = \"Average number of physically unhealthy days reported in past 30 days (age-adjusted)\", \n       y = \"Life Expectancy\", \n       subtitle = \"County Health Rankings, University of Wisconsin Population Health Institute\") +\n  theme_minimal() +\n  facet_wrap(vars(State_Abbreviation))\n\n\n\n\n\nReflection: Which of the plots you created in the last two exercises better enables you to compare health disparities across states?\n# Overplotting\n\nThe final topic for today’s lab is overplotting - when we have so much overlapping data represented on a plot that it becomes difficult to draw any conclusions from it. Run the code below to see an example of this\n\nggplot(county_health_2021, \n       aes(x = Children_in_poverty_raw_value, \n           y = Math_scores_raw_value, \n           col = Broadband_access_raw_value)) +\n  geom_point() +\n  labs(title = \"Health Measures by County, US, 2021\",\n       x = \"Children in Poverty Measure\", \n       y = \"Math Scores Measure\",\n       subtitle = \"County Health Rankings, University of Wisconsin Population Health Institute\", \n       col = \"Broadband Access Measure\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_color_distiller(palette = \"Greens\", labels = scales::comma)\n\nWarning: Removed 448 rows containing missing values (geom_point).\n\n\n\n\n\nFortunately, there are some adjustments we can make to deal with overplotting. First, we can adjust the transparency of the shapes on our plot by setting the alpha argument in our geom function to a number between 1 and 0. We can also reduce the size of the shapes on our plot by setting the size argument in our geom function to a number between 1 and 0."
  },
  {
    "objectID": "lab2.html#exercise-8-adjust-the-alpha-and-size-of-points-on-plot",
    "href": "lab2.html#exercise-8-adjust-the-alpha-and-size-of-points-on-plot",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Exercise 8: Adjust the Alpha and Size of Points on Plot",
    "text": "Exercise 8: Adjust the Alpha and Size of Points on Plot\nCopy and paste the plot I created above into the code snippet below. In the geom_point() function, set the alpha to 0.75 and the size to 0.5. Notice the adjustments to the plot as you update these attributes.\n\nggplot(county_health_2021, \n       aes(x = Children_in_poverty_raw_value, \n           y = Math_scores_raw_value, \n           col = Broadband_access_raw_value)) +\n  geom_point(alpha = 0.75, size = 0.5) +\n  labs(title = \"Health Measures by County, US, 2021\",\n       x = \"Children in Poverty Measure\", \n       y = \"Math Scores Measure\",\n       subtitle = \"County Health Rankings, University of Wisconsin Population Health Institute\", \n       col = \"Broadband Access Measure\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_color_distiller(palette = \"Greens\", labels = scales::comma)\n\nWarning: Removed 448 rows containing missing values (geom_point).\n\n\n\n\n\n\nReflection: What conclusions would you draw from this plot? Sometimes, overplotting can also be an issue when we are plotting points discretely. Run the code below and review the plot.\n\n\nggplot(ne_county_health_2021, \n       aes(x = State_Abbreviation, \n           y = Flu_vaccinations_raw_value)) +\n  geom_point() +\n  labs(title = \"Health Measures by County, New England, 2021\",\n       x = \"State\", \n       y = \"Flu Vaccination Measure\",\n       subtitle = \"County Health Rankings, University of Wisconsin Population Health Institute\") +\n  theme_minimal()\n\n\n\n\nWhile it may look fine at first glance, it turns out that some counties have the exact same flu vaccination measure and are overlapping each other on that plot. We could add an alpha argument to see where there are overlaps.\n\nggplot(ne_county_health_2021, \n       aes(x = State_Abbreviation, \n           y = Flu_vaccinations_raw_value)) +\n  geom_point(alpha = 0.35) +\n  labs(title = \"Health Measures by County, New England, 2021\",\n       x = \"State\", \n       y = \"Flu Vaccination Measure\",\n       subtitle = \"County Health Rankings, University of Wisconsin Population Health Institute\") +\n  theme_minimal()\n\n\n\n\nHowever, in this case, to be sure that every county is represented on the plot, a better option, would be to add jitter to the plot. Jitter offsets the points from their original position slightly so that we can make out points that overlap. ## Exercise 9: Add jitter to the Plot\nCopy the plot that I created above into the code snippet below. Using the cheatsheet, find the geom function for creating a jittered plot. Set the width of the jitter to 0.15\n\nggplot(ne_county_health_2021, \n       aes(x = State_Abbreviation, \n           y = Flu_vaccinations_raw_value)) +\n  geom_jitter(width = 0.15) +\n  labs(title = \"Health Measures by County, New England, 2021\",\n       x = \"State\", \n       y = \"Flu Vaccination Measure\",\n       subtitle = \"County Health Rankings, University of Wisconsin Population Health Institute\") +\n  theme_minimal()\n\n\n\n\n\nReflection: Which New England states have counties with the highest flu vaccination rates? In which states are there greater disparities in vaccination rates across counties?\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\n…"
  },
  {
    "objectID": "lab2.html#national-bridge-inventory-dataset",
    "href": "lab2.html#national-bridge-inventory-dataset",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "National Bridge Inventory Dataset",
    "text": "National Bridge Inventory Dataset\nEvery year the U.S. Federal Highway Administration publishes a dataset listing every federally-regulated bridge and tunnel in the U.S., along with its location, design features, operational conditions, and inspection ratings. The data is used to review the safety of these transportation infrastructures for the traveling public. As you can imagine, for politicians promising to the improve the state of transportation infrastructure, this dataset is integral to determining where to allocate improvement funds:\n\n\nToday, we are going to look at a subset of 2022 NBI data for Massachusetts and for Hampshire County, MA, and we are going to focus solely on highway bridges (excluding pedestrian and railroad bridges). We’re going to look at what kinds of variables might contribute to poor bridge conditions, where there are poor bridge conditions, and which entities are responsible for maintaining them. The data documentation for this dataset is quite thick, so I will provide you with a data dictionary for today.\n\n\n\n\n\n\n\nVARIABLE NAME\nDESCRIPTION\n\n\n\n\nSTRUCTURE_NUMBER_008\nUnique ID for the bridge\n\n\nCOUNTY_CODE_003_L\nName of the county where the bridge is located\n\n\nROUTE_PREFIX_005B_L\nRoute signing prefix for the inventory route\n\n\nMAINTENANCE_021_L\nThe actual name(s) of the agency(s) responsible for the maintenance of the structure\n\n\nYEAR_BUILT_027\nThe year of construction of the structure\n\n\nADT_029\nThe average daily traffic volume for the inventory route based on most recent available data\n\n\nSTRUCTURE_KIND_043A_L\nThe kind of material and/or design of the structure\n\n\nSTRUCTURAL_EVAL_067\nA rating of the structural evaluation of the bridge based on inspections of its main structures, substructures, and/or its load ratings\n\n\nBRIDGE_IMP_COST_094\nEstimated costs for bridge improvements"
  },
  {
    "objectID": "lab2.html#visualization-aesthetics",
    "href": "lab2.html#visualization-aesthetics",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Visualization Aesthetics",
    "text": "Visualization Aesthetics\nThe visual cues that we select to display on a plot largely depend on the kind of data that we have available. Last week we discussed the differences between categorical variables and numeric variables. Certain types of visual cues are more suited for representing categorical variables, while other types of visual cues are more suited for representing numeric variables. For instance, we wouldn’t use different shapes to represent different numeric values because there is no obvious ordering to a group of shapes (e.g. a triangle isn’t necessarily larger or greater than a circle; they’re just different). Because of this, shapes are much more appropriate for representing nominal categorical variables. Size is a much more effective visual cue for numeric variables because size can increase as the values in the data increase.\n\n\n\n\n\n\n\n\nCue\nEffective for what kinds of variables?\nExample where applied?\n\n\n\n\nShape\nCategorical\nPoints on scatterplots\n\n\nSize\nNumeric\nPoints on scatterplots\n\n\nArea\nNumeric\nBars in bar plots\n\n\nColor\nCategorical (Qualitative palette)\nNumeric (Sequential/Divergent)\nPoints on scatterplots;\nBars in bar plots;\nLines in a line plot\n\n\nPosition\nCategorical; Numeric\nPoints on scatterplots\n\n\nAngle\nNumeric\nSlices in pie chart\n\n\n\nYou’ll remember from lecture that we map variables onto these visual cues via the aesthetic function (aes()) in ggplot().\n\n\n\n\n\n\nTip\n\n\n\nYou may wish to reference this ggplot cheatsheet when completing this lab. Note how this cheatsheet is organized. There are headings for things like:\n\nBasics\nGeom functions\nScales\nCoordinate systems\n\nThe first tip to find what you’re looking for is to consider what heading the graphic element will likely fall under. Also note that, for many functions on this cheatsheet:\n\nan image is provided for how the plot will transform,\nthe function and its arguments are referenced in bold, and\na text description of what will happen when you apply the function is provided\n\nFinally note that most of these functions listed on this cheatsheet are appended to the ggplot() object with a + sign.\n\n\n\nPosition\nSince today we will be working with scatterplots, let’s start by talking about position. When we refer to position, we refer to the location of a point on a Cartesian plane (i.e. its position on an x-axis and its position on a y-axis). We can create scatterplots by mapping a variable in our dataset onto the x-axis and another variable onto the y-axis (aes(x = VARIABLE_NAME, y = VARIABLE_NAME)). Let’s take a look at what happens when we map the year a bridge was built onto the x aesthetic and the bridge’s structural evaluation onto the y aesthetic for all Hampshire County Massachusetts highway bridges.\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point() +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal()\n\n\n\n\nEach point on the plot corresponds to one observation (row) in the dataset. Since each row in this dataset is one Hampshire County, MA highway bridge, each point on this plot also represents one Hampshire County, MA highway bridge. The position of the point indicates to us the year that bridge was built and its structural evaluation. Zooming out to look at all of this data we can see that newer bridges tend to have higher structural evaluations.\n\nOverplotting\nYou’ll notice that parts of this plot can be challenging to read because some points overlap each other making it hard to distinguish one from the next. This is an example of overplotting, and there are a number of strategies we can take to addressing it. Most of these strategies involve revising attributes of the points on the plot (e.g. the size, transparency, and position of the points).\nNote that attributes are different than aesthetics. Recall that when we assign aesthetics in a plot, we are adjusting the visual cues on a plot according to the values in a variable. The visual cues will be different based on the values in that variable (e.g. size will be greater with greater values). On the other hand, when we adjust attributes, we are adjusting the visual cues on a plot in a fixed way (i.e. every point will be styled in the same way regardless of its value). Because of this attributes are assigned outside of the aes() (aesthetic) function. Let’s adjust the plot we created above by reducing the size of every point (size =), reducing the transparency of every point (alpha =), and adding \"jitter\" to the plot (position=). Recall that alpha is assigned between 0 and 1: 0 being transparent and 1 being opaque. Jitter means that we add a bit of random noise to the points on the plot in order to prevent overlapping points.\n\nQuestion\n\nIn the code below, adjust the size to 2 and the alpha to 0.5. Set the position to \"jitter\". Note how this changes the plot.\n\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(size = 1, alpha = 1, position = \"identity\") +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nColor\nColor can be used on plots to either distinguish between discrete values in a categorical variable or to represent the range of values in numeric variable. We use different kinds of color palettes for each of these scenarios. Palettes refer to a range of colors. We can have palettes with discrete colors (e.g. red, orange, and blue) or palettes with a gradient of colors (e.g. lightest red to darkest red).\n\n\n\n\n\n\n\n\nPalette\nEffective for what kinds of variables?\nExample\n\n\n\n\nQualitative\nCategorical\nRed, yellow, blue\n\n\nSequential\nNumeric\nLight blue to dark blue\n\n\nDivergent\nNumeric, extending in two directions (e.g. >1 and <1)\nBlue to purple to red\n\n\n\n\nQuestion\n\nIn the plot below, color the points on the plot with a qualitative palette by setting col= to the categorical variable ROUTE_PREFIX_005B_L. Remember that, in this case, color is an aesthetic, so this must be added inside of the aes() function. Add a label for the legend by setting col= in the labs() function to a phrase that describes the color variable.\n\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       col = \"ADD LEGED LABEL HERE\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nQuestion\n\nCopy the completed plot from the last exercise below but swap out the variable you mapped to the color aesthetic with a numeric variable from the dataset. Be sure to also adjust the legend label. What kind of palette gets created? What new information do you gain about Hampshire County bridges from the plot? Note your interpretations in your quarto file.\n\n\n\n\n\n\n\nShape\nA different way to differentiate data on a plot is to map the shape aesthetic onto the plot. In this case, rather than all observations in the dataset appearing as points on a plot, observations will appear as different shapes based on their associated values in a categorical variable.\n\nQuestion\n\nIn the plot below, assign the shape of the data on the plot by setting shape= to the categorical variable STRUCTURE_KIND_043A_L. Remember that, in this case, shape is an aesthetic, so this must be added inside of the aes() function. Add a label for the legend by setting shape= in the labs() function to a phrase that describes the shape variable.\n\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       shape = \"ADD LEGED LABEL HERE\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nSize\nWhile we might map a categorical variable onto the the shape aesthetic, we can alternatively map a numeric variable onto the size aesthetic. For instance, note what we learn when we map the variable for average daily traffic onto the size aesthetic below.\n\nnbi_hampshire %>%\n  ggplot(aes(x = ROUTE_PREFIX_005B_L, \n             y = STRUCTURAL_EVAL_067, \n             size = ADT_029)) +\n  geom_point(alpha = 0.5, position = \"jitter\") +\n  coord_flip() +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Route Prefix\", \n       y = \"Structural Evaluation\",\n       size = \"Average Daily Traffic\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote how I moved my legend position to the bottom using + theme(legend.position = \"bottom\") in the code above.\n\n\n\nQuestion\n\nCopy one of your plots from above, and inside the aes() function, set size= to a numeric variable not already represented on the plot. Keep in mind that you’ll need to remove the size= attribute from geom_point() for the points to size correctly. Be sure to also adjust the legend labels. What new information do you gain about Hampshire County bridges from the plot?"
  },
  {
    "objectID": "lab2.html#scales",
    "href": "lab2.html#scales",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Scales",
    "text": "Scales\nWhen we map a variable onto an aesthetic, we are only indicating that the variable should be mapped. We are not indicating how the variable should be mapped. In order to indicate how we want a variable mapped to an aesthetic, we can adjust its scales. Scales are adjusted by tacking the following onto a ggplot() object: + scale_<aesthetic>_<type>(). For instance, let’s say that I wanted to adjust the scale of my x-axis to a log scale. I would attached + scale_x_log10() to my ggplot() object.\n\n\n\n\n\n\n\n\nScale\nDescription\nExample\n\n\n\n\nContinuous\nNumeric values are mapped along a continuum\n+ scale_y_continuous()\n\n\nDiscrete\nCategorical values are mapped into discrete buckets\n+ scale_color_discrete()\n\n\nBinned\nNumeric values are mapped into discrete bins\n+ scale_size_binned()\n\n\nLog\nNumeric values are mapped logarthmically\n+ scale_y_log10()\n\n\nDate-Time\nNumeric values are mapped along a timeline\n+ scale_x_datetime()\n\n\n\nLet’s talk about how we would adjust the scales for each of the aesthetics we’ve covered so far.\n\nPosition\nWe can adjust the scale of our x and y-axes by adding + scale_x_<type>() or + scale_y_<type>() to our plots. Note what happens when we attempt to create a scatterplot that shows the relationship between the year a bridge was built and the bridge improvement costs for all MA highway bridges.\n\nnbi_ma %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = BRIDGE_IMP_COST_094)) +\n  geom_point(size = 1, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Bridge Improvement Costs\") +\n  theme_minimal() + \n  scale_y_continuous(labels = scales::comma)\n\nWarning: Removed 1 rows containing missing values (geom_point).\n\n\n\n\n\nDue to huge disparities in costs for bridge improvements, this plot is difficult to interpret. Most bridge improvement costs are under $10,000,000, but with at least one bridge with costs just under a $1,000,000,000, the vast majority of the points on the plot appear at the very bottom of the y-axis scale and are largely indiscernible from one another. This is a case when it makes sense to apply a log scale to the y-axis.\n\nQuestion\n\nIn the plot below, change the y-axis scale from continuous to log10. You might reference the formula specified above or reference the ggplot cheatsheet for help. Note that your y-axis will appear in scientific notation. Convert it to comma notation by adding: labels = scales::comma as an argument in the scale function call.\n\n\nnbi_ma %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = BRIDGE_IMP_COST_094)) +\n  geom_point(size = 1, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Bridge Improvement Costs\") +\n  theme_minimal() + \n  scale_y_continuous(labels = scales::comma)\n\nWarning: Removed 1 rows containing missing values (geom_point).\n\n\n\n\n\n\n\n\n\n\nSometimes I might wish to group certain numeric values into bins on a plot. For instance, let’s say I just want to see how many bridges are structurally deficient in comparison to bridges that are operationally sound. Typically a bridge is considered structurally deficient when it scores 4 or lower. So I want to group the numeric values in STRUCTURAL_EVAL_067 into two bins: 0-4 and 4-9. To do this, I will set the scale to: + scale_y_binned() and add an argument to establish the bin breaks: breaks = c(4, 9) as well as an argument to label the bin breaks: labels = c(\"Structurally Deficent\", \"No Deficiencies\"). Check out what happens to the y-axis scale when I do this below.\n\nnbi_ma %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(size = 1, alpha = 0.2, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal() + \n  scale_y_binned(breaks = c(4, 9), labels = c(\"Structurally Deficent\", \"No Deficiencies\"))\n\n\n\n\n\n\n\nQuestion\n\nThe Interstate Highway Act of 1956 (signed under Dwight D. Eisenhower) established the U.S.’s interstate highway system and is considered the country’s largest public works project. Adjust the x-axis scale of the plot below to help tell this story. Specifically, bin years into before 1954 (when Eisenhower was elected) and 1954-2021. Set the label at 1954 to “Eisenhower Elected” and the label at 2021 to “Today.”\n\n\nnbi_ma %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = ROUTE_PREFIX_005B_L)) +\n  geom_point(size = 1, alpha = 0.2, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Route Prefix\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nColor\nWe’ve already talked about how both categorical and numeric variables can be mapped to the color aesthetic. However, sometimes, we want to be able to further customize which colors should appear on the plot and how they appear on the plot. The RColorBrewer package, which you installed earlier in the lab includes a number of palettes for coloring points on a plot. Check them out below:\n\nRColorBrewer::display.brewer.all() \n\n\n\n\nNote how the first set of palettes is sequential, the second set is categorical, and the third set is divergent. We can assign these palettes to our plots using one of two functions: + scale_color_brewer() for categorical data and + scale_color_distiller() for numeric data. Within this function, we can set the argument palette equal to one of the palettes specified in the image above.\n\nQuestion\n\nAdd `+ scale_color_brewer()` to the ggplot() object below and assign a categorical palette. Refer to the ggplot cheatsheet or the help pages for help with formatting the function call.\n\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067,\n             col = ROUTE_PREFIX_005B_L)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, Massachusetts, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       col = \"Route Prefix\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nQuestion\n\nAdd `+ scale_color_distiller()` to the ggplot() object below and assign a sequential palette. By default the colors will be ordered from darkest to lightest. Add an argument to the function call to reverse the direction of the colors. Refer to the ggplot cheatsheet or the help pages for help with formatting the function call.\n\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067,\n             col = ADT_029)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       col = \"Average Daily Traffic\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nSize and Shape\nThe following represents the values associated with ggplot() point shapes.\n\nWe can manually assign shapes using + scale_shape_manual(values = c(<shape_values>)).\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067, \n             shape = MAINTENANCE_021_L)) +\n  geom_point(alpha = 0.5, size = 2, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       shape = \"Maintainer\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",\n        legend.box=\"vertical\") +\n  guides(shape = guide_legend(nrow = 3, byrow = TRUE)) +\n  scale_shape_manual(values = c(15:17))\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that the guides() function above allows us to wrap the legend into three rows!\n\n\nAlso note how we bin point sizes just like we binned values on the x and y axis.\n\nnbi_hampshire %>%\n  ggplot(aes(x = ROUTE_PREFIX_005B_L, \n             y = STRUCTURAL_EVAL_067, \n             size = ADT_029)) +\n  geom_point(alpha = 0.5, position = \"jitter\") +\n  coord_flip() +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Route Prefix\", \n       y = \"Structural Evaluation\",\n       size = \"Average Daily Traffic\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_size_binned(breaks = c(0, 500, 5000, 100000))"
  },
  {
    "objectID": "lab2.html#faceting",
    "href": "lab2.html#faceting",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Faceting",
    "text": "Faceting\nFaceting involves breaking out a single plot into multiple smaller plots based on the value in a variable. Faceting is very helpful when we have a categorical variable with many distinct values. If we tried to color by that variable, the colors would likely be indistinguishable from one another. For instance, check out what happens when we try to color by the COUNTY_CODE_003_L variable below.\n\nnbi_ma %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067, \n             col = COUNTY_CODE_003_L)) +\n  geom_point(alpha = 0.5, size = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       col = \"County\") +\n  theme_minimal()\n\n\n\n\nThere are so many counties that it’s extremely challenging to distinguish between colors on this plot. In this case, instead of using a color aesthetic, we facet the plot by adding + facet_wrap(vars(COUNTY_CODE_003_L)) to the ggplot() object. Check out what happens when we do that below.\n\nnbi_ma %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(alpha = 0.1, size = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal() +\n  facet_wrap(vars(COUNTY_CODE_003_L))\n\n\n\n\n\nQuestion\n\nFacet the plot below by STRUCTURE_KIND_043A_L to see the degree of structural deficiencies for different kinds of highway bridges of different ages in Massachusetts.\n\n\nnbi_ma %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(alpha = 0.1, size = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nIn response to concerns regarding domestic security, some stakeholders have questioned whether the Federal Highway Administration should be publicly disclosing information about the location and deficiencies of U.S. bridges and other forms of transportation infrastructure. What do you see as the value of making this data available to the public? Does its value outweigh national security concerns? Are these concerns legitimate? Share your ideas on our sds-192-discussions Slack channel."
  },
  {
    "objectID": "learning-styles.html",
    "href": "learning-styles.html",
    "title": "Course Study Planning",
    "section": "",
    "text": "How do I study for this class?\n\n… and in the past I’ve sometimes struggled to answer that question because everyone learns differently. With this in mind, I’ve developed the following chart to help students figure out how best to study for this class given their personal learning style.\nThis chart is modeled based on research indexing different kinds of learning styles across four dimensions (active/reflective, sensing/intuitive, visual/verbal, and sequential/global). This model was formulated by Richard M. Felder and Linda K. Silverman. Later, Richard M. Felder and Barbara A. Soloman developed an assessment tool for learning about a respondents’ learning preferences.\nAs the first part of this assignment, you should take this assessment here, and review your results against the descriptions of the dimensions here.\nOnce you’ve reviewed your results, you can reference the following chart to see what I recommend in terms of your studying. Importantly, you should check out the study tips along all four dimensions (i.e. identify where you stand in each row of the table below). In the Study Plan assignment posted in Moodle, reflect on which study tips seem like they will be most supportive to you in 200-300 words.\n\n\n\n\n\n\nTip\n\n\n\nIt’s important to note that these are just suggestions for directing your study habits. As with all indexes, this one has its limits. In writing up your response, focus on those suggestions that speak to you! Later in the semester, you will have an opportunity to reflect on how studying is going and adjust as necessary. Remember that I’m here to help you figure out how best to navigate this course!\n\n\n\n\n\nDimension\nStudy Tips\nDimension\nStudy Tips\n\n\n\n\nMore Active (learn by doing)\n\nBe sure to complete all course lab assignments. Active learners will get a lot out of these!\nWork through practice problems in the course text.\nAsk tutors in the Spinelli Center if they can help you work through practice problems.\nBonus: Find online tutorials related to the course standards, and see if you can follow along to complete them in R.\nBonus: Reach out to me for some good practice datasets and see if you can apply the coding skills we are learning to those datasets!\n\nMore Reflective (learn by thinking)\n\nWhile reviewing reading materials and lecture notes, see if you can stop and write short paragraphs summarizing a course concept or what is happening when we use certain functions in R.\nAsk tutors in the Spinelli Center if they can help you talk out a particular concept.\nFind a friend and see if you can explain a challenging course concept to them. …or see if you can explain the concept to me in office hours!\nForm a study group and build in time to talk through challenging course concepts with each other.\n\n\n\nSensing Preference (preference for learning facts)\n\nWhen reading the course texts, you may pay extra attention to the sections where the authors walk through real world examples where certain data science tools would be applied.\nAsk tutors in the Spinelli Center if they can walk you through a concept by showing a real world example of where it would be applied.\nIf course material is feeling too abstract, ask me to give an example where it may be applied in the real world in class or in office hours.\n\nIntuitive Preference (preference for learning theories and relationships)\n\nWhen reading the course texts, you may pay extra attention to the sections where the authors explain the meaning, rationales, and theories behind course concepts and tools and how they relate to one another.\nAsk tutors in the Spinelli Center if they can walk you through a concept by explaining its meaning and function and how it gets applied.\nIf course material is feeling too rigid or mechanical, ask me to explain the theories behind certain functions in class or in office hours.\n\n\n\nMore Visual (learn by seeing)\n\nBe sure to pay close attention to diagrams and images in course reading assignments and lecture slides.\nIf you’re not following along with a concept in lecture, ask me if we can pause and draw it out on the white board.\nAsk tutors in the Spinelli Center to help you draw a diagram or image to explain a challenging concept.\nAs you are reviewing lecture notes and readings, see if you can summarize a challenging concept by drawing a diagram or schematic of the concept. For instance, you might draw what happens to a data frame when you applying a certain wrangling verb.\nThere are often really helpful icon diagrams in R cheatsheets. I will do my best to always link relevant cheatsheets in course lab assignments.\n\nMore Verbal (learn by hearing and saying)\n\nWatch my lab recaps in Perusall and ask questions when you are struggling to understand a concept from the lab.\nIf you’re not following along with a concept in lecture, ask me if we can pause and explain the concept in a different way.\nAsk tutors in the Spinelli Center if they can explain a concept in a different way.\nForm a study group so that you can listen to other students explain course concepts.\nFind YouTube videos that explain course concepts that you are struggling with.\nBonus: DataCamp offers a number of courses on topics we will cover in the course, and students can create a free account! You may wish to work through a DataCamp course related to a concept you are struggling with.\n\n\n\nMore Sequential (learn step-by-step)\n\nAsk tutors in the Spinelli Center if they can walk you step-by-step through a practice problem.\nIdentify practice problems that you struggled with in labs or in class, and write out step-by-step how you would tackle that problem.\nReview quiz questions you got incorrect by working with a classmate to write out the steps to the correct answer.\nAsk me to explain the logic behind the steps we take to answer a certain problem in class or in office hours.\n\nMore Global (learn big picture)\n\nAsk tutors in the Spinelli Center if they can help you draw connections between different course concepts and ideas.\nCreate concept maps that show the relationships between different course concepts.\nReview quiz questions you got incorrect by working with a classmate to relate the question to another question you got correct or to another concept that you understand well.\nAsk me to explain how different course concepts and ideas connect to one another in class or in office hours."
  },
  {
    "objectID": "lab4.html",
    "href": "lab4.html",
    "title": "Lab 4: Git and GitHub",
    "section": "",
    "text": "There are many reasons that Git and GitHub are essential infrastructures for collaborative coding projects. For one Git saves snapshots of a code repository at different stages of a project so that we can track how it has changed over time and revert back to an older version if we discover a more recent error. We call this version control. Certain Git features also facilitate many people working on a coding project at once, by providing a number of tools to help prevent collaborators from over-writing each other’s work. These features also make it possible for developers to simultaneously modify, extend, and test components of the code without jeopardizing the project’s current functionality. Further, GitHub supports code publication; by publishing code on GitHub, you contribute to an open access/free software community, enabling others to learn from and build off of your work.\nDespite all of these awesome benefits, there can be a significant learning curve when getting started with Git and GitHub. There are new vocabularies, workflows, and error mitigation strategies to learn when getting started. This lab is designed to help you get acquainted with the concepts behind Git and GitHub, suggested workflows for collaborating on projects in this course, and error resolution strategies.\n\n\n\nCreate, update, and close issues\nBranch a repo\nIssue pull requests\nAddress common push/pull errors\nAddress merge conflicts"
  },
  {
    "objectID": "lab4.html#review-of-key-terms",
    "href": "lab4.html#review-of-key-terms",
    "title": "Lab 4: Git and GitHub",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\nRepo\n\nCollaborative storage space for folders, documents, data, and code\n\nBranch\n\nAn isolated version of a repo that can be modified without affecting the main branch\n\nClone\n\nCreates a copy of a repo stored in a remote space (e.g. GitHub) to your local machine (e.g. your computer)\n\nPull\n\nDownloads the latest version of a repo from remote space (e.g. GitHub) to your local machine (e.g. your computer)\n\nStage\n\nThe process of marking which changes of the code are ready to be saved\n\nCommit\n\nA stored snapshot of a repo at a particular moment in time\n\nPush\n\nUploads commits from your local machine (e.g. your computer) to a remote space (e.g. GitHub)\n\nPull Request\n\nA request for modified code to be integrated with a different branch\n\nMerge\n\nThe process of integrating code modifications from one branch into another branch"
  },
  {
    "objectID": "lab4.html#github-flow",
    "href": "lab4.html#github-flow",
    "title": "Lab 4: Git and GitHub",
    "section": "GitHub Flow",
    "text": "GitHub Flow\nIn my opinion, there are two kinds of workflows for GitHub. There’s the quick and dirty version, and there’s the long and elegant version. Below are the differences between these two workflows (don’t worry about if you don’t understand what the steps mean right now; we will learn all of them in the lab).\n\n\n\n\n\n\n\nQuick and Dirty Version\nLong and Elegant Version\n\n\n\n\n\nPull recent changes from GitHub to local machine.\nMake edits and save them\nStage and commit changes.\nPush changes from local machine to GitHub.\n\nIn the quick and dirty version, all of this occurs in the main branch.\n\nCreate an issue at GitHub.com\nBranch the repo at GitHub.com.\nPull recent changes from GitHub.com to local machine.\nMake edits in the new branch and save them.\nStage and commit changes.\nPush changes from local machine to GitHub.com.\nCreate a pull request at GitHub.com\nAssign a reviewer to review the proposed changes and wait for their approval.\nMerge changes, while also closing the issue and deleting the branch.\n\n\n\n\nTypically I recommend the long and elegant version as it is designed to avoid errors and ensure that collaborators are all on the same page regarding changes to files. However, occasionally when you have to make small, quick changes to a file, and it won’t impact your team mate’s work, it will make more sense to follow the quick and dirty workflow. The goal for today is to get practice in the long and elegant version.\n\nRepo\n\n\n\n\n\n\nExercise 1\n\n\n\nNavigate to _____ to accept the assignment. Enter your project group members.\n\n\nThis will create a GitHub repository called github-lab. You’ll notice that the repository has a few files - README.md, github-practice.Rmd, .gitingore. You’ll also notice in the repo’s right sidebar that your group members are listed as collaborators. This means that you all have access to read and write to this repository.\n\n\nClone\n\n\n\n\n\n\nExercise 2\n\n\n\nAll group members should clone the repo to their RStudio environment. To do so, copy the repo’s URL. Then in RStudio click on File > New Project > Version Control > Git, and enter then pasted the copied URL into the window that appears. Note what you see in the RStudio files pane after cloning the repo.\n\n\nRemember that cloning creates a copy of a remote repo on a local machine. In creating this project, you’ve copied all of the files that make up the github-lab repo at GitHub.com to your local computer. This means that you will find all of the files associated with this repo by navigating to the folder where you created the project on your computer.\nIt’s important to note that this is not just any old folder on your computer though. By cloning, you’ve created a git folder. This means that the folder has been set up in a way where git can track the changes that you make to it over time, and it knows that there is a remote version of the repository somewhere that you might want to keep it consistent with.\n\n\nIssues\n\n\n\n\n\n\nExercise 3\n\n\n\nNavigate back to GitHub, and click on the repo’s Issues tab. Each member of your team should create an issue by clicking the ‘New Issue’ button. Title the issue: “Adding <your name> to the assignment.” Submit the issue, and to the left of the screen, assign the issue to yourself.\n\n\nIssues support project planning by allowing you to track changes you hope to make to your project over time. By assigning issues to certain collaborators on your project team, you can have clear documentation of who is responsible for what.\n\n\n\n\n\n\nTip\n\n\n\nIn my own projects, I use Issues for a number of purposes. Sometimes I use Issues to bugs that I notice in my code that need to be fixed. Other times I use them to track features that I would like to add to my code down the road. Oftentimes, in my public repositories, I encourage others that are using my code to submit issues to ask questions about how something works, to report bugs, or to request features.\n\n\n\n\nBranch\nWhen you first create a repository all of the code will be stored in the main branch of the repository. If you were to think of a project like a tree growing up from the ground, then the main branch would be the like the trunk of the tree. One goal of a branching workflow in GitHub is to keep the most stable and polished versions of code in the main branch. So what do we do in the meantime - when we’re editing code and trying to sort out its bugs, and it’s not quite in that stable and polished state yet? That’s where branching comes in.\nWhen we create a branch of our repo, GitHub creates a separate copy of the repo where you can make changes without impacting what’s in the main branch. Later, once we’re done making changes and things are stable and polished, we will have the opportunity to merge those changes back into the main branch.\n\n\n\n\n\n\n\nTip\n\n\n\nBranching can get pretty wild in GitHub. You can have branches of branches of branches. I don’t recommend this. A good workflow is to create a branch for making specific changes, merge those changes back into main, delete the branch, and then create a new branch for the next batch of changes.\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\nClick on the Code tab on your repo’s page at GitHub.com. Directly below this tab, you will see a dropdown that is currently labeled “main.” This means that you are in the main branch. Each member of your team should click the down arrow, and create a branch by entering their first name into the textbox that appears, and then clicking “Create branch.”\n\n\n\n\n\nPull\nAs of right now, the branches that were created in the previous step only exist on GitHub.com, they don’t exist yet on your local machine. To get these changes to your local machine, you need to Pull the changes. Remember how we said that the super fancy git folder knows that there is a remote version of the repository somewhere that you might want to keep it consistent with? When we pull changes to our local machines, we are basically saying, check that remote version for changes, and then pull them into the repo on my computer.\n\n\n\n\n\n\nExercise 5\n\n\n\nHead back to RStudio. In the Environment pane, you will notice a tab labeled “Git.” It’s important to note that this tab will only appear in projects that are built from super fancy git folders. This is your RStudio command center for Git and GitHub. When you click on this tab, you will see a few buttons in the navigation bar. To pull changes, you should click the blue downward arrow. Click this button to pull the branches created remotely to your local machine.\n\n\n\n\n\nSwitch Branches\nEven though you pulled the new branches to your local machine, you are still currently working in the main branch. Remember that we always want to keep the main branch stable and polished. This is not where we are going to make edits. Instead, you will make edits in the branch that you just created. Later, we will merge those changes back into the main branch.\n\n\n\n\n\n\nExercise 6\n\n\n\nIn the top right hand corner of the Git tab, you will see a dropdown currently set to “main”. Click the downward arrow, and switch to your branch by selecting the appropriate branch.\n\n\n\n\n\nMake Changes\n\n\n\n\n\n\nExercise 7\n\n\n\nOnce in your branch, open github-practice.Rmd from the files pane. Decide within your group who will be Group member 1, 2, 3, and so on. Each group member should edit this file on their own machines by adding their name and only their group name to the appropriate location in the document (line 5, 7, or 9) based on their group member. It’s very important that this be the only section of the document you edit. Save the file by clicking File>Save.\n\n\n\n\nStage\nSometimes we make a changes to a few files, save them, and we’re ready to create a snapshot of our repo (i.e. create a commit) with some of those changes. Remember that creating this snapshot is almost like taking a photo of the repo at this particular moment, allowing us to later go back to that photo to see what the repo looked like in that moment. To let Git know which changes we want to include in that snapshot, we need to stage the files. Staging basically says, “these files are ready to be included in the snapshot.”\n\n\n\n\n\n\nExercise 8\n\n\n\nOnce you save the file you’ll notice in the RStudio Git pane that the file name appears after a blue square labeled “M” (which stands for Modified). This means that the file is ready for staging. Stage the file - indicating that it’s ready for committing - by clicking the checkbox in front of the file name.\n\n\n\n\n\nCommit\nAs we just noted, committing changes basically means taking a snapshot of a repo at a particular moment in time. Commits are given unique hashes - sort of like a unique identifier that enables us to access the snapshot of the repo at a later date. In collaborative projects, it is typically recommended to commit often - after any major changes are made to a file. This ensures that we can eventually go back to look at very specific changes. It’s also important to label commits with descriptive titles so that we can recall what changes within that commit.\nTo help put this into context, think back to our photograph metaphor. Let’s say that we are a photographer assigned to document how a baby develops in the first year of its life. If the photographer only took one photograph when the baby was 1 year old, we wouldn’t have a lot of documentation regarding how the baby developed! …so instead, let’s say that the photographer took a snapshot of the baby after every major milestone - their first laugh, their first solid food, their first crawl, their first word. We would have a lot more to go by when trying to understand how the baby developed. Same goes for committing code often.\nNow let’s say the photographer handed the batch of photos to the parents, and said - “look, here’s how your baby developed over time.” The parents might not remember which photograph was taken after which milestone. …but if the photographer were to label each photograph with things like “baby had first laugh,” the parents would be able to easily go back to specific moments in their baby’s development. This is why we want descriptive commit messages. We want to later be able to go back and scan through what changes were made after each commit.\n\n\n\n\n\n\nExercise 9\n\n\n\nCommit your changes by clicking the ‘Commit’ button in the Git pane. When you click this button, a new window will open showcasing the changes that have been made to the staged file. You should enter a commit message in the window that appears. Remember that commit messages should be descriptive. In this case, something like “added <your-name>’s name” would work. Click commit. Now a snapshot of this version of the code repo has been taken.\n\n\n\n\nPush\nOnly your local machine knows that a change has been made to the code. Remember again how we said that the super fancy git folder knows that there is a remote version of the repository somewhere that you might want to keep it consistent with? Now we want to do the opposite of pulling changes from GitHub to our local machines. Instead, we want to push the changes on our local machines to GitHub.\n\n\n\n\n\n\nExercise 10\n\n\n\nClick the Green upward arrow in the Git pane to push your changes to GitHub.\n\nOnce all group members have pushed their changes, head back over to GitHub. On the main code page, switch between branches and check out the contents of github-practice.Rmd in each branch. What differences do you notice?\n\n\n\n\nPull Request\nNote that now we have a few versions of our repo in separate branches on GitHub.com, and in each of those versions of the repo, the github-practice.Rmd file looks a little bit different. Now that we’ve made our changes and things are stable and polished, we want to move all of those changes into the main branch. To do this, we are going to issue a Pull Request. This is a request that signals to all of our collaborators that we are ready to move our changes back into the main branch.\n\n\n\n\n\n\nExercise 11\n\n\n\nOn your repo’s page in GitHub.com, click the “Pull Requests” tab, and then click the green “New Pull Request” button. You’re requesting to pull the changes from your personal branch into the main branch. This means that the base branch should be main, and the compare branch should be your personal branch.\n\nYou’ll see a screen where you can compare your branch to the main branch. Click the button to “Create Pull Request,” enter a descriptive title of the changes made, and then click “Create Pull Request” again.\n\n\n\n\nReview Pull Requests\nI recommend that you get in the habit of reviewing your collaborator’s work before merging their changes into the main branch. By creating pull requests, we scaffold an opportunity to review each other’s work before fully integrating the changes.\nNow there should be a pull request for all members of your team. Assign one team member to review one other team member’s code. All team members should have one reviewer.\n\n\n\n\n\n\nExercise 12\n\n\n\nOpen your own pull request in GitHub.com, and in the right sidebar, assign the team member responsible for reviewing your changes as a “Reviewer.”\n\nThen navigate to the pull request you are responsible for reviewing. Click on the “Files Changed” tab. Note that the left side of the screen shows the previous version of the file, and the right side of the screen shows the new version of the file. Lines in red have been deleted, and lines in green have been added.\n\nAfter looking through the changes, click the green button “Review changes.” Leave a note for your collaborator, indicating your evaluation of their changes. If everything looks good, check the radio button for “Approve.” If there are issues, check the radio button for “Request Changes.” Then click the button to “Submit Review.”\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf your reviewer requested changes, you should go back to RStudio, and make sure you are in your own branch. Then make the requested changes, save the file, stage the file, commit the changes, and push again. The changes to your file will be tracked in your pull request. After this, you may move on to the Merge step. See here for further options on dismissing or re-requesting reviews.\n\n\n\n\nMerge\nOnce all reviewers have approved changes, we are ready to merge those changes into the main branch. Open each pull request. If everything is good and ready to merge, you will see a green checkmark that says “This branch has no conflicts with the base branch.”\n\n(If you get a message that there are conflicts, call myself or one of the Data Assistants over.)\n\n\n\n\n\n\nExercise 13\n\n\n\nClick the button to “Merge Pull Request”. In the comment box that appears, enter the text “closes #”. When you enter this text, you will see a dropdown of issues and pull requests currently in the repo. Issues will have an icon that appears as a circle with a dot in the center.\n\nSelect the issue associated with this pull request, and then click “Confirm Merge.” This will both merge the changes into the main branch and simultaneously close the issue you opened earlier. Finally, click the button to the delete the branch. Once this has been completed for all pull requests, head back over to the “Code” tab at GitHub.com, and check out github-practice.Rmd. What has happened to the file since merging the code? Navigate to the “Issues” tab. What has happened to the issues since confirming the merge?\n\n\n\n\n\n\n\n\nBefore moving on to the next section...\n\n\n\nYou’ve now deleted branches at GitHub.com that your local machines don’t know have been deleted. Before moving on to the next step, you should navigate back to RStudio and pull these changes to your local machine by clicking the blue downward arrow. To streamline things in the next section of the lab, we are going to work entirely in the main branch (something that I would otherwise not recommend)."
  },
  {
    "objectID": "lab4.html#error-resolution",
    "href": "lab4.html#error-resolution",
    "title": "Lab 4: Git and GitHub",
    "section": "Error Resolution",
    "text": "Error Resolution\nThe workflow presented above seems to work all fine and dandy. …but there are a number of factors that can impede this seamless workflow. In this final section of the lab, we will go over three kinds of errors that you might come across in the workflow above, and talk about how you would resolve them. I can almost guarantee that you will deal with some of these issues when working on your group projects, so I would encourage you to keep this lab handy when engaging in project work.\n\nPush error\nA push error occurs when we make changes to files on our local machines, and go to commit and push them to GitHub.com, but other changes had already been made to the file at GitHub.com that were not yet pulled into our local environments. We get an error because our local repo is inconsistent with the remote repo. To fix this error, we need to pull changes to our local machine, and try committing and pushing again. Let’s replicate this error:\n\n\n\n\n\n\nExercise 14\n\n\n\nOne of your partners should navigate to the GitHub.com repository. Click on the file README.md. Click the pencil icon to edit the file. Replace the text: ADD NAME 1 HERE ADD NAME 2 HERE, and so on with your names. Scroll to the bottom of the page and commit changes noting in the message that your names were added.\nOther partner: Return to RStudio. Do not pull changes yet. Open github-practice.Rmd. On line 40 change the ncol() function to dim(). Save the file. Stage and commit your changes. Click the green upward arrow to push your changes. You should get an error that looks like this.\n\nFollow the steps above to resolve the error.\n\n\nAn easy way to avoid a push error is to always click the blue downward arrow to pull remote changes before starting to edit files on your local machine.\n\n\nPull error\nA pull error occurs when changes have already been made to the same location (the same line number) in both a remote file and a local file, and then we try to pull the changes from the remote repo to our local machines. As far as Git can tell, there are two options for what this line is supposed to look like, and it can’t tell which to prioritize. So Git recommends that, as a first step, we commit the changes that we made locally. It’s basically saying, let’s take a snapshot of your local repo as it looks right now, so that later we can figure out what to do about this conflict.\nIf this seems confusing imagine this: let’s say you write a paper, and you share it with one of your classmates to review. The classmate reads through it, makes suggested changes to the opening sentence, and sends it back you. …but, while your classmate was reviewing the paper, you were getting antsy about the paper deadline and started making your own edits to the paper, including edits to the opening sentence. Now you’re trying to incorporate the changes from your classmate’s review, and you’re not sure what to do about that opening sentence. As a first step, you have two options you can scrap your recent changes (maybe your classmate’s suggestions were better!) or you can save a separate copy of the file with your recent changes and figure out later how to resolve the differences. That’s exactly what we are going to do here:\nTo fix this error, you should stage and commit your local changes and then try pulling again. Let’s replicate this error:\n\n\n\n\n\n\nExercise 15\n\n\n\nOne of your partners should navigate to the GitHub.com repository. Click on the file github-practice.Rmd. Click the pencil icon to edit the file. Replace the code on line 47 with the following: colnames(pioneer_valley_2013).\nScroll to the bottom of the page and commit changes noting in the message how you updated the function.\nOther partner: Return to RStudio. Do not pull changes yet. Open github-practice.Rmd. Replace the code on line 47 with the following: ncol(pioneer_valley_2013)\nSave the file. Click the blue downward button to Pull changes. You should get an error that looks like this.\n\nFollow the steps above to resolve the error.\n\n\n\n\nMerge conflict\nSo now we have these two snapshots of github-practice.Rmd, and they are in conflict with one another. If we try to push our changes back to GitHub.com, Git is not going to know what to do. Should the file at GitHub.com look like the version currently at GitHub.com, should it look like the snapshot that we just commit to our local machines, or should it look like something else entirely?\nLet’s return to the example of trying to incorporate a peer’s edits to a paper that you have recently made changes to. We have to figure out what to do about that opening sentence. Do we want our version, their version, or some combination of the two? This is what it is like to fix a merge conflict.\nTo fix this error, open the file with conflicts and edit the lines with conflict.\n\n\n\n\n\n\nExercise 16\n\n\n\nOne of your partners should try to pull changes by clicking the blue downward arrow in RStudio. You will get an error that looks like this:\n{fig-alt=“This is the window that we see when we get a merge conflict. It says: CONFLICT (content): Merge conflict in README.md}\nTo fix this one your partners should open the file with the conflict. In this case it will be github-practice.Rmd. Scroll to the section of the file with the conflict. It will now look something like this:\n    <<<<<<< HEAD\n\n    ncol(pioneer_valley_2013)\n\n    =======\n\n    colnames(pioneer_valley_2013)\n\n    >>>>>>> ee175895783b64e0e1f696d9456be4c4c7c3f3bf\nThe code following HEAD represents the recent changes you made on your local machine, and the code right before the long string of characters represents the changes that were made in an earlier commit (the long string of characters is the commit hash). Decide what that line should look like and delete all other content. This means you must delete “<<<<<<< HEAD”, “=======”, and “>>>>>>> <long-hash>”, and you likely should delete at least one other line. Save the file, stage the file by clicking the checkbox next to the file in the Git page, and then commit your changes, and push them to GitHub.com.\n\n\n\n\n\n\n\n\nAvoiding Merge Conflicts\n\n\n\nYou may have noticed that the most frustrating merge conflicts tend to emerge when we have two people working on the same line of a repo’s file. The most effective way to avoid merge conflicts is to ensure that collaborators are working on different documents or different lines in a file. One way you might do this when starting to work on your group project is to open a file that you all plan to work on and having one of your team mates section off space of that file for different people to work. It might look something like this:\n\nOnce this change has been made, that group mate should stage, commit, and push the file to GitHub, and all other group mates should pull the change to their local machines.\n\n\n\n\nPath of Least Resistance\nI have been working with GitHub for years, and even to this day, I run into instances where things become so inconsistent between my local machine and the repo at GitHub.com that the fastest way to fix things is just to save local copies of the files that I’ve changed to somewhere else on my machine, delete the super fancy Git folder from its current location, and then re-clone the most up-to-date remote version to my local machine. Then I can figure out how I want to edit the most up-to-date version with my changes. This comic from XKCD captures this widely acknowledged solution beautifully:\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nIncreasingly, when data science researchers publish a paper in a journal, they are making the code they used to reach certain results freely available on GitHub.com for other researchers and the public to review. This is in part a response to the reproducibility crisis that you learned about in SDS 100. What do you see as the social benefits to making the code behind a data science finding publicly available online? What might be some of the social consequences of making this code freely available? How might we mitigate these consequences? Share your ideas on our sds-192-discussions Slack channel."
  },
  {
    "objectID": "lab1-template.html",
    "href": "lab1-template.html",
    "title": "Lab 1: Understanding Datasets",
    "section": "",
    "text": "Install the RScorecard package by entering the following into your Console: install.packages(\"rscorecard\")\nCreate a Scorecard API Key at this link. Shortly after you fill out the form, you will be emailed a key. Copy that key into code chunk below, replacing all of the following text in sc_key(): Sys.getenv(“SCORECARD_KEY”). Be sure to wrap the key in quotation marks.\nDownload the Scorecard Data Dictionary and Technical Documentation for Institution-Level Data Files here.\nRun the code below to the import 2018 Scorecard data for Massachusetts into R. Call me or one of the data assistants over if you get an error.\n\n\nlibrary(tidyverse)\nlibrary(rscorecard)\nsc_key(Sys.getenv(\"SCORECARD_KEY\")) # Replace Sys.getenv(\"SCORECARD_KEY\") here with your API Key in quotation marks\n\nscorecard <- sc_init() %>%\n  sc_year(2018) %>%                 #Note how we are looking at only 2021 data here!\n  sc_filter(stabbr == \"MA\") %>%     #Note how we are looking at only Massachusetts data here!\n  sc_select(unitid, instnm, city, highdeg, control, ugds, adm_rate, costt4_a, costt4_p, pcip27, pctfloan, admcon7, wdraw_orig_yr2_rt, cdr3) %>%\n  sc_get()\n\n\n\n\n\n\n\nExercise 1\n\n\n\nSee if you can identify a unique key for this dataset. Write some lines of code to determine whether the column you’ve identified can act as unique key for the data. Hint: You need to check whether the values in the column ever repeat.\n\n\n\n# Write code to check if values in column are all unique here.\n\n\n\n\n\n\n\nExercise 2\n\n\n\nAt this point, you’ve probably figured out that each row in this dataset is a higher education institution. …but there are many different ways that we can define higher education institutions, and that will impact what gets included and excluded in our data. Referencing the Technical Documentation, locate a definition for the unit of observation in this dataset. What institutions are included, and what institutions are excluded? Summarize a definition below.\n\n\n\n\n\n\n\n\nYour Response\n\n\n\nFill response here.\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\nFor each of the variable names in the scorecard data frame, look up the associated name in the data dictionary. You will need to search for the variable name in the sixth column of the data dictionary (I recommend using Ctrl-F to quickly locate the variable name in the spreadsheet.) Once you’ve found the variable name, reference column 1 to determine what this variable means, and reference columns 7 and 8 to see what possible values will appear in that column.\nIdentify one nominal variable, one ordinal variable, one discrete variable, and one continuous variable in scorecard and list their variable names below. Then uncomment the lines below and use the typeof() function to see how R determined their data types. Did any surprise you?\n\n\n\n\n\n\n\n\nYour Response\n\n\n\n\nNominal variable: _____\nOrdinal variable: _____\nDiscrete variable: _____\nContinuous variable: _____\n\n\n\n\n#typeof(scorecard$_____)\n#typeof(scorecard$_____)\n#typeof(scorecard$_____)\n#typeof(scorecard$_____)\n\n\n\n\n\n\n\nExercise 4\n\n\n\nWrite code below to recode the admcon7 variable and store the results in a new variable in scorecard called admcon7_text. You’ll need to look up the values in the data dictionary. If you’ve done this correctly, running this code should produce a barplot that displays multiple bars.\n\n\n\nscorecard$admcon7_text <-\n  recode(\n    scorecard$admcon7, \n    #Fill replacements here\n    .default = NA_character_\n  )\n\nggplot(scorecard, aes(x = admcon7_text)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\nCalculate the number of missing values in both the costt4_a and the costt4_p columns. Reference the NOTES column in the data dictionary to determine why there are so many NA values in these columns. Add a comment to the code chunk below, explaining the reason for missing values in these columns.\n\n\n\n# Calculate the number of NA values here\n\n# Add comment to explain missing values here"
  },
  {
    "objectID": "lab3-template.html",
    "href": "lab3-template.html",
    "title": "Lab 3: Plotting Frequencies and Distributions",
    "section": "",
    "text": "Tip\n\n\n\nToday’s set-up is a little more complicated than usual, so be sure to take it slow and ask questions as they come up!\n\n\n\nInstall the Spotify R package by entering the following into your Console: install.packages(\"spotifyr\")\nLog-in to Spotify’s Developer Dashboard here. If you have a Spotify account, you can log-in with that account. Otherwise, you should create one.\nClick the ‘Create an App’ button to create an app named “SDS 192 Class Project”. You can indicate that this is a “Project for SDS 192 class”\nClick Edit Settings. Under the heading Redirect URIs copy and paste this URL: http://localhost:1410/, and click Add. Scroll to the bottom of the window and click Save. This is going to allow us to authenticate our Spotify accounts through our local computers.\nClick the Users and Access button. Scroll down to Add New User, and add your name and the email address associated with your Spotify account. Click Add.\nClick “Show Client Secret”. Copy client id and secret below, and then run the code chunk.\n\n\nlibrary(spotifyr)\n# id <- 'FILL CLIENT ID HERE'\n# secret <- 'FILL CLIENT SECRET HERE'\n# Sys.setenv(SPOTIFY_CLIENT_ID = id)\n# Sys.setenv(SPOTIFY_CLIENT_SECRET = secret)\n\n\nBelow replace poiril with your Spotify username. This is the ID that appears in the upper right hand corner when you log into your Spotify account (not your developer account.)\nSearch Spotify for your favorite music genre and select three playlists from the search. Note that this code will only work for playlists, and playlists may be a ways down in the search results.\nWhen you click on a playlist, notice the URL in the navigation bar of your web browser. It should look something like spotify.com/playlist/LONG_STRING_OF_CHARACTERS. Copy the long string of characters at the end of the URL, and paste it into the function below. Your characters should replace the example playlist I’ve added: 7ryj1GwAWYUY36VQd4uXoq.\nRepeat this for the other two playlists, replacing my other examples. Then run the code.\n\n\nlibrary(tidyverse)\nspotify_playlists <- get_playlist_audio_features(\n  username = \"poiril\",\n  playlist_uris = c(\"7ryj1GwAWYUY36VQd4uXoq\",\n                    \"3DYUw0nHB9o8tLZKQup4zp\",\n                    \"37i9dQZF1DXdOEFt9ZX0dh\"),\n  authorization = get_spotify_access_token()\n) %>%\n  select(-c(track.artists, \n            track.available_markets, \n            track.album.artists, \n            track.album.available_markets, \n            track.album.images))\n\n\n\n\n\n\n\nExercise 1\n\n\n\nHow many songs are in each playlist? Create a plot to visualize this, and order the results by the number of songs. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nExercise 2\n\n\n\nWhat is the distribution of valence across all of the songs (in intervals of 0.1 valence)? Create a histogram to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nExercise 3\n\n\n\nWhat is the distribution of valence across all of the songs (in intervals of 0.1 valence) in each playlist? Create a histogram to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nExercise 4\n\n\n\nWhat are differences in the summary statistics (max, min, median, etc.) of the valence of songs in each playlist? Create grouped boxplot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nExercise 5\n\n\n\nDo happier songs tend to be more danceable in each playlist? Create a plot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context. Also be sure to adjust your plot to address overplotting.\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nExercise 6\n\n\n\nDo songs composed in the minor or major mode tend to be happier in each playlist? Create a plot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nExercise 7\n\n\n\nDo happier songs tend to have a higher tempo across all playlists? What role might the song’s mode play? Create a plot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context. Also be sure to adjust your plot to address overplotting.\n\n\n\n#Create boxplot here\n\n\n\n\n\n\n\nExercise 8\n\n\n\nBased on all of your plots, summarize what you learned about the joyfulness of these three playlists below.\n\n\n\n\n\n\n\n\nYour Response\n\n\n\nFill response here."
  },
  {
    "objectID": "problem-solving-template.html",
    "href": "problem-solving-template.html",
    "title": "Problem Solving",
    "section": "",
    "text": "# Create three vectors\na <- 1, 2, 3, 4, 5\nb <- \"a\", \"b\", \"c\", \"d\", \"e\"\nc <- TRUE, FALSE, TRUE, TRUE, FALSE\n\nError: <text>:2:7: unexpected ','\n1: # Create three vectors\n2: a <- 1,\n         ^\n\n\n\n# Add the values in the vector a\na_added <- add(a)\n\nError in add(a): could not find function \"add\"\n\n\n\n# Multiply the previous output by 3\nthree_times_a_added <- added_a * 3\n\nError in eval(expr, envir, enclos): object 'added_a' not found\n\n\n\n# Create a dataframe with col1 and col2\ndf <- data.frame(\n  col1 = c(1, 2, 3)\n  col2 = c(\"a\", \"b\", \"c\")\n\nError: <text>:4:3: unexpected symbol\n3:   col1 = c(1, 2, 3)\n4:   col2\n     ^\n\n\n\n# Add a new column to df\ndf$col3 <- c(TRUE, FALSE)\n\nError in df$col3 <- c(TRUE, FALSE): object of type 'closure' is not subsettable\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nCopy and paste one line of code from this lab as a threaded comment on the Slack message I posted on September 7, 2022. Be sure the text is formatted as code.\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\nAccess the help pages for the function sort(). Write code below to sort the vector a (which you created in an earlier step) in decreasing order.\n\n\n\n# Write code here. \n\n\n\n\n\n\n\nExercise 4\n\n\n\nNavigate to this cheatsheet for base R. Imagine we collected the temperature of our home each day for the past ten days (see code below). Let’s say we wanted to find how each day ranked from coolest to hottest. For instance, I wanted to know the ranking for day 1 vs. day 2 vs. day 3, and so on. Using the cheatsheet, find the function that will allow you to generate a ranking of each day’s temperature. Search the help pages for this function to determine how to randomly rank two days with the same temperature. Write a comment to yourself describing how this function is different than sorting the data.\n\n\n\n#Create a vector of temperatures\ntemps_to_factor <- c(68, 70, 78, 75, 69, 80, 66, 66, 79)\n\n# Write code below to rank the days with random ties\n\n\n# Replace this line with a comment to yourself describing how this function is different than sorting the data. \n\n\n\n\n\n\n\nExercise 5\n\n\n\nAdd a comment to the code chunk above, citing this Stack Overflow post. Be sure to cite the post properly. You can read through the post to double check your answers."
  },
  {
    "objectID": "lab2-template.html",
    "href": "lab2-template.html",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "",
    "text": "Install the RColorBrewer package by entering the following into your R Console: install.packages(\"RColorBrewer\")\nRun the code below to the import the bridge inventory for Massachusetts and for Hampshire County into R. Call me or one of the data assistants over if you get an error.\n\n\nlibrary(tidyverse)\nlibrary(RColorBrewer)\ncounties <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_counties.csv\")\nroute_prefixes <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_route_pre.csv\")\nmaintenance <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_maintenance.csv\")\nkinds <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_kind.csv\")\n\nnbi_ma <- read.delim(\"https://www.fhwa.dot.gov/bridge/nbi/2022/delimited/MA22.txt\", sep = \",\") %>%\n  left_join(counties) %>%\n  left_join(route_prefixes) %>%\n  left_join(maintenance) %>%\n  left_join(kinds) %>%\n  filter(SERVICE_ON_042A == 1) %>%\n  select(STRUCTURE_NUMBER_008, COUNTY_CODE_003_L, ROUTE_PREFIX_005B_L, MAINTENANCE_021_L, YEAR_BUILT_027, ADT_029, STRUCTURE_KIND_043A_L, STRUCTURAL_EVAL_067, BRIDGE_IMP_COST_094) %>%\n  mutate(STRUCTURE_KIND_043A_L = \n           case_when(\n             STRUCTURE_KIND_043A_L == \"Concrete continuous\" ~ \"Concrete\",\n             STRUCTURE_KIND_043A_L == \"Steel continuous\" ~ \"Steel\",\n             STRUCTURE_KIND_043A_L == \"Prestressed concrete continuous\" ~ \"Prestressed concrete\",\n             TRUE ~ STRUCTURE_KIND_043A_L)) %>%\n  mutate(BRIDGE_IMP_COST_094 = BRIDGE_IMP_COST_094 * 1000)\n\nnbi_hampshire <- nbi_ma %>% filter(COUNTY_CODE_003_L == \"Hampshire\")\n\n\n\n\n\n\n\nExercise 1\n\n\n\nBelow, adjust the size to 2 and the alpha to 0.5, and the position to \"jitter\". Note how this changes the plot.\n\n\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(size = 1, alpha = 1, position = \"identity\") +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nIn the plot below, color the points on the plot with a qualitative palette by setting col= to the categorical variable ROUTE_PREFIX_005B_L. Remember that, in this case, color is an aesthetic, so this must be added inside of the aes() function. Add a label for the legend by setting col= in the labs() function to a phrase that describes the color variable.\n\n\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       col = \"ADD LEGED LABEL HERE\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\nCopy the plot from above below but swap out the variable your mapped to the color aesthetic with a numeric variable from the dataset. Be sure to also adjust the legend label. What kind of palette gets created? What new information do you gain about Hampshire County bridges from the plot?\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nYour Response\n\n\n\nFill response here.\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\nIn the plot below, assign the shape of the data on the plot by setting shape= to the categorical variable STRUCTURE_KIND_043A_L. Remember that, in this case, shape is an aesthetic, so this must be added inside of the aes() function. Add a label for the legend by setting shape= in the labs() function to a phrase that describes the shape variable.\n\n\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       shape = \"ADD LEGED LABEL HERE\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\nCopy one of your plots from above, and inside the aes() function, set size= to a numeric variable not already represented on the plot. Keep in mind that you’ll need to remove the size= attribute from geom_point() for the points to size correctly. Be sure to also adjust the legend labels. What new information do you gain about Hampshire County bridges from the plot?\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nYour Response\n\n\n\nFill response here.\n\n\n\nnbi_ma %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = BRIDGE_IMP_COST_094)) +\n  geom_point(size = 1, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Bridge Improvement Costs\") +\n  theme_minimal() + \n  scale_y_continuous(labels = scales::comma)\n\nWarning: Removed 1 rows containing missing values (geom_point).\n\n\n\n\n\n\n\n\n\n\n\nExercise 6\n\n\n\nCopy the plot that I created above, and change the y-axis scale from continuous to log10. You might reference the formula specified above or reference the ggplot cheatsheet for help. Note that your y-axis will appear in scientific notation. Convert it to comma notation by adding: labels = scales::comma as an argument in the scale function call.\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nExercise 7\n\n\n\nThe Interstate Highway Act of 1956 (signed under Dwight D. Eisenhower) established the U.S.’s interstate highway system and is considered the country’s largest public works project. Adjust the x-axis scale of the plot below to help tell this story. Specifically, bin years into before 1954 (when Eisenhower was elected) and 1954-2021. Set the label at 1954 to “Eisenhower Elected” and the label at 2021 to “Today.”\n\n\n\nnbi_ma %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = ROUTE_PREFIX_005B_L)) +\n  geom_point(size = 1, alpha = 0.2, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Route Prefix\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nExercise 8\n\n\n\nAdd `+ scale_color_brewer()` to the ggplot() object below and assign a categorical palette. Refer to the ggplot cheatsheet or the help pages for help with formatting the function call.\n\n\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067,\n             col = ROUTE_PREFIX_005B_L)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, Massachusetts, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       col = \"Route Prefix\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\n\n\nExercise 9\n\n\n\nAdd `+ scale_color_distiller()` to the ggplot() object below and assign a sequential palette. By default the colors will be ordered from darkest to lightest. Add an argument to the function call to reverse the direction of the colors. Refer to the ggplot cheatsheet or the help pages for help with formatting the function call.\n\n\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067,\n             col = ADT_029)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       col = \"Average Daily Traffic\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nExercise 10\n\n\n\nFacet the plot below by STRUCTURE_KIND_043A_L to see the degree of structural deficiencies for different kinds of highway bridges of different ages in Massachusetts.\n\n\n\nnbi_ma %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(alpha = 0.1, size = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal()"
  },
  {
    "objectID": "lab5-template.html",
    "href": "lab5-template.html",
    "title": "Lab 5: Exploratory Data Analysis",
    "section": "",
    "text": "library(tidyverse)\n\nsqf_url <- \"https://www1.nyc.gov/assets/nypd/downloads/zip/analysis_and_planning/stop-question-frisk/sqf-2011-csv.zip\"\ntemp <- tempfile()\ndownload.file(sqf_url, temp)\nsqf_zip <- unzip(temp, \"2011.csv\")\nsqf_2011 <- read.csv(sqf_zip, stringsAsFactors = FALSE) \nsqf_2011_race_cat <- read.csv(\"https://raw.githubusercontent.com/lindsaypoirier/STS-101/master/Data/SQF/sqf_race_categories.csv\", stringsAsFactors = FALSE) \nrm(sqf_url)\nrm(temp)\nrm(sqf_zip)\n\n\nsqf_2011 <- \n  sqf_2011 %>% \n  select(pct, race, age, frisked, pistol, riflshot, asltweap, knifcuti, machgun, othrweap, sumissue, arstmade) %>%\n  left_join(sqf_2011_race_cat, by = \"race\") %>%\n  mutate(across(frisked:arstmade, \n         ~ case_when(. == \"Y\" ~ 1, . == \"N\" ~ 0)))\nrm(sqf_2011_race_cat)\n\n\n\n\n\n\n\nExercise 1\n\n\n\nAdd new columns indicating 1) whether a weapon was found or 2) an arrest/summons was made.\n\n\n\nsqf_2011 <- \n  sqf_2011 %>%\n  #Add a variable for weapon found\n  _____(wpnfound = case_when(pistol == 1 |\n                               riflshot == 1 | \n                               asltweap == 1 |\n                               knifcuti == 1 | \n                               machgun == 1 | \n                               othrweap == 1 ~ 1,\n                             TRUE ~ 0))\nsqf_2011 <- \n  sqf_2011 %>%\n  #Add a variable for arrest made or summons issued\n  _____(arrestsumm = case_when(sumissue == 1 | \n                                arstmade == 1 ~ 1,\n                               TRUE ~ 0))\n\n\n\n\n\n\n\nExercise 2\n\n\n\nSubset the dataset to the six variables listed in the data dictionary above.\n\n\n\nsqf_2011 <-\n  sqf_2011 %>%\n  _____(pct, arrestsumm, _____, wpnfound, race_cat, _____)\n\n\n\n\n\n\n\nExercise 3\n\n\n\nCalculate the number of stops in 2011. Hint: Keep in mind that every row in the dataset represents one stop.\n\n\n\ntotal_stops <-\n  sqf_2011 %>%\n  summarize(Count = _____) %>%\n  pull()\n\ntotal_stops\n\n\n\n\n\n\n\nExercise 4\n\n\n\nHow many stops did not result in an arrest or summons in 2011? What percentage of stops did not result in an arrest or summons?\n\n\n\nsqf_2011 %>%\n  #Subset to rows where suspect innocent\n  _____(arrestsumm _____ 0) %>% \n  #Calculate number of observations\n  _____(total_innocent = n(), \n            percent_innocent = _____ / total_stops * 100)\n\n\n\n\n\n\n\nExercise 5\n\n\n\nIn how many stops were the individuals aged 14-24? In what percentage of stops were the individuals aged 14-24?\n\n\n\nsqf_2011 %>%\n  #Subset to rows where suspect age 14-24\n  _____(age _____ 14 & age _____ 24) %>% \n  #Calculate number of observations and percentage of observations\n  _____(total_14_24 = _____, \n            percent_14_24 = n() / total_stops * 100)\n\n\ntotal_stops_age_recorded <-\n  sqf_2011 %>%\n  #Subset to rows where age is not 999\n  _____(age _____ 999) %>% \n  summarize(Count = n()) %>%\n  pull()\n\nsqf_2011_sub %>%\n  filter(age >= 14 & age <= 24) %>%\n  summarize(total_14_24 = n(), \n            percent_14_24 = n() / total_stops_age_recorded * 100)\n\n\n\n\n\n\n\nExercise 6\n\n\n\nHow many stops were there per race in 2011? What percentage of stops per race in 2011? Arrange by number of stops in descending order.\n\n\n\ntotal_stops_race_recorded <-\n  sqf_2011 %>%\n  #Subset to rows where race_cat is not NA or \"OTHER\"\n  filter(_____(race_cat) & race_cat _____ \"OTHER\") %>% \n  summarize(Count = n()) %>%\n  pull()\n\nsqf_2011 %>%\n  #Subset to rows where race_cat is not NA or \"OTHER\"\n  _____(_____(race_cat) & race_cat _____ \"OTHER\") %>% \n  #Group by race\n  _____(race_cat) %>% \n  #Calculate number of observations\n  _____(stops = n(), \n            percent_stops = n() / total_stops_race_recorded * 100) %>%\n  #Sort by stops in descending order\n  _____(_____(stops)) \n\n\nsqf_2011 %>%\n  #Subset to rows where race_cat is \"WHITE-HISPANIC\" or \"BLACK-HISPANIC\"\n  _____(race_cat _____ c(\"WHITE-HISPANIC\", \"BLACK-HISPANIC\")) %>% \n  #Calculate number of observations\n  _____(stops_Latinx = _____, \n            percent_Latinx = n() / total_stops_race_recorded * 100)\n\n\n\n\n\n\n\nExercise 7\n\n\n\nWhat percentage of stops in 2011 resulted in a frisk per race? What percentage of stops in 2011 resulted in a weapon found per race? What percentage of stops in 2011 resulted in an arrest or summons per race? In your resulting data table, each row should be a race, and there should be columns for stops, percent_stops, percent_frisked, percent_wpnfound , and percent_arrestsum.\n\n\n\n# Write code here."
  },
  {
    "objectID": "slides/day1.html",
    "href": "slides/day1.html",
    "title": "Day One: Introductions",
    "section": "",
    "text": "What is data science?: My view\n.pull-left[]\n.pull-right[] —\n\n\nCase Study 1: ACLU Fights Discriminatory Housing\n.pull-left[]\n.pull-right[] —\n\n\nCase Study 2: EPA Tracks Environmental Injustice\n.pull-left[]\n.pull-right[] —\n\n\nCase Study 3: Geena Davis Institute Studies Gender Biases in Films\n.pull-left[]\n.pull-right[] —\n\n\nTopics covered in this course\n.pull-left[]\n.pull-right[] —\n\n\nWho is the professor? Why is an anthropologist teaching data science?\n.pull-left[ * Please call me Lindsay (preferred), Professor Poirier, or Dr. Poirier * Previously Assistant Professor of Science and Technology Studies at UC Davis * Lab Manager at BetaNYC * M.S./Ph.D. in Science and Technology Studies from Rensselaer Polytechnic Institute * B.S. in Information Technology and Web Science from Rensselaer Polytechnic Institute * Dancing, crafting, cooking, re-watching the same TV series over and over again. * I have a very spunky dog Madison who you may hear on Zoom calls. ]\n.pull-right[ ]\n\n\n\nExercise\n\nIn break-out rooms, navigate to the Jamboard in the chat window.\nNavigate to the slide associated with your breakout room number.\nElect one person to serve as a facilitator. This person is responsible for calling on students as they raise their hands.\nEveryone should add a sticky note with their name, their major, and the number of cups of coffee they drink per day to slide.\nUsing the sticky notes, talk through the following prompt:\n\nFind the non-STEM major in your group in which students with that major on average drink the most cups of coffee per day. Repeat for STEM majors.\n\nIn the text box, formally write-out the steps you took to come to your answers in as much detail as possible.\n\n\n\n\nCoding can be intimidating!\n\nCoding is like learning a new language. When you are first learning it, it all feels completely unfamiliar. I will work to support you in building the vocabulary and syntax to code in R.\nCoding can be frustrating. I regularly lose hours of my day in trying to find bugs in my code. I will work to give you resources and skills to navigate coding frustrations.\nCoding social environments have historically been exclusionary. I will work to reduce barriers to coding in whatever ways I can.\n\n\n\n\nSyllabus Review\n\nPolicies\nStandards Grading\nCourse Website\nMoodle\nPerusall\nSlack\n\n\n\n\nFor Wednesday\n\nReview Syllabus and Grading Contract in Perusall\nInstall Slack Desktop and set notifications\nFill out first day of class questionnaire\nNext time: Datasets, Data Ethics Framework, More on Standards Grading"
  },
  {
    "objectID": "slides/Day1-Intro.html",
    "href": "slides/Day1-Intro.html",
    "title": "Day One: Introductions",
    "section": "",
    "text": "What is data science?: My view\n.pull-left[]\n.pull-right[]\n\n\n\nCase Study 1: ACLU Fights Discriminatory Housing\n.pull-left[]\n.pull-right[]\n\n\n\nCase Study 2: EPA Tracks Environmental Injustice\n.pull-left[]\n.pull-right[]\n\n\n\nCase Study 3: Geena Davis Institute Studies Gender Biases in Films\n.pull-left[]\n.pull-right[]\n\n\n\nTopics covered in this course\n.pull-left[]\n.pull-right[]\n\n\n\nWho is the professor? Why is an anthropologist teaching data science?\n.pull-left[ * Please call me Lindsay (preferred), Professor Poirier, or Dr. Poirier * Previously Assistant Professor of Science and Technology Studies at UC Davis * Lab Manager at BetaNYC * M.S./Ph.D. in Science and Technology Studies from Rensselaer Polytechnic Institute * B.S. in Information Technology and Web Science from Rensselaer Polytechnic Institute * Dancing, crafting, cooking, re-watching the same TV series over and over again. * I have a very spunky dog Madison who you may hear on Zoom calls. ]\n.pull-right[ ]\n\n\n\nExercise\n\nIn break-out rooms, navigate to the Jamboard in the chat window.\nNavigate to the slide associated with your breakout room number.\nElect one person to serve as a facilitator. This person is responsible for calling on students as they raise their hands.\nEveryone should add a sticky note with their name, their major, and the number of cups of coffee they drink per day to slide.\nUsing the sticky notes, talk through the following prompt:\n\nFind the non-STEM major in your group in which students with that major on average drink the most cups of coffee per day. Repeat for STEM majors.\n\nIn the text box, formally write-out the steps you took to come to your answers in as much detail as possible.\n\n\n\n\nCoding can be intimidating!\n\nCoding is like learning a new language. When you are first learning it, it all feels completely unfamiliar. I will work to support you in building the vocabulary and syntax to code in R.\nCoding can be frustrating. I regularly lose hours of my day in trying to find bugs in my code. I will work to give you resources and skills to navigate coding frustrations.\nCoding social environments have historically been exclusionary. I will work to reduce barriers to coding in whatever ways I can.\n\n\n\n\nSyllabus Review\n\nPolicies\nStandards Grading\nCourse Website\nMoodle\nPerusall\nSlack\n\n\n\n\nFor Wednesday\n\nReview Syllabus and Grading Contract in Perusall\nInstall Slack Desktop and set notifications\nFill out first day of class questionnaire\nNext time: Datasets, Data Ethics Framework, More on Standards Grading"
  },
  {
    "objectID": "slides/Day1-Intro.html#what-is-data-science-common-view",
    "href": "slides/Day1-Intro.html#what-is-data-science-common-view",
    "title": "Day One: Introductions",
    "section": "What is data science?: Common view",
    "text": "What is data science?: Common view\n\n\n\ninterdisciplinary field combining computer science, mathematics/statistics, and domain expertise to extract meaningful information from unstructured data points\n\n\n Hckum, CC BY-SA 4.0 https://creativecommons.org/licenses/by-sa/4.0, via Wikimedia Commons"
  },
  {
    "objectID": "slides/Day1-Intro.html#what-is-data-science-my-view",
    "href": "slides/Day1-Intro.html#what-is-data-science-my-view",
    "title": "Day One: Introductions",
    "section": "What is data science?: My view",
    "text": "What is data science?: My view\n\n\n\ninterdisciplinary field combining computer science, mathematics/statistics, and domain expertise to extract meaningful information from unstructured data points\nalso involves art, design, hermeneutics, communication, and ability to grapple with ethical dilemmas\n\n\n Hckum, CC BY-SA 4.0 https://creativecommons.org/licenses/by-sa/4.0, via Wikimedia Commons"
  },
  {
    "objectID": "slides/Day1-Intro.html#case-study-2-epa-tracks-environmental-injustice",
    "href": "slides/Day1-Intro.html#case-study-2-epa-tracks-environmental-injustice",
    "title": "Day One: Introductions",
    "section": "Case Study 2: EPA Tracks Environmental Injustice",
    "text": "Case Study 2: EPA Tracks Environmental Injustice\n\n\n\nEnvironmental Protection Agency hires data scientists to produce insights regarding environmental health risks\nFindings implicate environmental policies, funding allocations, and legal actions against states and industries\nThis tool, visualizes environmental and demographic indicators to highlight communities experiencing environmental injustices."
  },
  {
    "objectID": "slides/Day1-Intro.html#case-study-3-geena-davis-institute-studies-gender-biases-in-films",
    "href": "slides/Day1-Intro.html#case-study-3-geena-davis-institute-studies-gender-biases-in-films",
    "title": "Day One: Introductions",
    "section": "Case Study 3: Geena Davis Institute Studies Gender Biases in Films",
    "text": "Case Study 3: Geena Davis Institute Studies Gender Biases in Films\n\n\n\nGeena Davis Institute collaborated with University of Southern California’s Signal Analysis and Interpretation Laboratory (SAIL)\nDeveloped a machine learning tool to measure representation of diverse groups in films by studying screen time and speaking"
  },
  {
    "objectID": "slides/Day1-Intro.html#topics-covered-in-this-course",
    "href": "slides/Day1-Intro.html#topics-covered-in-this-course",
    "title": "Day One: Introductions",
    "section": "Topics covered in this course",
    "text": "Topics covered in this course\n\n\n\ndata visualization\ndata wrangling\nprogramming with data\nmapping\ndata retrieval\ndata science infrastructures and workflows\ndata science ethics\n\n\n https://towardsdatascience.com/the-25-best-data-visualizations-of-2018-93643f0aad04"
  },
  {
    "objectID": "slides/Day1-Intro.html#who-is-the-professor-why-is-an-anthropologist-teaching-data-science",
    "href": "slides/Day1-Intro.html#who-is-the-professor-why-is-an-anthropologist-teaching-data-science",
    "title": "Day One: Introductions",
    "section": "Who is the professor? Why is an anthropologist teaching data science?",
    "text": "Who is the professor? Why is an anthropologist teaching data science?\n\nPlease call me Lindsay (preferred), Professor Poirier, or Dr. Poirier\nAssistant Professor of SDS and cultural anthropologist\nPreviously Assistant Professor of Science and Technology Studies at UC Davis\nLab Manager at BetaNYC\nM.S./Ph.D. in Science and Technology Studies from Rensselaer Polytechnic Institute\nB.S. in Information Technology and Web Science from Rensselaer Polytechnic Institute\nDancing, crafting, cooking, re-watching the same TV series over and over again.\nI have a very spunky dog Madison."
  },
  {
    "objectID": "slides/Day1-Intro.html#exercise",
    "href": "slides/Day1-Intro.html#exercise",
    "title": "Day One: Introductions",
    "section": "Exercise",
    "text": "Exercise\nDemonstration: Find the non-STEM major in the class for which students with that major on average drink the most cups of coffee per day. Repeat for STEM majors."
  },
  {
    "objectID": "slides/Day1-Intro.html#coding-can-be-intimidating",
    "href": "slides/Day1-Intro.html#coding-can-be-intimidating",
    "title": "Day One: Introductions",
    "section": "Coding can be intimidating!",
    "text": "Coding can be intimidating!\n\nCoding is like learning a new language. When you are first learning it, it all feels completely unfamiliar. I will work to support you in building the vocabulary and syntax to code in R.\nCoding can be frustrating. I regularly lose hours of my day in trying to find bugs in my code. I will work to give you resources and skills to navigate coding frustrations.\nCoding social environments have historically been exclusionary. I will work to reduce barriers to coding in whatever ways I can."
  },
  {
    "objectID": "slides/Day1-Intro.html#prepping-for-this-class",
    "href": "slides/Day1-Intro.html#prepping-for-this-class",
    "title": "Day One: Introductions",
    "section": "Prepping for this Class",
    "text": "Prepping for this Class\n\nNavigating Course Website\nStandards Grading\nPerusall\nSlack"
  },
  {
    "objectID": "slides/Day1-Intro.html#for-friday",
    "href": "slides/Day1-Intro.html#for-friday",
    "title": "Day One: Introductions",
    "section": "For Friday",
    "text": "For Friday\n\nInstall Slack Desktop and set notifications\nComplete Syllabus Quiz\nFill out first day of class questionnaire\nLet me know if you will be using a Chromebook, asap\nBring charged tech to class on Friday"
  },
  {
    "objectID": "getting-started.html",
    "href": "getting-started.html",
    "title": "Configuring RStudio with GitHub",
    "section": "",
    "text": "Below are instructions for setting up our course environment. We’re going to walk through them together, but different groups might get stuck at different points in the process. If you get stuck, hold up a block flag, which indicates to me that you need help.\n\n\n\n\n\n\nTip\n\n\n\nI’m going to start off by saying that I dread this day every time I teach R in a course. Students have different computers, sometimes have older versions of software installed on them, and different levels of experience setting up coding environments. I’ll also admit that as a former CS undergrad, for me, the absolute hugest barrier to getting started on any project was getting my coding environment set up. We are going to run into frustrations today; it’s inevitable. I’m going to do whatever I can to support you through them, which is why I’m carving out class time to do it. I don’t expect every issue we run into to be sorted out by the end of class or even by class on Monday. I’m telling you this because I don’t want you to worry if you run into roadblocks in setting up in your environment. We will get through them. On the other hand, some of you might breeze through this, having certain components already set up or having previous experience configuring GitHub. While not required, if you finish early and would be willing to help me troubleshoot other issues that come up, it would certainly be appreciated.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDon’t worry if you don’t know what certain terms mean at this point. We will be going over things like the difference between R and RStudio, what a package is, what it means to commit code, and what a quarto file is a little later in the semester. I just believe it will be easier to follow along with me in learning those things if you can follow along on your own computers. That’s why we’re taking this step to get things up and running so earlier in the semester.\n\n\n\n\nYou should create a GitHub account here. When choosing a username, I encourage you to:\n\nInclude some version of your actual name\nKeep it short!\nAvoid including things that designate this moment in time. (Don’t include “Smith student” or “2022”)\n\n\n\n\nI’m assuming that everyone in this course already has R and RStudio installed - either because you are in SDS 100, and you installed things yesterday, or because you’ve taken a course in R before. If you haven’t installed R and RStudio, follow these instructions to do so.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen you open RStudio, if you get a prompt to install “git” command line tools, don’t dismiss it. Click ‘Install’. You’ll see why in the next step.\n\n\nNote the different sections of the RStudio interface.\n\n\n\n\n\n\n\nPotential Blocker: Every time I try to open RStudio, I’m being prompted to reinstall RStudio.\n\n\n\nThis was happening for a few students on Windows machines. The reason is that you are clicking the file to install/RStudio, rather than the file to open the application. See if you can find the icon for RStudio when you open C:/ProgramFiles. The icon should look like this:\n\n\n\nRStudio Icon\n\n\n\n\n\n\n\n\n\n\nPotential Blocker: I’m seeing this image when I try to open RStudio:\n\n\n\n\nThis means that you likely installed RStudio but not R. Check back in on the instructions in Step 2 to make sure that you install R and RStudio.\n\n\n\n\n\n\n\n\n\nThis is going to be different for different students depending on what type of operating system you are using. Scroll to the instructions for your operating system.\n\n\nIf you just recently installed RStudio, when you open RStudio, you may see a window that looks like this:\n\nIf you see this image, you should click ‘Install’. It will likely take a few moments to install.\nIf you don’t see this image, it could be because git is already installed on your computer. You can check by navigating to the ‘Console’ pane at the bottom of your screen and clicking on the ‘Terminal’ tab. Enter the following in that window: which git. If git is not installed, the window will tell you so. If it is installed, it will report a file path. My file path looks like this: /usr/bin/git, but yours may look different.\nIf git is not installed at this point, then you should copy and paste the following into your Terminal and click enter:\nxcode-select --install\n\n\n\nCheck if git is already installed by navigating to the Console pane at the bottom of your screen and clicking on the ‘Terminal’ tab. Enter the following in the Terminal: where git. If git is not installed, the window will tell you so. If it is installed, it will report a file path. My file path looks like this: /usr/bin/git, but yours may look different.\n\nIf git is not installed at this point, then you should install Git for Windows here. When installing, you will be prompted about “Adjusting your PATH environment”. When this happens, you should select “Git from the command line and also from 3rd-party software”.\n\n\n\n\n\n\nBefore moving to step 5...\n\n\n\nAfter you’ve installed git, quit out of RStudio and then reopen it. We want to be absolutely sure that git is properly installed before moving on. Navigate to the Console pane at the bottom of your screen and clicking on the Terminal tab. If you are using a Mac, enter which git. If you are using a Windows, enter where git. If git is installed, you should see a file path to git (most likely in a bin folder).\n\nIf you get an indication that git can’t be found, raise a blocker flag.\n\n\n\n\n\n\nMany of you will have already installed the usethis package in SDS 100 yesterday. If for some reason you didn’t, you should follow the instructions below to install usethis.\n\nEnsure that you are in the ‘Console’ at the bottom of your screen.\nCopy and paste install.packages(\"usethis\") into your console and click enter.\nYou will be asked “Do you want to install from sources the packages which need compilation? (Yes/no/cancel)”, and you can type ‘no’. After that, a bunch of lines of code will run with red font.\n\n\n\n\n\n\n\nPotential Blocker: I’m getting an error message that rlang is not installed.\n\n\n\nIn your console type install.packages(\"rlang\"). Then re-try Step 7.\n\n\n\n\n\n\nCopy and paste library(usethis) into your Console to load the library.\nCopy and paste the following into your console, replacing USERNAMEHERE with your GitHub username and EMAILHERE with your GitHub email: use_git_config(user.name = \"USERNAMEHERE\", user.email = \"EMAILHERE\")\n\n\n\n\n\n\n\nPotential Blocker: I’m getting an error message that gert is not installed.\n\n\n\nIn your console type install.packages(\"gert\"). Then re-try Step 6.\n\n\n\n\n\n\n\n\nPotential Blocker: I’m getting an error message that RStudio can’t find git.\n\n\n\nFollowing the instructions at this link: https://happygitwithr.com/rstudio-see-git to tell RStudio where to find git.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you’ve configured git on your computer before, you can edit your git config by following these instructions:\n\nEdit gitconfig file from David Keyes on Vimeo.\n\n\n\n\n\n\n\nNavigate to Github.com and log-in.\nAt Github.com, click on your user icon (top right hand corner) and then ‘Settings’.\nAt the very bottom of the left side panel, click ‘Developer Settings’\n\n\n\nIn the right sidebar, click ‘Personal Access Tokens’, and then on the page that appears, click ‘Generate New Token’.\nIn the ‘Note’ field, enter “SDS 192”\nSet the ‘Expiration’ to ‘Custom’, and then select a date after December 12, 2022.\nCheck the box next to ‘repo’ in Select Scopes.\nClick the green ‘Generate Token’ button.\nKeep this window open because you will need to copy your Personal Access Token in the next step.\n\n\n\n\n\n\n\nTip\n\n\n\nThis video summarizes these steps:\n\nCreate a Personal Access Token (PAT) on GitHub from David Keyes on Vimeo.\n\n\n\n\n\n\n\nInstall the gitcreds package by entering the following into your RStudio Console: install.packages(\"gitcreds\")\nLoad the gitcreds package by entering the following into your RStudio Console: library(gitcreds)\nEnter gitcreds_set() into your Console.\nYou will be prompted to enter your password or token. Head back over to Github.com, and copy the Personal Access Token that you created in the last step. It will be a long string of characters. Paste it into your RStudio Console.\n\n\n\n\n\n\n\nPotential Blocker: I’m getting the following error when I try to set my git credentials:\n\n\n\nError in new_git_error(\"git_error\", args = args, stdout = out, status = attr(out,  :\n  System git failed: xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun\nSwitch to the Terminal tab in the bottom left hand corner of RStudio and enter the following:\nxcode-select --install\nThen try again.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis video summarizes these steps:\n\nStore Personal Access Token to Connect RStudio and GitHub from David Keyes on Vimeo.\n\n\n\n\n\n\n\n\n\nPotential Blocker: I’m get an error System git failed: fatal: bad config line\n\n\n\nIf you get the following error:\nError in new_git_error(\"git_error\", args = args, stdout = out, status = attr(out,  :\n  System git failed: fatal: bad config line ...\n…this means that you probably added some unrecognized characters in your git config file. Enter:\nlibrary(usethis)\nedit_git_config()\nThis will open your config file. Check to make sure you don’t have extra quotation marks in your user name or email address. It should look something like this.\n\n\n\nRStudio Icon\n\n\n\n\n\n\n\n\n\n\n\nIn Moodle, scroll to today’s date to find the Course Infrastructure Set-up link. Click this link to enter our GitHub classroom. This will copy a GitHub template repo into your GitHub account. It might take a few moments.\n\n\n\nOnce the repo is created, copy the link to the repo that was just created to your clipboard. It should look something like https://github.com/sds-192-intro-fall22/getting-started-YOUR-USER-NAME\n\n\n\nGitHub URL\n\n\n\n\n\n\nIn RStudio, click File > New Project > Version Control > Git\nIn the window that appears, paste the URL that you copied in the last step into the ‘Repository URL’ box.\nWhere it says ‘Create project as subdirectory of:’, click ‘Browse’. Create a new folder called “SDS 192.” I suggest that you store this folder in you Documents folder, so that you can easily find it.\nClick ‘Create Project.’\n\n\n\n\n\n\n\nTip\n\n\n\nThis video summarizes these steps:\n\nHow to Connect RStudio Projects with GitHub Repositories: GitHub First from David Keyes on Vimeo.\n\n\n\n\n\n\n\n\n\nPotential Blocker: I’m getting an error indicating that support for password authentical was removed.\n\n\n\nIf you get this error:\nremote: Support for password authentication was removed on August 13, 2021. Please use a personal access token instead.\nThis means that you likely set up some git credentials on your computer in the past. GitHub used to identify you by a password, but now they use the special token you generated in an earlier step. To fix this, you can enter the following into your console:\ninstall.packages(\"gitcreds\")\nlibrary(gitcreds)\ngitcreds_set()\nYou’ll see the following message:\n1: Keep these credentials\n2: Replace these credentials\n3: See the password / token\nClick 2. Then enter the personal access token you created in step 9. You should be able to create a project after this.\n\n\n\n\n\n\nOn the initial install, the Files tab will be in the lower right hand corner of RStudio. Look for the getting-started.qmd file in that pane and then open it.\n\nFollow instructions in the file. Make sure to save the file.\nWhen the file tells you to commit your code, turn your attention to the Environment pane of RStudio (i.e. upper right hand pane on initial install).\nClick on the ‘Git’ tab, and then click the blank checkbox next to getting-started.qmd. As we will learn in a few weeks, this stages a file in git.\n\nIn this same ‘Git’ pane, click the ‘Commit’ button. In the window that appears, you will be prompted to write a commit message. Enter the following in the commit message window: “added name to getting-started.qmd”\nClick the ‘Commit’ button. Again, in a few weeks, we will learn what this means.\nIf you’ve done this correctly, when you close the window, there will be no files listed in the Git pane.\n\n\n\n\n\n\n\nTip\n\n\n\nThis video summarizes these steps:\n\nMake a Commit and View More History from David Keyes on Vimeo.\n\n\n\n\n\n\nIn the same ‘Git’ pane, you will see a little green upwards arrow, click this arrow to push your code to GitHub.com.\n\n\n\n\n\n\n\nPotential Blocker: When I push my code, I get an a notification that everything is up to date.\n\n\n\n\nIf you get this notification, and you are sure you haven’t already pushed the changes, it likely means one of three things:\n\nYou didn’t make changes to any files.\nYou didn’t save your changes to the files.\nYou didn’t commit your changes to the files.\n\nIf this is the case, return back to step 12, and make sure that changes were made, saved, and committed, and then try pushing again."
  },
  {
    "objectID": "slides/Day2-DataFundamentals.html#observations-vs.-variables-vs.-values",
    "href": "slides/Day2-DataFundamentals.html#observations-vs.-variables-vs.-values",
    "title": "Data Fundamentals",
    "section": "Observations vs. Variables vs. Values",
    "text": "Observations vs. Variables vs. Values\n\n\n\nObservationsVariablesValues\n\n\n\n\nObservations refer to individual units or cases of the data being collected.\n\nIf I was collecting data about each student in this course, one student would be an observation.\nIf I was collecting census data and aggregating it at the county level, one county would be an observation.\n\n\n\n\n\n\n\nVariables describe something about an observation.\n\nIf I was collecting data about each student in this course, ‘major’ might be one variable.\nIf I was collecting county-level census data, ‘population’ might be one variable.\n\n\n\n\n\n\n\nValues refer to the actual value associated with a variable for a given observation.\n\nIf I was collecting data about each student’s major in this course, one value might be SDS.\n\n\n\n\n\n\n\n\nGrolemund, Garrett, and Hadley Wickham. n.d. R for Data Science. Accessed March 31, 2019. https://r4ds.had.co.nz/."
  },
  {
    "objectID": "slides/Day2-DataFundamentals.html#how-can-we-refer-to-certain-rows-columns-or-values-in-a-dataset",
    "href": "slides/Day2-DataFundamentals.html#how-can-we-refer-to-certain-rows-columns-or-values-in-a-dataset",
    "title": "Data Fundamentals",
    "section": "How can we refer to certain rows, columns, or values in a dataset?",
    "text": "How can we refer to certain rows, columns, or values in a dataset?\n\nAn index is a formal way of identifying the data at certain positions in a dataset.\nIndexes are usually formatted as two numbers in brackets (e.g. [3,4]).\nThe first number refers to the row’s position in the dataset. The second number refers to the column’s position in the dataset.\n\n[3,4] will refer to the value three rows down and four rows over.\n\nWe can refer to an entire row of data by leaving the value in the second position of the index blank (e.g. [3,] will refer to the third row.)\nWe can refer to an entire column of data by leaving the value in the first position of the index blank (e.g. [,4] will refer to the fourth column)\n\nAlternatively, we can refer to a column by its column name."
  },
  {
    "objectID": "slides/Day2-DataFundamentals.html#key-considerations-for-rectangular-datasets",
    "href": "slides/Day2-DataFundamentals.html#key-considerations-for-rectangular-datasets",
    "title": "Data Fundamentals",
    "section": "Key Considerations for Rectangular Datasets",
    "text": "Key Considerations for Rectangular Datasets\n\n\n\nAll rows in a rectangular dataset are of equal length.\nAll columns in a rectangular dataset are of equal length.\n\n\n\n\n\n\n\nUnderstanding Check\n\n\nLet’s say I have a rectangular dataset documenting student names and majors, and I was missing major information for one student. What would this look like in a rectangular dataset?\n\n\n\n\n\nGrolemund, Garrett, and Hadley Wickham. n.d. R for Data Science. Accessed March 31, 2019. https://r4ds.had.co.nz/."
  },
  {
    "objectID": "slides/Day2-DataFundamentals.html#is-this-dataset-rectangular",
    "href": "slides/Day2-DataFundamentals.html#is-this-dataset-rectangular",
    "title": "Data Fundamentals",
    "section": "Is this dataset rectangular?",
    "text": "Is this dataset rectangular?"
  },
  {
    "objectID": "slides/Day2-DataFundamentals.html#is-this-a-rectangular-dataset",
    "href": "slides/Day2-DataFundamentals.html#is-this-a-rectangular-dataset",
    "title": "Data Fundamentals",
    "section": "Is this a rectangular dataset?",
    "text": "Is this a rectangular dataset?"
  },
  {
    "objectID": "slides/Day2-DataFundamentals.html#how-do-i-find-out-more-information-about-a-dataset",
    "href": "slides/Day2-DataFundamentals.html#how-do-i-find-out-more-information-about-a-dataset",
    "title": "Data Fundamentals",
    "section": "How do I find out more information about a dataset?",
    "text": "How do I find out more information about a dataset?\n\n\nMetadata can be referred to as “data about data”\nMetadata provides important contextual information to help us interpret a dataset.\nThere are two types of metadata associated with datasets:\n\n\n\nAdministrativeDescriptive\n\n\n\n\nAdministrative metadata tells us how a dataset is managed and its provenance, or the history of how it came to be in its current form:\n\nWho created it?\nWhen was it created?\nWhen was it last updated?\nWho is permitted to use it?\n\n\n\n\n\n\n\nDescriptive metadata tells us information about the contents of a dataset:\n\nWhat does each row refer to?\nWhat does each column refer to?\nWhat values might appear in each cell?"
  },
  {
    "objectID": "slides/Day2-DataFundamentals.html#types-of-variables",
    "href": "slides/Day2-DataFundamentals.html#types-of-variables",
    "title": "Data Fundamentals",
    "section": "Types of Variables",
    "text": "Types of Variables\n\n\n\n\n\n\n\nCategorical Variables\nNumeric Variables\n\n\n\n\nNominal Variables: Named or classified labels (e.g. names, zip codes, hair color)\nDiscrete Variables: Countable variables (e.g. number of students in this class)\n\n\nOrdinal Variables: Ordered labels (e.g. letter grades, pollution levels)\nContinuous Variables: Measured variables (e.g. temperature, height)"
  },
  {
    "objectID": "slides/Day2-DataFundamentals.html#for-friday",
    "href": "slides/Day2-DataFundamentals.html#for-friday",
    "title": "Day Two: Data Fundamentals",
    "section": "For Friday",
    "text": "For Friday\n\nInstall Slack Desktop and set notifications\nComplete Syllabus Quiz\nFill out first day of class questionnaire\nLet me know if you will be using a Chromebook, asap\nBring charged tech to class on Friday"
  },
  {
    "objectID": "slides/Day2-DataFundamentals.html#for-wednesday",
    "href": "slides/Day2-DataFundamentals.html#for-wednesday",
    "title": "Data Fundamentals",
    "section": "For Wednesday",
    "text": "For Wednesday\n\nStart Problem Solving Lab"
  },
  {
    "objectID": "slides/Day2-DataFundamentals.html#exercise",
    "href": "slides/Day2-DataFundamentals.html#exercise",
    "title": "Data Fundamentals",
    "section": "Exercise",
    "text": "Exercise\nComing Soon!"
  },
  {
    "objectID": "slides/Day2-DataFundamentals.html#observations-vs.-variables-vs.-values-.smaller-.nonincremental",
    "href": "slides/Day2-DataFundamentals.html#observations-vs.-variables-vs.-values-.smaller-.nonincremental",
    "title": "Day Two: Data Fundamentals",
    "section": "Observations vs. Variables vs. Values {.smaller, .nonincremental}",
    "text": "Observations vs. Variables vs. Values {.smaller, .nonincremental}\n\n\n\nObservationsVariablesValues\n\n\n\nObservations refer to individual units or cases of the data being collected.\n\nIf I was collecting data about each student in this course, one student would be an observation.\nIf I was collecting census data and aggregating it at the county level, one county would be an observation.\n\n\n\n\n\nVariables describe something about an observation.\n\nIf I was collecting data about each student in this course, ‘major’ might be one variable.\nIf I was collecting county-level census data, ‘population’ might be one variable.\n\n\n\n\n\nValues refer to the actual value associated with a variable for a given observation.\n\nIf I was collecting data about each student’s major in this course, one value might be SDS.\n\n\n\n\n\n\n\nGrolemund, Garrett, and Hadley Wickham. n.d. R for Data Science. Accessed March 31, 2019. https://r4ds.had.co.nz/."
  },
  {
    "objectID": "slides/Day2-DataFundamentals.html#how-do-i-find-out-more-information-about-a-dataset-.smaller-.nonincremental",
    "href": "slides/Day2-DataFundamentals.html#how-do-i-find-out-more-information-about-a-dataset-.smaller-.nonincremental",
    "title": "Day Two: Data Fundamentals",
    "section": "How do I find out more information about a dataset? {.smaller, .nonincremental}",
    "text": "How do I find out more information about a dataset? {.smaller, .nonincremental}\n\nMetadata can be referred to as “data about data”\nMetadata provides important contextual information to help us interpret a dataset.\nThere are two types of metadata associated with datasets:\n\n\nAdministrativeDescriptive\n\n\n\nAdministrative metadata tells us how a dataset is managed and its provenance, or the history of how it came to be in its current form:\n\nWho created it?\nWhen was it created?\nWhen was it last updated?\nWho is permitted to use it?\n\n\n\n\n\nDescriptive metadata tells us information about the contents of a dataset:\n\nWhat does each row refer to?\nWhat does each column refer to?\nWhat values might appear in each cell?`"
  },
  {
    "objectID": "slides/Day2-DataFundamentals.html#where-do-i-find-metadata-for-a-dataset",
    "href": "slides/Day2-DataFundamentals.html#where-do-i-find-metadata-for-a-dataset",
    "title": "Data Fundamentals",
    "section": "Where do I find metadata for a dataset?",
    "text": "Where do I find metadata for a dataset?\n\nOftentimes metadata is recorded in a dataset codebook or data dictionary.\nThese documents provide definitions for the observations and variables in a dataset and tell you the accepted values for each variable.\nLet’s say that I have a dataset of student names, majors, and class years. A codebook or data dictionary might tell me that:\n\nEach row in the dataset refers to one student.\nThe ‘Class Year’ variable refers to “the year the student is expected to graduate.”\nPossible values for the ‘Major’ variable are Political Science, SDS, and Sociology."
  },
  {
    "objectID": "templates/lab1.html",
    "href": "templates/lab1.html",
    "title": "Lab 1: Understanding Datasets",
    "section": "",
    "text": "Question\n\nSee if you can identify a unique key for this dataset. Write some lines of code to determine whether the column you’ve identified can act as unique key for the data. Hint: You need to check whether the values in the column ever repeat.\n\n\n# Write code to calculate number of rows in scorecard\n\n# Write code to calculate the number of unique values in the column you've identified as a unique key \n\n# Do these numbers match?\n\n\n\nQuestion\n\nAt this point, you’ve probably figured out that each row in this dataset is a higher education institution. …but there are many different ways that we can define higher education institutions, and that will impact what gets included and excluded in our data. Referencing the Technical Documentation, locate a definition for the unit of observation in this dataset. What institutions are included, and what institutions are excluded? Summarize a definition below.\n\n\n\nQuestion\n\nFor each of the variable names in the scorecard data frame, look up the associated name in the data dictionary. You will need to search for the variable name in the sixth column of the data dictionary (I recommend using Ctrl-F to quickly locate the variable name in the spreadsheet.) Once you’ve found the variable name, reference column 1 to determine what this variable means, and reference columns 7 and 8 to see what possible values will appear in that column.\nIdentify one nominal variable, one ordinal variable, one discrete variable, and one continuous variable in scorecard and list their variable names below. Then uncomment the lines below and use the typeof() function to see how R determined their data types. Did any surprise you?\n\n\n#typeof(scorecard$_____)\n#typeof(scorecard$_____)\n#typeof(scorecard$_____)\n#typeof(scorecard$_____)\n\n\n\nQuestion\n\nWrite code below to recode the admcon7 variable and store the results in a new variable in scorecard called admcon7_text. You’ll need to look up the values in the data dictionary. If you’ve done this correctly, running this code should produce a barplot that displays multiple bars.\n\n\nscorecard$admcon7_text <-\n  recode(\n    scorecard$admcon7, \n    #Fill replacements here\n    .default = NA_character_\n  )\n\nggplot(scorecard, aes(x = admcon7_text)) +\n  geom_bar() +\n  coord_flip()\n\n\n\nQuestion\n\nIn a code chunk below, calculate the number of missing values in both the costt4_a and the costt4_p columns. Reference the NOTES column in the data dictionary to determine why there are so many NA values in these columns. Add a comment to the code chunk below, explaining the reason for missing values in these columns.\n\n\n# Write code here\n\n# Add comment here"
  },
  {
    "objectID": "lab1.html#ethical-considerations",
    "href": "lab1.html#ethical-considerations",
    "title": "Lab 1: Understanding Datasets",
    "section": "Ethical Considerations",
    "text": "Ethical Considerations\nReferencing the College Scorecard data documentation, see if you can determine which students are included in calculations of earnings and debt. How might the data’s coverage bias the values that get reported? What might be the social consequences of these biases? Share your ideas on our sds-192-discussions Slack channel."
  },
  {
    "objectID": "solutions/problem-solving.html",
    "href": "solutions/problem-solving.html",
    "title": "Problem Solving",
    "section": "",
    "text": "Question\n\nCopy and paste one line of code from this lab as a threaded comment on the Slack message I posted on September 7, 2022. Be sure the text is formatted as code.\n\n\n\nQuestion\n\nAccess the help pages for the function sort(). Write code below to sort the vector a (which you created in an earlier step) in decreasing order.\n\n\n\n\n\n\n[1] 5 4 3 2 1\n\n\n\n\nQuestion\n\nNavigate to this cheatsheet for base R. Imagine we collected the temperature of our home each day for the past ten days (see code below).\n\n\n\nLet’s say we wanted to find how each day ranked from coolest to hottest. For instance, I wanted to know the ranking for day 1 vs. day 2 vs. day 3, and so on. Using the cheatsheet, find the function that will allow you to generate a ranking of each day’s temperature. Search the help pages for this function to determine how to randomly rank two days with the same temperature. Write a comment to yourself describing how this function is different than sorting the data.\n\n\n\n\n\n\n[1] 3 5 7 6 4 9 2 1 8\n\n\n\n\nQuestion\n\nAdd a comment to the code chunk above, citing this Stack Overflow post. Be sure to cite the post properly. You can read through the post to double check your answers."
  },
  {
    "objectID": "solutions/lab1.html",
    "href": "solutions/lab1.html",
    "title": "Lab 1: Understanding Datasets",
    "section": "",
    "text": "Question\n\nSee if you can identify a unique key for this dataset. Write some lines of code to determine whether the column you’ve identified can act as unique key for the data. Hint: You need to check whether the values in the column ever repeat.\n\n\n\n\n\n\nlogical(0)\n\n\n\n\nQuestion\n\nAt this point, you’ve probably figured out that each row in this dataset is a higher education institution. …but there are many different ways that we can define higher education institutions, and that will impact what gets included and excluded in our data. Referencing the Technical Documentation, locate a definition for the unit of observation in this dataset. What institutions are included, and what institutions are excluded? Summarize a definition below.\n\n\n\nQuestion\n\nFor each of the variable names in the scorecard data frame, look up the associated name in the data dictionary. You will need to search for the variable name in the sixth column of the data dictionary (I recommend using Ctrl-F to quickly locate the variable name in the spreadsheet.) Once you’ve found the variable name, reference column 1 to determine what this variable means, and reference columns 7 and 8 to see what possible values will appear in that column.\nIdentify one nominal variable, one ordinal variable, one discrete variable, and one continuous variable in scorecard and list their variable names below. Then uncomment the lines below and use the typeof() function to see how R determined their data types. Did any surprise you?\n\n\n\n\n\n\n[1] \"integer\"\n\n\n[1] \"integer\"\n\n\n[1] \"integer\"\n\n\n[1] \"double\"\n\n\n\n\nQuestion\n\nWrite code below to recode the admcon7 variable and store the results in a new variable in scorecard called admcon7_text. You’ll need to look up the values in the data dictionary. If you’ve done this correctly, running this code should produce a barplot that displays multiple bars.\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\nIn a code chunk below, calculate the number of missing values in both the costt4_a and the costt4_p columns. Reference the NOTES column in the data dictionary to determine why there are so many NA values in these columns. Add a comment to the code chunk below, explaining the reason for missing values in these columns.\n\n\n\n\n\n\n[1] 60\n\n\n[1] 116"
  },
  {
    "objectID": "templates/problem-solving.html",
    "href": "templates/problem-solving.html",
    "title": "Problem Solving",
    "section": "",
    "text": "Question\n\nCopy and paste one line of code from this lab as a threaded comment on the Slack message I posted on September 7, 2022. Be sure the text is formatted as code.\n\n\n\nQuestion\n\nAccess the help pages for the function sort(). Write code below to sort the vector a (which you created in an earlier step) in decreasing order.\n\n\n# Write code here\n\n\n\nQuestion\n\nNavigate to this cheatsheet for base R. Imagine we collected the temperature of our home each day for the past ten days (see code below).\n\n#Create a vector of temperatures\ntemps_to_factor <- c(68, 70, 78, 75, 69, 80, 66, 66, 79)\n\nLet’s say we wanted to find how each day ranked from coolest to hottest. For instance, I wanted to know the ranking for day 1 vs. day 2 vs. day 3, and so on. Using the cheatsheet, find the function that will allow you to generate a ranking of each day’s temperature. Search the help pages for this function to determine how to randomly rank two days with the same temperature. Write a comment to yourself describing how this function is different than sorting the data.\n\n\n# Write code here\n\n# Write comment here\n\n\n\nQuestion\n\nAdd a comment to the code chunk above, citing this Stack Overflow post. Be sure to cite the post properly. You can read through the post to double check your answers."
  },
  {
    "objectID": "lab1.html#why-not-use-name-as-a-unique-key",
    "href": "lab1.html#why-not-use-name-as-a-unique-key",
    "title": "Lab 1: Understanding Datasets",
    "section": "Why not use name as a unique key?",
    "text": "Why not use name as a unique key?\nNote that NAME is typically not an appropriate variable to use as a unique key. Let me provide an example to demonstrate this. When I worked for BetaNYC, I was trying to build a map of vacant storefronts in NYC by mapping all commercially zoned properties in the city, and then filtering out those properties where a business was licensed or permitted. This way the map would only include properties where there wasn’t a business operating. One set of businesses I was filtering out was restaurants. The only dataset that the city had made publicly available for restaurant permits was broken. It was operating on an automated process to update whenever there was a change in the permit; however, whenever a permit was updated, rather than updating the appropriate fields in the existing dataset, it was creating a new row in the dataset that only included the permit holder (the restaurant name), the permit type, and the updated fields. Notably the unique permit ID was not being included in this new row. We pointed this issue out to city officials, but fixing something like this can be slow and time-consuming, so in the meantime, we looked into whether we could clean the data ourselves by aggregating the rows that referred to the same restaurant. However, without the permit ID it was impossible to uniquely identify the restaurants in the dataset. Sure, we had the restaurant name, but do you know how many Wendy’s there are in NYC?"
  },
  {
    "objectID": "solutions/lab2.html",
    "href": "solutions/lab2.html",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "",
    "text": "Question\n\nIn the code below, adjust the size to 2 and the alpha to 0.5. Set the position to \"jitter\". Note how this changes the plot.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\nIn the plot below, color the points on the plot with a qualitative palette by setting col= to the categorical variable ROUTE_PREFIX_005B_L. Remember that, in this case, color is an aesthetic, so this must be added inside of the aes() function. Add a label for the legend by setting col= in the labs() function to a phrase that describes the color variable.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\nCopy the completed plot from the last exercise below but swap out the variable you mapped to the color aesthetic with a numeric variable from the dataset. Be sure to also adjust the legend label. What kind of palette gets created? What new information do you gain about Hampshire County bridges from the plot? Note your interpretations in your quarto file.\n\n\n\n\n\n\n\n\nQuestion\n\nIn the plot below, assign the shape of the data on the plot by setting shape= to the categorical variable STRUCTURE_KIND_043A_L. Remember that, in this case, shape is an aesthetic, so this must be added inside of the aes() function. Add a label for the legend by setting shape= in the labs() function to a phrase that describes the shape variable.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\nCopy one of your plots from above, and inside the aes() function, set size= to a numeric variable not already represented on the plot. Keep in mind that you’ll need to remove the size= attribute from geom_point() for the points to size correctly. Be sure to also adjust the legend labels. What new information do you gain about Hampshire County bridges from the plot?\n\n\n\n\n\n\n\n\nQuestion\n\nIn the plot below, change the y-axis scale from continuous to log10. You might reference the formula specified above or reference the ggplot cheatsheet for help. Note that your y-axis will appear in scientific notation. Convert it to comma notation by adding: labels = scales::comma as an argument in the scale function call.\n\n\n\nWarning: Removed 1 rows containing missing values (geom_point).\n\n\n\n\n\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 211 rows containing missing values (geom_point).\n\n\n\n\n\n\n\nQuestion\n\nThe Interstate Highway Act of 1956 (signed under Dwight D. Eisenhower) established the U.S.’s interstate highway system and is considered the country’s largest public works project. Adjust the x-axis scale of the plot below to help tell this story. Specifically, bin years into before 1954 (when Eisenhower was elected) and 1954-2021. Set the label at 1954 to “Eisenhower Elected” and the label at 2021 to “Today.”\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\nAdd `+ scale_color_brewer()` to the ggplot() object below and assign a categorical palette. Refer to the ggplot cheatsheet or the help pages for help with formatting the function call.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\nAdd `+ scale_color_distiller()` to the ggplot() object below and assign a sequential palette. By default the colors will be ordered from darkest to lightest. Add an argument to the function call to reverse the direction of the colors. Refer to the ggplot cheatsheet or the help pages for help with formatting the function call.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\nFacet the plot below by STRUCTURE_KIND_043A_L to see the degree of structural deficiencies for different kinds of highway bridges of different ages in Massachusetts."
  },
  {
    "objectID": "templates/lab2.html",
    "href": "templates/lab2.html",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "",
    "text": "Question\n\nIn the code below, adjust the size to 2 and the alpha to 0.5. Set the position to \"jitter\". Note how this changes the plot.\n\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(size = 1, alpha = 1, position = \"identity\") +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal()\n\n\n\n\n\n\nQuestion\n\nIn the plot below, color the points on the plot with a qualitative palette by setting col= to the categorical variable ROUTE_PREFIX_005B_L. Remember that, in this case, color is an aesthetic, so this must be added inside of the aes() function. Add a label for the legend by setting col= in the labs() function to a phrase that describes the color variable.\n\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       col = \"ADD LEGED LABEL HERE\") +\n  theme_minimal()\n\n\n\n\n\n\nQuestion\n\nCopy the completed plot from the last exercise below but swap out the variable you mapped to the color aesthetic with a numeric variable from the dataset. Be sure to also adjust the legend label. What kind of palette gets created? What new information do you gain about Hampshire County bridges from the plot? Note your interpretations in your quarto file.\n\n\n\nQuestion\n\nIn the plot below, assign the shape of the data on the plot by setting shape= to the categorical variable STRUCTURE_KIND_043A_L. Remember that, in this case, shape is an aesthetic, so this must be added inside of the aes() function. Add a label for the legend by setting shape= in the labs() function to a phrase that describes the shape variable.\n\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       shape = \"ADD LEGED LABEL HERE\") +\n  theme_minimal()\n\n\n\n\n\n\nQuestion\n\nCopy one of your plots from above, and inside the aes() function, set size= to a numeric variable not already represented on the plot. Keep in mind that you’ll need to remove the size= attribute from geom_point() for the points to size correctly. Be sure to also adjust the legend labels. What new information do you gain about Hampshire County bridges from the plot?\n\n\n\nQuestion\n\nIn the plot below, change the y-axis scale from continuous to log10. You might reference the formula specified above or reference the ggplot cheatsheet for help. Note that your y-axis will appear in scientific notation. Convert it to comma notation by adding: labels = scales::comma as an argument in the scale function call.\n\n\nnbi_ma %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = BRIDGE_IMP_COST_094)) +\n  geom_point(size = 1, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Bridge Improvement Costs\") +\n  theme_minimal() + \n  scale_y_continuous(labels = scales::comma)\n\nWarning: Removed 1 rows containing missing values (geom_point).\n\n\n\n\n\n\n\nQuestion\n\nThe Interstate Highway Act of 1956 (signed under Dwight D. Eisenhower) established the U.S.’s interstate highway system and is considered the country’s largest public works project. Adjust the x-axis scale of the plot below to help tell this story. Specifically, bin years into before 1954 (when Eisenhower was elected) and 1954-2021. Set the label at 1954 to “Eisenhower Elected” and the label at 2021 to “Today.”\n\n\nnbi_ma %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = ROUTE_PREFIX_005B_L)) +\n  geom_point(size = 1, alpha = 0.2, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Route Prefix\") +\n  theme_minimal()\n\n\n\n\n\n\nQuestion\n\nAdd `+ scale_color_brewer()` to the ggplot() object below and assign a categorical palette. Refer to the ggplot cheatsheet or the help pages for help with formatting the function call.\n\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067,\n             col = ROUTE_PREFIX_005B_L)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, Massachusetts, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       col = \"Route Prefix\") +\n  theme_minimal()\n\n\n\n\n\n\nQuestion\n\nAdd `+ scale_color_distiller()` to the ggplot() object below and assign a sequential palette. By default the colors will be ordered from darkest to lightest. Add an argument to the function call to reverse the direction of the colors. Refer to the ggplot cheatsheet or the help pages for help with formatting the function call.\n\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067,\n             col = ADT_029)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       col = \"Average Daily Traffic\") +\n  theme_minimal()\n\n\n\n\n\n\nQuestion\n\nFacet the plot below by STRUCTURE_KIND_043A_L to see the degree of structural deficiencies for different kinds of highway bridges of different ages in Massachusetts.\n\n\nnbi_ma %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(alpha = 0.1, size = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal()"
  },
  {
    "objectID": "exercises/data-fundamentals.html",
    "href": "exercises/data-fundamentals.html",
    "title": "Day 2: Data Fundamentals",
    "section": "",
    "text": "Navigate to the NYC Urban Park Ranger Animal Condition Response dataset.\nNote the information listed in the “About this Dataset” section. This is administrative metadata.\nNote the attachment in this section with the file name: UrbanParkRangerAnimalConditionResponse_DataDictionary_20181107.xlsx. This contains descriptive metadata for this dataset.\nScroll to the ‘Table Preview’ at the bottom of the page. This previews this data as a rectangular dataset.\nAnswer the following questions by discussing in your groups:\n\n\nWhat is the unit of observation in this dataset? In other words, what does each row signify? How do you know?\nHow frequently is this dataset updated? How do you know?\nWhat are the possible values for the Species Status variable in this dataset? How do you know?\nWhat is the value at index [4,4] in this dataset? How do you know?\nIdentify the index of one missing value in this dataset.\nIdentify one nominal variable, one ordinal variable, one discrete variable, and one continuous variable.\n\n\nAs a preview of what you will be able to do in a few weeks, here is a visualization of the most commons species for which services are requested in NYC!\n\n\nlibrary(tidyverse)\nnyc_urban_ranger <- \n  read_csv(\"https://data.cityofnewyork.us/api/views/fuhs-xmg2/rows.csv\",\n           name_repair = make.names)\n\nnyc_urban_ranger %>%\n  group_by(Species.Description) %>%\n  summarize(Count = sum(X..of.Animals)) %>%\n  top_n(10, Count) %>%\n  ggplot(aes(x = reorder(Species.Description, Count), y = Count)) +\n  geom_col() +\n  coord_flip() +\n  labs(title=\"Top 10 Most Common Species for which NYC Urban Park Rangers Assistance is Requested, 2018-2020\", x = \"Species\", y = \"Count\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size=8))"
  },
  {
    "objectID": "labs/lab6.html",
    "href": "labs/lab6.html",
    "title": "Lab 6: Joining Datasets",
    "section": "",
    "text": "In terms of data analysis, this lab has one goal: to determine the number of industrial facilities that are currently in violation of both the Clean Air Act and the Clean Water Act in California. To achieve this goal, we’re going to have to do some data wrangling and join together some datasets published by the EPA. We’re going to practice applying different types of joins to this data and consider what we learn with each.\n\n\n\nIdentify join keys\nDiscern the differences between types of joins\nDiscern what happens when we perform many-to-many joins\nApply filtering joins to identify missing data\nPractice data wrangling"
  },
  {
    "objectID": "labs/lab6.html#review-of-key-terms",
    "href": "labs/lab6.html#review-of-key-terms",
    "title": "Lab 6: Joining Datasets",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\n\n\n\n\n\nTip\n\n\n\nYou may wish to reference this Data Wrangling Cheatsheet when completing this lab.\n\n\n\nJoin Key\n\nA variable shared across two datasets that identifies the observations that will be merged"
  },
  {
    "objectID": "labs/lab6.html#twitter-dataset",
    "href": "labs/lab6.html#twitter-dataset",
    "title": "Lab 6: Joining Datasets",
    "section": "Twitter Dataset",
    "text": "Twitter Dataset"
  },
  {
    "objectID": "labs/lab6.html#setting-up-your-environment",
    "href": "labs/lab6.html#setting-up-your-environment",
    "title": "Lab 6: Joining Datasets",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nInstall the echor package in your Console.\nRun the code below to load today’s data frames into your environment.\n\n\nlibrary(tidyverse)\nlibrary(echor)\n\necho_air_ca <- \n  echoAirGetFacilityInfo(p_st = \"CA\", \n                         qcolumns = \"1,4,5,8,48,99,100,101,102\") |>\n  select(-SourceID)\n\necho_water_ca <- \n  echoWaterGetFacilityInfo(p_st = \"CA\", \n                           qcolumns = \"1,4,5,9,184,185,186,187\") |>\n  select(-SourceID)\n\n\nRun the code below to load the data dictionary for echo_air_ca into your environment.\n\n\necho_air_dd <- echoAirGetMeta() |>\n  filter(ColumnID %in% c(1,4,5,8,99,100,101,102)) |>\n  select(ColumnID, ObjectName, Description)\n\necho_water_dd <- echoWaterGetMeta() |>\n  filter(ColumnID %in% c(1,4,5,9,184,185,186,187)) |>\n  select(ColumnID, ObjectName, Description)\n\n\nOpen both echo_air_dd and echo_water_dd to review the data dictionaries for these datasets."
  },
  {
    "objectID": "labs/lab5.html",
    "href": "labs/lab5.html",
    "title": "Lab 5: Exploratory Data Analysis",
    "section": "",
    "text": "In 1968, the Supreme Court case Terry v. Ohio ruled that a police officer could stop an individual without “probable cause” but with “reasonable suspicion” the suspect had committed a crime. Further, an officer could frisk an individual without “probable cause” but with “reasonable suspicion” the suspect was carrying a weapon. In the early 1990s, stop, question, and frisk became a widely prevalent tactic in policing in NYC. However, as the practice expanded in the 2000s, civil rights organizations began calling attention to the ways in which it was fueling the over-policing of NYC neighborhoods, along with racial profiling.\nIn 2011, David Floyd and David Ourlicht filed a class action lawsuit (on behalf of themselves and other minority civilians in NYC) against the City of New York, Police Commissioner Raymond Kelly, Mayor Michael Bloomberg, and a number of NYPD officers. Their argument was that the NYPD had stopped them without “reasonable suspicion” and that the defendants had sanctioned policies for stopping civilians on the basis of race or national origin. Honorable Shira A. Scheindlin, who oversaw the US District Court Case, ultimately ruled that stop and frisk was being carried out in an unconstitutional way. The officer-reported data that we will examine in this lab was leveraged as key evidence throughout this case.\nIn this lab, you will apply the 6 data wrangling verbs we learned this week in order to analyze data regarding NYPD stop, question, and frisk. Specifically, we will replicate data analysis performed by the NYCLU in 2011 to demonstrate how the practice was being carried out unconstitutionally in New York. We are specifically going to look at three issues:\n\nHow many times did the NYPD stop an individual in 2011?\nIn how many 2011 stops was the stopped individual found to be guilty of a crime?\nIn how many 2011 stops that involved a frisk was the individual found to be carrying a weapon?\n\nWe are also going to examine the racial breakdown for each of these three questions to assess the extent to which stop and frisk was being carried out in a discriminatory way in 2011.\n\n\n\nApply data wrangling verbs to subset and aggregate data\nConsider the implications of racial categorization"
  },
  {
    "objectID": "labs/lab5.html#review-of-key-terms",
    "href": "labs/lab5.html#review-of-key-terms",
    "title": "Lab 5: Exploratory Data Analysis",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\n\n\n\n\n\nTip\n\n\n\nYou may wish to reference this Data Wrangling Cheatsheet when completing this lab.\n\n\n\nfilter\n\nfilters to rows (observations) that meet a certain criteria\n\nselect\n\nkeeps only selected variables (columns)\n\narrange\n\nsorts values in a variable (column)\n\nsummarize\n\ncalculates a single value by performing an operation across a variable (column); summarizing by n() calculates the number of observations in the column\n\ngroup_by\n\ngroups rows (observations) by shared values in a variable (column); when paired with summarize(), performs an operation in each group\n\nmutate\n\ncreates a new variable (column) and assigns values according to criteria we provide"
  },
  {
    "objectID": "labs/lab5.html#stop-question-and-frisk-dataset",
    "href": "labs/lab5.html#stop-question-and-frisk-dataset",
    "title": "Lab 5: Exploratory Data Analysis",
    "section": "Stop, Question, and Frisk Dataset",
    "text": "Stop, Question, and Frisk Dataset\nEvery time an officer stops a civilian in NYC, the officer is supposed to complete a UF-250 form (see image below) outlining the details of the stop.\n\n\n\nUF250 Side 2, Dan Nguyen’s Blog\n\n\n\n\n\nUF250 Side 2, Dan Nguyen’s Blog\n\n\nAs a result of some high profile police shootings in the late 1990s and mid-2000s, both the New York State Attorney General’s Office and the New York Civil Liberties Union began to examine NYPD stop and frisk activity for racial profiling. With pressure from these organizations, in the mid-2000s, information recorded on UF-250 forms began getting reported in public databases.\nSince then, the NYCLU has been using the data to conduct annual reports investigating the degree of racial profiling in stop and frisk activity in NYC. Through this research, they have been able to show that stops increased almost 700% from 2002 to 2011 - the year that marked the height of stop and frisk activity in NYC.\nWhen the NYPD’s use of stop and frisk went before the US District Court in the early 2010s, these public databases were integral in proving that stop and frisk was being carried out in NYC in an unconstitutional way. The ruling mandated that the NYPD create a policy outlining when stops were authorized, and since the practice has declined. See (Smith 2018) and (Southall and Gold 2019) for more information."
  },
  {
    "objectID": "labs/lab5.html#setting-up-your-environment",
    "href": "labs/lab5.html#setting-up-your-environment",
    "title": "Lab 5: Exploratory Data Analysis",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nRun the code below to load today’s data frames into your environment.\n\n\nlibrary(tidyverse)\n\nsqf_url <- \"https://www1.nyc.gov/assets/nypd/downloads/zip/analysis_and_planning/stop-question-frisk/sqf-2011-csv.zip\"\ntemp <- tempfile()\ndownload.file(sqf_url, temp)\nsqf_zip <- unzip(temp, \"2011.csv\")\nsqf_2011 <- read.csv(sqf_zip, stringsAsFactors = FALSE) \nsqf_2011_race_cat <- read.csv(\"https://raw.githubusercontent.com/lindsaypoirier/STS-101/master/Data/SQF/sqf_race_categories.csv\", stringsAsFactors = FALSE) \nrm(sqf_url)\nrm(temp)\nrm(sqf_zip)\n\n\nWe will analyze data for all stops in 2011. This data can be found here. However, we’re only going to be considering a subset of the columns. Review the data dictionary for the columns we’re using in this analysis.\n\n\n\n\n\n\n\n\n\nVARIABLE\nDEFINITION\nPOSSIBLE VALUES\n\n\n\n\npct\nPrecinct of the stop\n1:123\n\n\narrestsum\nWas an arrest made or summons issued?\n1 = Yes\n0 = No\n\n\nfrisked\nWas suspect frisked?\n1 = Yes\n0 = No\n\n\nwpnfound\nWas a weapon found on suspect?\n1 = Yes\n0 = No\n\n\nrace_cat\nSuspect’s race\nAMERICAN INDIAN/ALASKAN NATIVE\nASIAN/PACIFIC ISLANDER\nBLACK\nBLACK-HISPANIC\nOTHER\nWHITE\nWHITE-HISPANIC\n\n\nage\nSuspect’s age\n999 indicates a missing value\n\n\n\n\nThe original dataset codes each race with a single letter. Run the code below to add a column called race_cat that writes out each racial category in accordance with the data documentation. It also replaces every instance of “Y” in the dataset with 1 and every instance of “N” with 0. This will allow us to sum the Yes’s in the dataset. You’ll learn more about how this code works in coming weeks.\n\n\nsqf_2011 <- \n  sqf_2011 |> \n  select(pct, race, age, frisked, pistol, riflshot, asltweap, knifcuti, machgun, othrweap, sumissue, arstmade) |>\n  left_join(sqf_2011_race_cat, by = \"race\") |>\n  mutate(across(frisked:arstmade, \n         ~ case_when(. == \"Y\" ~ 1, . == \"N\" ~ 0)))\nrm(sqf_2011_race_cat)\n\n\nNavigate to the NYCLU’s webpage documenting statistics from analysis of annual stop and frisk data. Scroll to the data for 2011. In today’s lab, we are going to replicate their analysis in an attempt to produce/verify these numbers."
  },
  {
    "objectID": "labs/lab5.html#analysis",
    "href": "labs/lab5.html#analysis",
    "title": "Lab 5: Exploratory Data Analysis",
    "section": "Analysis",
    "text": "Analysis\nTerry v. Ohio ruled that officers with “reasonable suspicion” that a stopped individual was carrying a weapon may frisk that individual for weapons.\nIn order to assess how “reasonable suspicion” was being applied in NYC in 2011, we are eventually going to analyze how many 2011 stops resulted in a frisk, how many frisks resulted in a weapon found on an individual, and how many stops resulted in either an arrest or summons.\nThe original dataset had separate variables for indicating whether a pistol, rifle, assault weapon, knife, machine gun, or other weapon was found on a suspect. We will create a variable equal to 1 if any of these weapons were found on the suspect. Further, the original dataset had separate variables for indicating whether a stop resulted in an arrest made or summons issued. We will create a variable equal to 1 if either occurred.\n\nQuestion\n\nAdd two new columns. The first should indicate whether a weapon was found, and the second should indicate whether an arrest/summons was made.\n\n\nsqf_2011 <- \n  sqf_2011 |>\n  #Add a variable for weapon found\n  _____(wpnfound = case_when(pistol == 1 |\n                               riflshot == 1 | \n                               asltweap == 1 |\n                               knifcuti == 1 | \n                               machgun == 1 | \n                               othrweap == 1 ~ 1,\n                             TRUE ~ 0))\nsqf_2011 <- \n  sqf_2011 |>\n  #Add a variable for arrest made or summons issued\n  _____(arrestsumm = case_when(sumissue == 1 | \n                                arstmade == 1 ~ 1,\n                               TRUE ~ 0))\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\ncase_when() allows us to assign values based on whether certain criteria is met. Here is the basic formula:\ncase_when(\n  <criteria to check> ~ <set to this value if criteria is met>, \n  TRUE ~ <set to this value if criteria is not met>\n  )\nCheck out the code above. In the first line of code, we set the value for wpnfound to 1 if the first criteria is met (i.e. if pistol equals 1 or riflshot equals 1, and so on.) Otherwise, we set the value for wpnfound to 0.\n\n\nNow that you’ve run this code, open up the data frame in the View window. What is the unit of observation in this dataset? What variables describe each row?\n\n\n\n\n  \n\n\n\nWe don’t need all of these columns in this data frame for this analysis. In fact, to replicate the NYCLU’s 2011 analysis, we only need six columns from this data frame.\n\n\n\nQuestion\n\nSubset the dataset to the six variables listed in the data dictionary above.\n\n\nsqf_2011 <-\n  sqf_2011 |>\n  _____(pct, arrestsumm, _____, wpnfound, race_cat, _____)\n\n\n\n\n\n\nIn order to be able to determine the percentage of stops that resulted in a frisk, weapon found, or arrest/summons, we are going to need to know how many total stops were conducted in NYC in 2011. We are going to store that value in the variable total_stops so that we can refer to it later.\n\n\n\nQuestion\n\nCalculate the number of stops in 2011. If you are not sure which function to use below, you may want to refer to the list of Summary functions in the the Data Wrangling cheatsheet. Remember that each row in the data frame is a stop.\n\n\ntotal_stops <-\n  sqf_2011 |>\n  summarize(Count = _____) |>\n  pull()\n\ntotal_stops\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nsummarize() will return a data frame with the summarized value. Remember a data frame is a two-dimensional table with rows and columns (even if it’s a 1x1 table). If we want just the value, we can call pull() to extract the value from that two–dimensional table.\n\n\nRecall that for stops to be legally justified, officers must have “reasonable suspicion” that an individual committed a crime. This means that officers must be able to offer “specific and articulable facts,” along with rational inferences based on those facts, that an individual had committed a crime, in order to justify a stop. Reasonable suspicion requires more than a ‘hunch’.\nFor this reason, one of first metrics that the NYCLU calculates in their reports is the number and percentage of stops in which in the stopped individuals turned out to be innocent.\n\n\n\nQuestion\n\nHow many stops did not result in an arrest or summons in 2011? What percentage of stops did not result in an arrest or summons?\n\n\nsqf_2011 |>\n  #Subset to rows where suspect innocent\n  _____(arrestsumm _____ 0) |> \n  #Calculate number of observations\n  _____(total_innocent = n(), \n            percent_innocent = _____ / total_stops * 100)\n\n\n\n\n\n\nIn order to assess how stop and frisk may be carried out in a discriminatory way, the NYCLU’s reports go on to look at stops through the lens of a number of demographic indicators, including the age, race, gender, and ethnicity of the individuals stopped. For starters, let’s look at how many young people (14-24) were stopped in NYC in 2011.\n\n\n\nQuestion\n\nIn how many stops were the individuals aged 14-24? In what percentage of stops were the individuals aged 14-24?\n\n\nsqf_2011 |>\n  #Subset to rows where suspect age 14-24\n  _____(age _____ 14 & age _____ 24) |> \n  #Calculate number of observations and percentage of observations\n  _____(total_14_24 = _____, \n            percent_14_24 = n() / total_stops * 100)\n\n\n\n\n\n\nWhy doesn’t this match the values we see on the NYCLU website?\nNote the following from the NYCLU’s 2011 report on Stop, Question, and Frisk data:\n\n“In a negligible number of cases, race and age information is not recorded in the database. Throughout this report, percentages of race and age are percentages of those cases where race and age are recorded, not of all stops.”\n\nNote that it is very common in government datasets for missing values to get coded as 999 or -999. Here we want to exclude those values in the age column.\n\n\n\nQuestion\n\nFix the code below to calculate the currect number of stops for individuals 14-24.\n\n\ntotal_stops_age_recorded <-\n  sqf_2011 |>\n  #Subset to rows where age is not 999\n  _____(age _____ 999) |> \n  summarize(Count = n()) |>\n  pull()\n\nsqf_2011 |>\n  filter(age >= 14 & age <= 24) |>\n  summarize(total_14_24 = n(), \n            percent_14_24 = n() / total_stops_age_recorded * 100)\n\n\n\n\nThis still doesn’t match the values we see on the website, but it does match the values we see in the NYCLU’s 2011 report on Stop, Question, and Frisk data. This is typically when I would reach out to a representative at the NYCLU to inquire about the discrepancy.\n\n\nQuestion\n\nHow many stops were there per race in 2011? What percentage of stops per race in 2011? Arrange by number of stops in descending order.\n\n\ntotal_stops_race_recorded <-\n  sqf_2011 |>\n  #Subset to rows where race_cat is not NA or \"OTHER\"\n  filter(_____(race_cat) & race_cat _____ \"OTHER\") |> \n  summarize(Count = n()) |>\n  pull()\n\nsqf_2011 |>\n  #Subset to rows where race_cat is not NA or \"OTHER\"\n  _____(_____(race_cat) & race_cat _____ \"OTHER\") |> \n  #Group by race\n  _____(race_cat) |> \n  #Calculate number of observations\n  _____(stops = n(), \n            percent_stops = n() / total_stops_race_recorded * 100) |>\n  #Sort by stops in descending order\n  _____(_____(stops)) \n\n\n\n\n\n\nNote how this dataset categorizes race. Many different government datasets categorize race in many different ways. How we categorize race matters for how we can talk about discrimination and racial profiling. Imagine if WHITE was not categorized separately from WHITE-HISPANIC. The values in this dataset would appear very differently!\nWhen compiling their report, the NYCLU chose to aggregate two racial categories in this dataset into the one category - Latinx - in order to advance certain claims regarding discrimination. What we should remember is that these racial categories are not reported by those stopped; they are recorded by officers stopping individuals. They may not reflect how individuals identify themselves.\n\n\n\nQuestion\n\nIn how many stops were the individuals identified as Latinx (i.e. “WHITE-HISPANIC” or “BLACK-HISPANIC”)? In what percentage of stops were the individuals identified as Latinx?\n\n\nsqf_2011 |>\n  #Subset to rows where race_cat is \"WHITE-HISPANIC\" or \"BLACK-HISPANIC\"\n  _____(race_cat _____ c(\"WHITE-HISPANIC\", \"BLACK-HISPANIC\")) |> \n  #Calculate number of observations\n  _____(stops_Latinx = _____, \n            percent_Latinx = n() / total_stops_race_recorded * 100)\n\n\n\n\n\n\nAs a final consideration, we are going to calculate some summary statistics to further our understanding of the discriminatory ways in which this practice was being carried out. To so so, we are going to look at a racial breakdown of which stops resulted in an arrest/summons, and which frisks resulted in a weapon found.\n\n\n\nQuestion\n\nWhat percentage of stops in 2011 resulted in a frisk per race? What percentage of stops in 2011 resulted in a weapon found per race? What percentage of stops in 2011 resulted in an arrest or summons per race? In your resulting data table, each row should be a race, and there should be columns for stops, percent_stops, percent_frisked, percent_wpnfound , and percent_arrestsumm.\n\n\n# Write code here. \n\n\n\n\n\n\nQuestion\n\nBelow, in 2-3 sentences, summarize what you learn from reviewing the summary statistics from the code above. What does this tell us about the constitutionality of 2011 stop and frisk activity in NYC?\n\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nIn her ruling in the 2011 District Court case Floyd vs. the City of New York, Honorable Shira Sheidlen remarked the following regarding the database of stops we analyzed today:\n\nBecause it is impossible to individually analyze each of those stops, plaintiffs’ case was based on the imperfect information contained in the NYPD’s database of forms (“UF-250s”) that officers are required to prepare after each stop. The central flaws in this database all skew toward underestimating the number of unconstitutional stops that occur: the database is incomplete, in that officers do not prepare a UF-250 for every stop they make; it is one-sided, in that the UF250 only records the officer’s version of the story; the UF-250 permits the officer to merely check a series of boxes, rather than requiring the officer to explain the basis for her suspicion; and many of the boxes on the form are inherently subjective and vague (such as “furtive movements”). Nonetheless, the analysis of the UF-250 database reveals that at least 200,000 stops were made without reasonable suspicion.\n\nWhat are some of the consequences of the incompleteness and/or one-sided nature of this dataset? How should we think about and communicate its flaws vs its value? How might this data collection program be redesigned so as to represent more diverse perspectives? Share your ideas on our sds-192-discussions Slack channel.\n\n\n\n#If you finish early, I encourage you to attempt to plot some of this data below using `ggplot()`!"
  },
  {
    "objectID": "labs/lab4.html",
    "href": "labs/lab4.html",
    "title": "Lab 4: Git and GitHub",
    "section": "",
    "text": "There are many reasons that Git and GitHub are essential infrastructures for collaborative coding projects. For one Git saves snapshots of a code repository at different stages of a project so that we can track how it has changed over time and revert back to an older version if we discover a more recent error. We call this version control. Certain Git features also facilitate many people working on a coding project at once, by providing a number of tools to help prevent collaborators from over-writing each other’s work. These features also make it possible for developers to simultaneously modify, extend, and test components of the code without jeopardizing the project’s current functionality. I use GitHub for assignment submission in this course because it offers a number of features for commenting on and making suggestions in your code, which will be super helpful when reviewing your group projects. Further, GitHub supports code publication; by publishing code on GitHub, you contribute to an open access/free software community, enabling others to learn from and build off of your work.\nDespite all of these awesome benefits, there can be a significant learning curve when getting started with Git and GitHub. There are new vocabularies, workflows, and error mitigation strategies to learn when getting started. This lab is designed to help you get acquainted with the concepts behind Git and GitHub, suggested workflows for collaborating on projects in this course, and error resolution strategies.\n\n\n\nCreate, update, and close issues\nBranch a repo\nIssue pull requests\nAddress common push/pull errors\nAddress merge conflicts"
  },
  {
    "objectID": "labs/lab4.html#review-of-key-terms",
    "href": "labs/lab4.html#review-of-key-terms",
    "title": "Lab 4: Git and GitHub",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\nRepo\n\nCollaborative storage space for folders, documents, data, and code\n\nBranch\n\nAn isolated version of a repo that can be modified without affecting the main branch\n\nClone\n\nCreates a copy of a repo stored in a remote space (e.g. GitHub) to your local machine (e.g. your computer)\n\nPull\n\nDownloads the latest version of a repo from remote space (e.g. GitHub) to your local machine (e.g. your computer)\n\nStage\n\nThe process of marking which changes of the code are ready to be saved\n\nCommit\n\nA stored snapshot of a repo at a particular moment in time\n\nPush\n\nUploads commits from your local machine (e.g. your computer) to a remote space (e.g. GitHub)\n\nPull Request\n\nA request for modified code to be integrated with a different branch\n\nMerge\n\nThe process of integrating code modifications from one branch into another branch"
  },
  {
    "objectID": "labs/lab4.html#github-flow",
    "href": "labs/lab4.html#github-flow",
    "title": "Lab 4: Git and GitHub",
    "section": "GitHub Flow",
    "text": "GitHub Flow\n\ngit vs. GitHub.com\nAt the beginning of this semester, many of you installed git on your computers. git is a series of commands available in our computers to save snapshots of files at different moments in time. On the other hand, GitHub.com is a website where we can store projects that have been configured with git commands. As you’ve been accepting lab assignments for this course, project folders (what we will call repositories) have been created for you at GitHub.com. If you navigate to GitHub.com right now, you will see a list of these project folders, and you can click through each of the them to see changes that have been made. However, when you created projects for each of these repositories in RStudio, you were using git commands on your computer to copy the project from GitHub.com to your computer. Similarly, you were using git commands when you staged, committed, and pushed your changes back to GitHub.com. In sum, git is a series of command line tools to manage changes to files. GitHub.com is a Web hosting platform for storing git projects.\n\n\nRemote vs. Local\nWe can store, edit, and publish files on GitHub.com without ever copying them to our local computers. Files at GitHub.com are technically stored on a server somewhere, so we will refer to the projects stored at GitHub.com as remote (i.e. distant, far-off, etc). Similarly, we can use git commands to save snapshots of different versions of files on our local computers without ever pushing the changes to GitHub.com, so we will refer to the projects stored on on our computers as local. In this course, we need the ability to move changes to files between our local and remote repositories.\nWhy can’t we just do everything at GitHub.com or everything on our local machines?? Well GitHub.com doesn’t have the nice environment for writing and running code that you have in RStudio, so we need to be able to move things to your local computers so that you can work on the code in RStudio. On the other hand, if all the work was done on your local computer and never pushed to GitHub.com, I (your instructor) would never see it! So the first important consideration in the GitHub Flow is how do we move changes made on our local machines to GitHub.com and vice versa.\nThe great news is that you’ve already been doing this! The primary way that we move changes between remote and local is through two git commands: pull and push. Pull copies changes from a remote repository to our local machines. Push pushes changes from our local machines to a remote repository. It looks like this:\n\nTypically, we want to make sure that we always follow this flow:\n\nPull changes from GitHub.com (remote) to local.\nMake changes locally,\nPush changes to GitHub.com.\n\nAs we are going to see later in the lab, the most frustrating issues with GitHub emerge when we break this flow.\n\n\nTo Branch or Not to Branch?\nIn my opinion, there are two kinds of workflows for GitHub. There’s the quick and dirty version, and there’s the long and elegant version. We just went over the Quick and Dirty version, and you’ve already had practice in it when submitting all of your labs for the course. Below are the differences between these two workflows (don’t worry about if you don’t understand what the steps mean right now; we will learn all of them in the lab).\n\n\n\n\n\n\n\nQuick and Dirty Version\nLong and Elegant Version\n\n\n\n\n\nPull recent changes from GitHub to local machine.\nMake edits and save them\nStage and commit changes.\nPush changes from local machine to GitHub.\n\nIn the quick and dirty version, all of this occurs in the main branch.\n\nCreate an issue at GitHub.com\nBranch the repo at GitHub.com.\nPull recent changes from GitHub.com to local machine.\nMake edits in the new branch and save them.\nStage and commit changes.\nPush changes from local machine to GitHub.com.\nCreate a pull request at GitHub.com\nAssign a reviewer to review the proposed changes and wait for their approval.\nMerge changes, while also closing the issue and deleting the branch.\n\n\n\n\nTypically I recommend the long and elegant version for group work as it is designed to avoid errors and ensure that collaborators are all on the same page regarding changes to files. However, occasionally when you have to make small, quick changes to a file, and it won’t impact your teammate’s work, it will make more sense to follow the quick and dirty workflow. The goal for today is to get practice in the long and elegant version.\n\n\nRepo\n\nQuestion\n\nNavigate to GitHub Classroom to accept the assignment. For this assignment, you will work with your Project 1 team members. You can find your group number at CATME.org.\n\n\n\nThis will create a repository at GitHub.com called github-practice. You’ll notice that the repository has a few files - README.md, github-practice.Rmd, .gitingore. You’ll also notice in the repo’s right sidebar that your group members are listed as collaborators. This means that you all have access to read and write to this repository.\n\n\n\n\nClone\nCloning is a git command that copies a remote repository to your local machine. In other words, it will copy all of the project files in the remote GitHub.com folder to your local computer.\n\nQuestion\n\nAll group members should clone the repo to their RStudio environment. To do so, copy the repo’s URL at GitHub.com. You will want to make sure you are in the main project folder when you copy this URL; it won’t work if you’ve clicked through to any of the files. Then in RStudio click on File > New Project > Version Control > Git, and paste the copied URL into the window that appears. Note what you see in the RStudio files pane after cloning the repo.\n\n\n\nRemember that cloning creates a copy of a remote repo on a local machine. In creating this project, you’ve copied all of the files that make up the github-practice repo at GitHub.com to your local computer. This means that you will find all of the files associated with this repo by navigating to the folder where you created the project on your computer.\nIt’s important to note that this is not just any old folder on your computer though. By cloning, you’ve created a git folder. This means that the folder has been set up in a way where git can track the changes that you make to it over time, and it knows that there is a remote version of the repository somewhere that you might want to keep it consistent with. If you tried to just create a new folder on your computer, it wouldn’t necessarily have these nice features.\n\n\n\n\nIssues\n\nQuestion\n\nNavigate back to GitHub.com, and click on the repo’s Issues tab. Each member of your team should create an issue by clicking the ‘New Issue’ button. Title the issue: “Adding <your name> to the assignment.” Submit the issue, and to the left of the screen, assign the issue to yourself.\n\n\n\n\nIssues support project planning by allowing you to track changes you hope to make to your project over time. By assigning issues to certain collaborators on your project team, you can have clear documentation of who is responsible for what.\n\n\n\n\n\n\nTip\n\n\n\nIn my own projects, I use Issues for a number of purposes. Sometimes I use Issues to bugs that I notice in my code that need to be fixed. Other times I use them to track features that I would like to add to my code down the road. Oftentimes, in my public repositories, I encourage others that are using my code to submit issues to ask questions about how something works, to report bugs, or to request features.\n\n\n\n\n\n\nBranch\nWhen you first create a repository, all of the code will be stored in the main branch of the repository. If you were to think of a project like a tree growing up from the ground, then the main branch would be the like the trunk of the tree. We don’t want the main branch to break because the whole tree could come down. One goal of a branching workflow in GitHub is to keep the most stable and polished versions of code in the main branch.\nSo what do we do in the meantime - when we’re editing code, potentially breaking things, trying to sort out its bugs, and it’s not quite in that stable and polished state yet? That’s where branching comes in.\nWhen we create a branch of our repo, GitHub creates a separate copy of the repo where you can make changes without impacting what’s in the main branch. Later, once we’re done making changes and things are stable and polished, we will have the opportunity to merge those changes back into the main branch.\nThink of it another way: You write a rockstar first draft of a final paper, and you decide to send it to a few friends to review. You could just share the original document with them and tell them “Have at it! Edit away!” The problem is that, if they are making changes directly to the original document, you could lose some of that awesome original text. A better option would be to create separate copies for each teammate to edit, send them those copies, and then figure out how to layer in their edits later. This is like branching. The original document would be the main branch, and each copy sent to a friend would be a branch off of main. Friends can make as many edits as they want in their branch because you still have the original stable and polished copy. Later, we can merge their changes back into the main branch.\n\n\n\n\n\n\n\nTip\n\n\n\nBranching can get pretty wild in GitHub. You can have branches of branches of branches. I don’t recommend this. A good workflow is to create a branch for making specific changes, merge those changes back into main, delete the branch, and then create a new branch for the next batch of changes.\n\n\n\nQuestion\n\nClick on the Code tab on your repo’s page at GitHub.com. Directly below this tab, you will see a dropdown that is currently labeled “main.” This means that you are in the main branch. Each member of your team should click the down arrow, and create a branch by entering their first name into the textbox that appears, and then clicking “Create branch.”\n\n\n\n\n\nPull\nAs of right now, the branches that were created in the previous step only exist on GitHub.com, they don’t exist yet on your local machine. To get these changes to your local machine, you need to Pull the changes. Remember how we said that the super fancy git folder knows that there is a remote version of the repository somewhere that you might want to keep it consistent with? When we pull changes to our local machines, we are basically saying, check that remote version for changes, and then pull them into the repo on my computer.\n\nQuestion\n\nHead back to RStudio. In the Environment pane, you will notice a tab labeled “Git.” It’s important to note that this tab will only appear in projects that are built from super fancy git folders. This is your RStudio command center for Git and GitHub. When you click on this tab, you will see a few buttons in the navigation bar. To pull changes, you should click the blue downward arrow. Click this button to pull the branches created remotely to your local machine.\n\n\n\n\n\nSwitch Branches\nEven though you pulled the new branches to your local machine, you are still currently working in the main branch. Remember that we always want to keep the main branch stable and polished. This is not where we are going to make edits. Instead, you will make edits in the branch that you just created. Later, we will merge those changes back into the main branch.\n\nQuestion\n\nIn the top right hand corner of the Git tab, you will see a dropdown currently set to “main”. Click the downward arrow, and switch to your branch by selecting the appropriate branch.\n\n\n\n\n\nMake Changes\n\nQuestion\n\nOnce in your branch, open github-practice.Rmd from the files pane. Decide within your group who will be Group member 1, 2, 3, and so on. Each group member should edit this file on their own local machines by adding their name and only their name to the appropriate location in the document (line 5, 7, or 9) based on their assigned number. It’s very important that this be the only section of the document you edit. Save the file by clicking File>Save.\n\n\n\n\nStage\nSometimes we make a changes to a few files, save them, and we’re ready to create a snapshot of our repo (i.e. create a commit) with some of those changes. Remember that creating this snapshot is almost like taking a photo of the repo at this particular moment, allowing us to later go back to that photo to see what the repo looked like in that moment. To let Git know which changes we want to include in that snapshot, we need to stage the files. Staging basically says, “these files are ready to be included in the snapshot.”\n\nQuestion\n\nOnce you save the file you’ll notice in the RStudio Git pane that the file name appears after a blue square labeled “M” (which stands for Modified). This means that the file is ready for staging. Stage the file - indicating that it’s ready for committing - by clicking the checkbox in front of the file name.\n\n\n\n\n\nCommit\nAs we just noted, committing changes basically means taking a snapshot of a repo at a particular moment in time. Commits are given unique hashes - sort of like a unique identifier that enables us to access the snapshot of the repo at a later date. In collaborative projects, it is typically recommended to commit often - after any major changes are made to a file. This ensures that we can eventually go back to look at very specific changes. It’s also important to label commits with descriptive titles so that we can recall what changes within that commit. Descriptive titles should detail what changes were made in the last round of edits.\nTo help put this into context, think back to our photograph metaphor. Let’s say that we are a photographer assigned to document how a baby develops in the first year of its life. If the photographer only took one photograph when the baby was 1 year old, we wouldn’t have a lot of documentation regarding how the baby developed! …so instead, let’s say that the photographer took a snapshot of the baby after every major milestone - their first laugh, their first solid food, their first crawl, their first word. We would have a lot more to go by when trying to understand how the baby developed. Same goes for committing code often.\nNow let’s say the photographer handed the batch of photos to the parents, and said - “look, here’s how your baby developed over time.” The parents might not remember which photograph was taken after which milestone. …but if the photographer were to label each photograph with things like “baby had first laugh,” the parents would be able to easily go back to specific moments in their baby’s development. This is why we want descriptive commit messages. We want to later be able to go back and scan through what changes were made after each commit.\nTo get a sense of what it means to be able to look back on these changes, check out the latest commits that I made to the GitHub repo for our course website. While I’m not going to claim to be the most diligent commit-er, you do get a basic sense of what changes were made to the repo following each commit from these commit messages. I could click through any of these links to see what my repo looked like at this moment in time.\n\n\nQuestion\n\nCommit your changes by clicking the ‘Commit’ button in the Git pane. When you click this button, a new window will open showcasing the changes that have been made to the staged file. You should enter a commit message in the window that appears. Remember that commit messages should be descriptive. In this case, something like “added <your-name>’s name” would work. Click commit. Now a snapshot of this version of the code repo has been taken.\n\n\n\n\nPush\nOnly your local machine knows that a change has been made to the code. Remember again how we said that the super fancy git folder knows that there is a remote version of the repository somewhere that you might want to keep it consistent with? Now we want to do the opposite of pulling changes from GitHub to our local machines. Instead, we want to push the changes on our local machines to GitHub.\n\nQuestion\n\nClick the Green upward arrow in the Git pane to push your changes to GitHub.\n\nOnce all group members have pushed their changes, head back over to GitHub. On the main code page, switch between branches and check out the contents of github-practice.Rmd in each branch. What differences do you notice?\n\n\n\n\nPull Request\nNote that now we have a few versions of our repo in separate branches on GitHub.com, and in each of those versions of the repo, the github-practice.Rmd file looks a little bit different. Now that we’ve made our changes and things are stable and polished, we want to move all of those changes into the main branch. To do this, we are going to issue a Pull Request. This is a request that signals to all of our collaborators that we are ready to move our changes back into the main branch.\n\nQuestion\n\nOn your repo’s page in GitHub.com, click the “Pull Requests” tab, and then click the green “New Pull Request” button. You’re requesting to pull the changes from your personal branch into the main branch. This means that the base branch should be main, and the compare branch should be your personal branch.\n\nYou’ll see a screen where you can compare your branch to the main branch. Click the button to “Create Pull Request,” enter a descriptive title of the changes made, and then click “Create Pull Request” again.\n\n\n\n\nReview Pull Requests\nI recommend that you get in the habit of reviewing your collaborator’s work before merging their changes into the main branch. By creating pull requests, we scaffold an opportunity to review each other’s work before fully integrating the changes.\nNow there should be a pull request for all members of your team. Assign one team member to review one other team member’s code. All team members should have one reviewer.\n\nQuestion\n\nOpen your own pull request in GitHub.com, and in the right sidebar, assign the team member responsible for reviewing your changes as a “Reviewer.”\n\nThen navigate to the pull request you are responsible for reviewing. Click on the “Files Changed” tab. Note that the left side of the screen shows the previous version of the file, and the right side of the screen shows the new version of the file. Lines in red have been deleted, and lines in green have been added.\n\nAfter looking through the changes, click the green button “Review changes.” Leave a note for your collaborator, indicating your evaluation of their changes. If everything looks good, check the radio button for “Approve.” If there are issues, check the radio button for “Request Changes.” Then click the button to “Submit Review.”\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf your reviewer requested changes, you should go back to RStudio, and make sure you are in your own branch. Then make the requested changes, save the file, stage the file, commit the changes, and push again. The changes to your file will be tracked in your pull request. After this, you may move on to the Merge step. See here for further options on dismissing or re-requesting reviews.\n\n\n\n\n\n\nMerge\nOnce all reviewers have approved changes, we are ready to merge those changes into the main branch. Open each pull request. If everything is good and ready to merge, you will see a green checkmark that says “This branch has no conflicts with the base branch.”\n\n(If you get a message that there are conflicts, call myself or one of the Data Assistants over.)\n\nQuestion\n\nClick the button to “Merge Pull Request”. In the comment box that appears, enter the text “closes #”. When you enter this text, you will see a dropdown of issues and pull requests currently in the repo. Issues will have an icon that appears as a circle with a dot in the center.\n\nSelect the issue associated with this pull request, and then click “Confirm Merge.” This will both merge the changes into the main branch and simultaneously close the issue you opened earlier. Finally, click the button to the delete the branch. Once this has been completed for all pull requests, head back over to the “Code” tab at GitHub.com, and check out github-practice.Rmd. What has happened to the file since merging the code? Navigate to the “Issues” tab. What has happened to the issues since confirming the merge?\n\n\n\n\n\n\n\n\n\nBefore moving on to the next section...\n\n\n\nYou’ve now deleted branches at GitHub.com that your local machines don’t know have been deleted. Before moving on to the next step, you should navigate back to RStudio and pull these changes to your local machine by clicking the blue downward arrow. To streamline things in the next section of the lab, we are going to work entirely in the main branch (something that I would otherwise not recommend)."
  },
  {
    "objectID": "labs/lab4.html#error-resolution",
    "href": "labs/lab4.html#error-resolution",
    "title": "Lab 4: Git and GitHub",
    "section": "Error Resolution",
    "text": "Error Resolution\nThe workflow presented above seems to work all fine and dandy. …but there are a number of factors that can impede this seamless workflow. In this final section of the lab, we will go over three kinds of errors that you might come across in the workflow above, and talk about how you would resolve them. I can almost guarantee that you will deal with some of these issues when working on your group projects, so I would encourage you to keep this lab handy when engaging in project work.\n\nPush error\nA push error occurs when we make changes to files on our local machines, and go to commit and push them to GitHub.com, but other changes had already been made to the file at GitHub.com that were not yet pulled into our local environments. We get an error because our local repo is inconsistent with the remote repo. To fix this error, we need to pull changes to our local machine, and try committing and pushing again. Let’s replicate this error:\n\nQuestion\n\nOne of your partners should navigate to the GitHub.com repository. Click on the file README.md. Click the pencil icon to edit the file. Replace the text: ADD NAME 1 HERE ADD NAME 2 HERE, and so on with your names. Scroll to the bottom of the page and commit changes noting in the message that your names were added.\nOther partner: Return to RStudio. Do not pull changes yet. Open github-practice.Rmd. On line 40 change the ncol() function to dim(). Save the file. Stage and commit your changes. Click the green upward arrow to push your changes. You should get an error that looks like this.\n\nFollow the steps above to resolve the error.\n\n\n\nAn easy way to avoid a push error is to always click the blue downward arrow to pull remote changes before starting to edit files on your local machine.\n\n\n\n\nPull error\nA pull error occurs when changes have already been made to the same location (the same line number) in both a remote file and a local file, and then we try to pull the changes from the remote repo to our local machines. As far as Git can tell, there are two options for what this line is supposed to look like, and it can’t tell which to prioritize. So Git recommends that, as a first step, we commit the changes that we made locally. It’s basically saying, let’s take a snapshot of your local repo as it looks right now, so that later we can figure out what to do about this conflict.\nIf this seems confusing imagine this: let’s say you write a paper, and you share it with one of your classmates to review. The classmate reads through it, makes suggested changes to the opening sentence, and sends it back you. …but, while your classmate was reviewing the paper, you were getting antsy about the paper deadline and started making your own edits to the paper, including edits to the opening sentence. Now you’re trying to incorporate the changes from your classmate’s review, and you’re not sure what to do about that opening sentence. As a first step, you have two options you can scrap your recent changes (maybe your classmate’s suggestions were better!) or you can save a separate copy of the file with your recent changes and figure out later how to resolve the differences. That’s exactly what we are going to do here:\nTo fix this error, you should stage and commit your local changes and then try pulling again. Let’s replicate this error:\n\nQuestion\n\nOne of your partners should navigate to the GitHub.com repository. Click on the file github-practice.Rmd. Click the pencil icon to edit the file. Replace the code on line 47 with the following: colnames(pioneer_valley_2013).\nScroll to the bottom of the page and commit changes noting in the message how you updated the function.\nOther partner: Return to RStudio. Do not pull changes yet. Open github-practice.Rmd. Replace the code on line 47 with the following: ncol(pioneer_valley_2013)\nSave the file. Click the blue downward button to Pull changes. You should get an error that looks like this.\n\nFollow the steps above to resolve the error.\n\n\n\n\nMerge conflict\nSo now we have these two snapshots of github-practice.Rmd, and they are in conflict with one another. If we try to push our changes back to GitHub.com, Git is not going to know what to do. Should the file at GitHub.com look like the version currently at GitHub.com, should it look like the snapshot that we just commit to our local machines, or should it look like something else entirely?\nLet’s return to the example of trying to incorporate a peer’s edits to a paper that you have recently made changes to. We have to figure out what to do about that opening sentence. Do we want our version, their version, or some combination of the two? This is what it is like to fix a merge conflict.\nTo fix this error, open the file with conflicts and edit the lines with conflict.\n\nQuestion\n\nOne of your partners should try to pull changes by clicking the blue downward arrow in RStudio. You will get an error that looks like this:\n{fig-alt=“This is the window that we see when we get a merge conflict. It says: CONFLICT (content): Merge conflict in README.md}\nTo fix this one your partners should open the file with the conflict. In this case it will be github-practice.Rmd. Scroll to the section of the file with the conflict. It will now look something like this:\n    <<<<<<< HEAD\n\n    ncol(pioneer_valley_2013)\n\n    =======\n\n    colnames(pioneer_valley_2013)\n\n    >>>>>>> ee175895783b64e0e1f696d9456be4c4c7c3f3bf\nThe code following HEAD represents the recent changes you made on your local machine, and the code right before the long string of characters represents the changes that were made in an earlier commit (the long string of characters is the commit hash). Decide what that line should look like and delete all other content. This means you must delete “<<<<<<< HEAD”, “=======”, and “>>>>>>> <long-hash>”, and you likely should delete at least one other line. Save the file, stage the file by clicking the checkbox next to the file in the Git page, and then commit your changes, and push them to GitHub.com.\n\n\n\n\n\n\n\n\n\nAvoiding Merge Conflicts\n\n\n\nYou may have noticed that the most frustrating merge conflicts tend to emerge when we have two people working on the same line of a repo’s file. The most effective way to avoid merge conflicts is to ensure that collaborators are working on different documents or different lines in a file. One way you might do this when starting to work on your group project is to open a file that you all plan to work on and having one of your team mates section off space of that file for different people to work. It might look something like this:\n\nOnce this change has been made, that group mate should stage, commit, and push the file to GitHub, and all other group mates should pull the change to their local machines.\n\n\n\n\n\n\nPath of Least Resistance\nI have been working with GitHub for years, and even to this day, I run into instances where things become so inconsistent between my local machine and the repo at GitHub.com that the fastest way to fix things is just to save local copies of the files that I’ve changed to somewhere else on my machine, delete the super fancy Git folder from its current location, and then re-clone the most up-to-date remote version to my local machine. Then I can figure out how I want to edit the most up-to-date version with my changes. This comic from XKCD captures this widely acknowledged solution beautifully:\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nIncreasingly, when data science researchers publish a paper in a journal, they are making the code they used to reach certain results freely available on GitHub.com for other researchers and the public to review. This is in part a response to the reproducibility crisis that you learned about in SDS 100. What do you see as the social benefits to making the code behind a data science finding publicly available online? What might be some of the social consequences of making this code freely available? How might we mitigate these consequences? Share your ideas on our sds-192-discussions Slack channel."
  },
  {
    "objectID": "labs/problem-solving.html",
    "href": "labs/problem-solving.html",
    "title": "Problem Solving",
    "section": "",
    "text": "This lab will introduce you to resources and techniques for problem solving in R. You should reference this lab often throughout the semester for reminders on best practices for addressing errors and getting help.\n\n\n\nInterpret error messages in R\nRead R cheatsheets\nAccess R help pages\nReference Stack Overflow and other online resources for help"
  },
  {
    "objectID": "labs/problem-solving.html#interpreting-error-messages",
    "href": "labs/problem-solving.html#interpreting-error-messages",
    "title": "Problem Solving",
    "section": "Interpreting Error Messages",
    "text": "Interpreting Error Messages\nThroughout this week, we have taken a look at different error messages that R presents when it can’t evaluate our code. Today, we will consider these in more detail. First, it’s important to make some distinctions between the kinds of messages that R presents to us when attempting to run code:\n\nErrors\n\nTerminate a process that we are trying to run in R. They arise when it is not possible for R to continue evaluating a function.\n\nWarnings\n\nDon’t terminate a process but are meant to warn us that there may be an issue with our code and its output. They arise when R recognizes potential problems with the code we’ve supplied.\n\nMessages\n\nAlso don’t terminate a process and don’t necessarily indicate a problem but simply provide us with more potentially helpful information about the code we’ve supplied.\n\n\nCheck out the differences between an error and a warning in R by reviewing the output in the Console when you run the following code chunks.\n\nError in R\n\nsum(\"3\", \"4\")\n\nError in sum(\"3\", \"4\"): invalid 'type' (character) of argument\n\n\n\n\nWarning in R\n\nvector1 <- 1:5\nvector2 <- 3:6\nvector1 + vector2\n\nWarning in vector1 + vector2: longer object length is not a multiple of shorter\nobject length\n\n\n[1]  4  6  8 10  8\n\n\nSo what should you do when you get an error message? How should you interpret it? Luckily, there are some clues and standardized components of the message the indicate why R can’t execute the code. Consider the following error message that you received when running the code above:\nError in sum(“3”, “4”) : invalid ‘type’ (character) of argument\nThere are three things we should pay attention to in this message:\n\nThe word “Error” indicates that this code did not run.\nThe text immediately after the word “in” tells us which specific function did not run.\nThe text after the colon gives us clues as why the code did not run.\n\nReviewing the error above, I can guess that there was a problem with the argument that I supplied to the sum() function, and specifically that I supplied a function of the wrong type.\n\nQuestion\n\nRun the codes below and check out the error messages. Review the code to fix each of the errors. Note that each subsequent code chunk relies on the previous code chunk, so you will need to fix the errors in order and run the chunks in order.\n\n\n# Create three vectors\na <- 1, 2, 3, 4, 5\nb <- \"a\", \"b\", \"c\", \"d\", \"e\"\nc <- TRUE, FALSE, TRUE, TRUE, FALSE\n\nError: <text>:2:7: unexpected ','\n1: # Create three vectors\n2: a <- 1,\n         ^\n\n\n\n\n\n\n# Add the values in the vector a\na_added <- add(a)\n\nError in add(a): could not find function \"add\"\n\n\n\n\n\n\n# Multiply the previous output by 3\nthree_times_a_added <- added_a * 3\n\nError in eval(expr, envir, enclos): object 'added_a' not found\n\n\n\n\n\n\n# Create a dataframe with col1 and col2\ndf <- data.frame(\n  col1 = c(1, 2, 3)\n  col2 = c(\"a\", \"b\", \"c\")\n\nError: <text>:4:3: unexpected symbol\n3:   col1 = c(1, 2, 3)\n4:   col2\n     ^\n\n\n\n\n\n\n# Add a new column to df\ndf$col3 <- c(TRUE, FALSE)\n\nError in df$col3 <- c(TRUE, FALSE): object of type 'closure' is not subsettable"
  },
  {
    "objectID": "labs/problem-solving.html#preparing-to-get-help",
    "href": "labs/problem-solving.html#preparing-to-get-help",
    "title": "Problem Solving",
    "section": "Preparing to Get Help",
    "text": "Preparing to Get Help\nWhen we do get errors in our code and need to ask for help in interpreting them, it’s important to provide collaborators with the information they need to help us. Sometimes when teaching R I will hear things like: “My code doesn’t work!” or “I’m stuck and don’t know what to do,” and it can be challenging to suss out the root of the issue without more information. Here are some strategies for describing issues you are having with your code:\n\nReference line numbers. Notice the left side of your template document has a series of numbers listed vertically next to each line? These are known as line numbers. Oftentimes, if you are having an issue with your code and ask me to review it, I will say something like: “Check out line 53.” By this I mean that you should scroll the document to the 53rd line. You can similarly tell me or your peers which line of your code you are struggling with.\nCompose good reproducible examples. A good reproducible example includes all of the lines of code that we need to reproduce an output on our own machines. This means that if you create a vector in a previous code snippet and then supply it as an argument in another code snippet, you are going to want to make sure that both of these lines of code appear in your reproducible example. Further, if the functions that you are using are from certain packages, you will want to make sure the library() call to load that package is in your reproducible example.\nUse the code and code block buttons in Slack to share example code. Certain characters like quotation marks and apostrophes are treated differently across RStudio and Slack. This is because these programs encode characters differently. For example, run the code chunk below and check out the output in your Console. The first line of code I typed directly into RStudio. The second I copied over from Slack.\n\n\n# typed directly into RStudio\ntoupper(\"apple\")\n# copied from Slack\ntoupper(“apple”)\n\nError: <text>:4:9: unexpected input\n3: # copied from Slack\n4: toupper(“\n           ^\n\n\nNotice the slight differences in the shape of the quotation marks? R recognizes the first but doesn’t recognize the second, even though I used the same keyboard key to create both. This is due to the fact that these two systems use different character encodings.\nThe Code button (for a single line of code) and Code Block button (for multiple lines of code) in Slack are useful tools for composing code and avoiding character encoding issues. If you click these buttons when typing a Slack message, you can enter code in the red outlined box that appears, and this will easily copy to RStudio. I will ask you to always use these features when copying code this semester.\n\n\n\n\n\n\nTip\n\n\n\nIn Slack, you can also wrap text backticks (` `) to have it output in a single-line code block, and three backticks (``` ```) to have it output as a multi-line code block.\n\n\n\nQuestion\n\nCopy and paste one line of code from this lab as a threaded comment on the Slack message I posted on September 7, 2022. Be sure the text is formatted as code."
  },
  {
    "objectID": "labs/problem-solving.html#help-pages",
    "href": "labs/problem-solving.html#help-pages",
    "title": "Problem Solving",
    "section": "Help pages",
    "text": "Help pages\nOne resource we’ve already discussed are the R help pages. I tend to use the help pages when I know the function I need to use, but can’t remember how to apply it or what its parameters are. Help pages typically include a description of the function, its arguments, details about the function, the values it produces, a list of related functions, and examples of its use. We can access the help pages for a function by typing the name of the function with a question mark in front of it into our Console (e.g. ?log or ?sum). Some help pages are well-written and include helpful examples, while others are spotty and don’t include many examples.\n\nQuestion\n\nAccess the help pages for the function sort(). Write code below to sort the vector a (which you created in an earlier step) in decreasing order.\n\n\n# Write code here"
  },
  {
    "objectID": "labs/problem-solving.html#cheatsheets",
    "href": "labs/problem-solving.html#cheatsheets",
    "title": "Problem Solving",
    "section": "Cheatsheets",
    "text": "Cheatsheets\nThe R community has developed a series of cheatsheets that list the functions made available through a particular package and their arguments. I tend to use cheatsheets when I know what I need to do to a dataset in R, but I can’t recall the function that enables me to do it.\n\nQuestion\n\nNavigate to this cheatsheet for base R. Imagine we collected the temperature of our home each day for the past ten days (see code below).\n\n#Create a vector of temperatures\ntemps_to_factor <- c(68, 70, 78, 75, 69, 80, 66, 66, 79)\n\nLet’s say we wanted to find how each day ranked from coolest to hottest. For instance, I wanted to know the ranking for day 1 vs. day 2 vs. day 3, and so on. Using the cheatsheet, find the function that will allow you to generate a ranking of each day’s temperature. Search the help pages for this function to determine how to randomly rank two days with the same temperature. Write a comment to yourself describing how this function is different than sorting the data.\n\n\n# Write code here\n\n# Write comment here"
  },
  {
    "objectID": "labs/lab1.html",
    "href": "labs/lab1.html",
    "title": "Lab 1: Understanding Datasets",
    "section": "",
    "text": "This lab is all about learning to understand the context and parts of a dataset by referencing and interpreting data dictionaries and technical data documentation.\n\n\n\nRead a data dictionary\nReference data documentation\nIdentify unique observations in a dataset\nUnderstand different variable types\nLook up value codes and recode a variable\nDetermine the number of missing values in a variable and why they are missing"
  },
  {
    "objectID": "labs/lab1.html#review-of-key-terms",
    "href": "labs/lab1.html#review-of-key-terms",
    "title": "Lab 1: Understanding Datasets",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\nRectangular Datasets\n\ndatasets in which all rows are the same length, and all columns are the same length\n\nObservations\n\nrows in a dataset; represent discrete entities we observe in the world\n\nVariables\n\ncolumns in a dataset; describe something about an observation\n\nVector\n\none-dimensional set of values that are all of the same type\n\nData Frame\n\na list of vectors of equal lengths; typically organizes data into a two-dimensional table composed of columns (the vectors) and rows\n\nUnique Key\n\nvariable (column) in the dataset that can be used to uniquely identify each row\n\nNominal categorical variables\n\nvariables that identify something else; sometimes, numbers are considered nominal categorical variables (e.g. zip code)\n\nOrdinal categorical variables\n\ncategorical variables that can be ranked or placed in a particular order (e.g. High, Medium, Low)\n\nDiscrete numeric variables\n\nnumeric variables that represent something that is countable (e.g. the number of students in a classroom, the number pages in a book)\n\nContinuous numeric variables are variables\n\nvariables in which it is always possible to measure the value more precisely (e.g. time can be measured with infinite amount of specificity - hours > minutes > seconds > milliseconds > microseconds > nanoseconds …)"
  },
  {
    "objectID": "labs/lab1.html#scorecard-dataset",
    "href": "labs/lab1.html#scorecard-dataset",
    "title": "Lab 1: Understanding Datasets",
    "section": "Scorecard Dataset",
    "text": "Scorecard Dataset\nIn his 2013 State of the Union Address, President Barack Obama announced his plans to create a “college scorecard” that would allow prospective students and parents to compare schools in terms of cost, offerings, diversity, completion rates, and post-graduate earnings. This data was first published in 2015 and since has undergone several improvements and revisions.\nThe College Scorecard dataset is massive. In fact, I thought long and hard about whether this was really the first dataset I wanted to introduce to you in a lab. It includes information about over 6500 institutions in the U.S., and has more than 3000 columns documenting information about those institutions. I chose this dataset for this lab because, if you can learn to read this data dictionary, you will be leaps and bounds ahead of the game in learning to read other data dictionaries. (It’s also just a super cool dataset, and hint, hint: you will get a chance to dive into it in much more detail in a few weeks). While the full data is available online, we are only going to work with a small subset of the data today."
  },
  {
    "objectID": "labs/lab1.html#setting-up-your-environment",
    "href": "labs/lab1.html#setting-up-your-environment",
    "title": "Lab 1: Understanding Datasets",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nInstall the RScorecard package by entering the following into your Console: install.packages(\"rscorecard\")\nCreate a Scorecard API Key at this link. Shortly after you fill out the form, you will be emailed a key. Copy that key into code chunk below, replacing all of the following text in sc_key(): Sys.getenv(“SCORECARD_KEY”). Be sure to wrap the key in quotation marks.\nDownload the Scorecard Data Dictionary and Technical Documentation for Institution-Level Data Files here.\nRun the code below to the import 2018 Scorecard data for Massachusetts into R. Call me or one of the data assistants over if you get an error.\n\n\nlibrary(tidyverse)\nlibrary(rscorecard)\nsc_key(Sys.getenv(\"SCORECARD_KEY\")) # Replace Sys.getenv(\"SCORECARD_KEY\") here with your API Key in quotation marks\n\nscorecard <- sc_init() |>\n  sc_year(2018) |>                 #Note how we are looking at only 2021 data here!\n  sc_filter(stabbr == \"MA\") |>     #Note how we are looking at only Massachusetts data here!\n  sc_select(unitid, instnm, city, highdeg, control, ugds, adm_rate, costt4_a, costt4_p, pcip27, pctfloan, admcon7, wdraw_orig_yr2_rt, cdr3) |>\n  sc_get()"
  },
  {
    "objectID": "labs/lab1.html#glimpsing-the-data",
    "href": "labs/lab1.html#glimpsing-the-data",
    "title": "Lab 1: Understanding Datasets",
    "section": "Glimpsing the Data",
    "text": "Glimpsing the Data\nWhen working with very large datasets, we need tools to help us get a sense of the dataset without having to load the entire data frame. For instance, we can view the first 6 rows of the dataset by calling head().\n\nhead(scorecard)\n\n# A tibble: 6 × 15\n  unitid instnm    city  highdeg control  ugds adm_rate costt4_a costt4_p pcip27\n   <int> <chr>     <chr>   <int>   <int> <int>    <dbl>    <int>    <int>  <dbl>\n1 164368 Hult Int… Camb…       4       2   574    0.571    60090       NA 0     \n2 164447 American… Spri…       4       2  1307    0.684    47742       NA 0     \n3 164465 Amherst … Amhe…       3       2  1855    0.128    71300       NA 0.110 \n4 164492 Anna Mar… Paxt…       4       2  1131    0.736    51109       NA 0     \n5 164535 Assabet … Marl…       1       1    50    0.452       NA    19703 0     \n6 164562 Assumpti… Worc…       4       2  2014    0.811    53303       NA 0.0162\n# ℹ 5 more variables: pctfloan <dbl>, admcon7 <int>, wdraw_orig_yr2_rt <dbl>,\n#   cdr3 <dbl>, year <dbl>\n\n\nstr() provides a great deal of information about the observations in the data frame, including the number of variables, the number of observations, the column names, their data types, and a list of observations.\n\nstr(scorecard)\n\ntibble [150 × 15] (S3: tbl_df/tbl/data.frame)\n $ unitid           : int [1:150] 164368 164447 164465 164492 164535 164562 164580 164599 164614 164632 ...\n $ instnm           : chr [1:150] \"Hult International Business School\" \"American International College\" \"Amherst College\" \"Anna Maria College\" ...\n $ city             : chr [1:150] \"Cambridge\" \"Springfield\" \"Amherst\" \"Paxton\" ...\n $ highdeg          : int [1:150] 4 4 3 4 1 4 4 1 3 4 ...\n $ control          : int [1:150] 2 2 2 2 1 2 2 3 2 2 ...\n $ ugds             : int [1:150] 574 1307 1855 1131 50 2014 2361 45 56 1921 ...\n $ adm_rate         : num [1:150] 0.571 0.684 0.128 0.736 0.452 ...\n $ costt4_a         : int [1:150] 60090 47742 71300 51109 NA 53303 68482 NA 26691 46315 ...\n $ costt4_p         : int [1:150] NA NA NA NA 19703 NA NA NA NA NA ...\n $ pcip27           : num [1:150] 0 0 0.11 0 0 ...\n $ pctfloan         : num [1:150] 0.0566 0.8333 0.1661 0.7464 0.4286 ...\n $ admcon7          : int [1:150] 3 5 1 3 3 3 1 NA 1 5 ...\n $ wdraw_orig_yr2_rt: num [1:150] NA 0.181 0.124 0.129 NA ...\n $ cdr3             : num [1:150] 0.043 0.073 0.014 0.073 0.065 0.044 0.019 0.06 0.024 0.048 ...\n $ year             : num [1:150] 2018 2018 2018 2018 2018 ...\n\n\nYou can also click on the name of your data frame in your Environment panel in RStudio, and it will open a new tab in RStudio that displays the data in a tabular format. Try clicking on scorecard in your Environment panel.\n\n\n\n\n\n\nTip\n\n\n\nThis is the same as calling View(scorecard) in your Console."
  },
  {
    "objectID": "labs/lab1.html#getting-to-know-this-dataset",
    "href": "labs/lab1.html#getting-to-know-this-dataset",
    "title": "Lab 1: Understanding Datasets",
    "section": "Getting to Know this Dataset",
    "text": "Getting to Know this Dataset\n\nObservations (Rows)\nIn starting our data analysis, we need to have a good sense of what each observation in our dataset refers to - or its observational unit. Think of it this way. If you were to count the number rows in your dataset, what would that number refer to? A unique key is a variable (or set of variables) that uniquely identifies an observation in the dataset. Think of a unique key as a unique way to identify a row and all of the values in it. There should never be more than one row in the dataset with the same unique key. A unique key tells us what each row in the dataset refers to.\n\nQuestion\n\nSee if you can identify a unique key for this dataset. Write some lines of code to determine whether the column you’ve identified can act as unique key for the data. Hint: You need to check whether the values in the column ever repeat.\n\n\n# Write code to calculate number of rows in scorecard\n\n# Write code to calculate the number of unique values in the column you've identified as a unique key \n\n# Do these numbers match?\n\n\n\n\n\n\n\n\n\n\n\n\nWhy not use name as a unique key?\n\n\n\nNote that NAME is typically not an appropriate variable to use as a unique key. Let me provide an example to demonstrate this. When I worked for BetaNYC, I was trying to build a map of vacant storefronts in NYC by mapping all commercially zoned properties in the city, and then filtering out those properties where a business was licensed or permitted. This way the map would only include properties where there wasn’t a business operating. One set of businesses I was filtering out was restaurants. The only dataset that the city had made publicly available for restaurant permits was broken. It was operating on an automated process to update whenever there was a change in the permit; however, whenever a permit was updated, rather than updating the appropriate fields in the existing dataset, it was creating a new row in the dataset that only included the permit holder (the restaurant name), the permit type, and the updated fields. Notably the unique permit ID was not being included in this new row. We pointed this issue out to city officials, but fixing something like this can be slow and time-consuming, so in the meantime, we looked into whether we could clean the data ourselves by aggregating the rows that referred to the same restaurant. However, without the permit ID it was impossible to uniquely identify the restaurants in the dataset. Sure, we had the restaurant name, but do you know how many Wendy’s there are in NYC?\n\n\nAnytime we count something in the world, we are not only engaging in a process of tabulation; we are also engaging in a process of defining. If I count the number of students in a class, I first have to define what counts as a student. If someone is auditing the class, do they count? If I, as the instructor, am learning from my students, do I count myself as a student? As I make decisions about how I’m going to define “student,” those decisions impact the numbers that I produce. When I change my definition of “student,” how I go about tabulating students also changes. Thus, as we prepare to count observations in a dataset, it is important to know how those observations are defined.\n\n\n\nQuestion\n\nAt this point, you’ve probably figured out that each row in this dataset is a higher education institution. …but there are many different ways that we can define higher education institutions, and that will impact what gets included and excluded in our data. Referencing the Technical Documentation, locate a definition for the unit of observation in this dataset. What institutions are included, and what institutions are excluded? Summarize a definition below.\n\n\n\n\nVariables (Columns)\nNote the column names for this dataframe, and the kinds of values that appear in those columns. Some of them (like city and year) might make sense to you immediately. Others (like pcip27 and highdeg) might be much more confusing. To figure out what we are looking out, we are going to need to refer to the dataset’s data dictionary.\nOpen the data dictionary you downloaded in an earlier step. It will open as an Excel file. Click on the tab labeled “Institution_Data_Dictionary”. There are thousands of variables in this dataset, falling into the broader categories of school, completion, admissions, cost, etc. Note how the file is organized, and specifically draw your attention to:\n\nColumn 1 (NAME OF DATA ELEMENT): This is a long description of the variable and gives you clues as to what is represented in it.\nColumn 6 (VARIABLE NAME): This is the column name for the variable. This is how you will reference the variable in R.\nColumn 7 (VALUE): These are the possible values for that variable. Note that for many categorical variables, the values are numbers. We are going to have to associate the numbers with their corresponding labels.\nColumn 8 (LABEL): These are the labels associated with the values recorded for the variable.\nColumn 11 (NOTES): This provides notes about the variable, including whether it is currently in use and what missing values indicate.\n\n\nQuestion\n\nFor each of the variable names in the scorecard data frame, look up the associated name in the data dictionary. You will need to search for the variable name in the sixth column of the data dictionary (I recommend using Ctrl-F to quickly locate the variable name in the spreadsheet.) Once you’ve found the variable name, reference column 1 to determine what this variable means, and reference columns 7 and 8 to see what possible values will appear in that column.\nIdentify one nominal variable, one ordinal variable, one discrete variable, and one continuous variable in scorecard and list their variable names below. Then uncomment the lines below and use the typeof() function to see how R determined their data types. Did any surprise you?\n\n\n#typeof(scorecard$_____)\n#typeof(scorecard$_____)\n#typeof(scorecard$_____)\n#typeof(scorecard$_____)\n\n\n\n\n\n\n\nValues (Cells)\nYou may have noticed that several categorical variables are coded as numbers in the imported dataset. For instance, look at the column control which designates the institution’s ownership. Running the code below, we see that the distinct values in that column are 1, 2, and 3.\n\nunique(scorecard$control)\n\n[1] 2 1 3\n\n\nWhen we reference that column in the data dictionary (row 27), we see that a 1 in that column designates that the institution is Public, a 2 that the institution is Private nonprofit, and a 3 that the institution is Private for-profit. While I can always look that up, sometimes it is helpful to have that information in our dataset. For instance, let’s say I create a bar plot that’s supposed to show how many higher education institutions have each type of ownership in MA (which you will learn how to do soon!). The plot can be confusing when control is a series of numbers.\n\nggplot(scorecard, aes(x = control)) +\n  geom_bar()\n\n\n\n\nWith this in mind, sometimes it can be helpful to recode the values in a column. Recoding data involves replacing the values in a vector according to criteria that we provide. Remember how all columns in a data frame are technically vectors? We can use the recode() function to recode all of the values in the control vector. We are going to store the recoded values in a new column in our dataset called control_text. Check out the code below to see how we do this. Reference the help pages for recode (i.e. ?recode) to help you interpret the code.\n\nscorecard$control_text <-\n  recode(\n    scorecard$control, \n    \"1\" = \"Public\", \n    \"2\" = \"Private nonprofit\", \n    \"3\" = \"Private for-profit\",\n    .default = NA_character_\n  )\n\nCheck out our barplot now!\n\nggplot(scorecard, aes(x = control_text)) +\n  geom_bar()\n\n\n\n\n\nQuestion\n\nWrite code below to recode the admcon7 variable and store the results in a new variable in scorecard called admcon7_text. You’ll need to look up the values in the data dictionary. If you’ve done this correctly, running this code should produce a barplot that displays multiple bars.\n\n\nscorecard$admcon7_text <-\n  recode(\n    scorecard$admcon7, \n    #Fill replacements here\n    .default = NA_character_\n  )\n\nggplot(scorecard, aes(x = admcon7_text)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\n\n\n\nMissing Values\nWhen we have missing values in a rectangular dataset, we have to provide a placeholder for the missing value in order for the dataset to remain rectangular. If we just skipped the value, then our dataset wouldn’t necessarily have rows of all equal lengths and columns of all equal lengths. In R, NA serves as that placeholder. Before we start analyzing data, it can be important to note how many NA values we have in a column so that we can determine if the data is representative.\nThe function is.na() checks whether a value is an NA value and returns TRUE if it is and FALSE if it isn’t. Providing a vector to is.na() will check this for every value in the vector and return a logical vector indicating TRUE/FALSE for every original value in the vector.\n\nis.na(scorecard$wdraw_orig_yr2_rt)\n\n  [1]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n [25] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE\n [37] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n [49] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE\n [73] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE\n [85] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE\n[109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE\n[121]  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE\n[133]  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE\n[145]  TRUE  TRUE FALSE  TRUE  TRUE FALSE\n\n\nWhen we sum() across a logical vector, R will calculate the number of TRUE values in the vector.\n\nsum(is.na(scorecard$wdraw_orig_yr2_rt))\n\n[1] 49\n\n# Note that this is the same as:\n\nscorecard$wdraw_orig_yr2_rt |> is.na() |> sum()\n\n[1] 49\n\n\n\nQuestion\n\nIn a code chunk below, calculate the number of missing values in both the costt4_a and the costt4_p columns. Reference the NOTES column in the data dictionary to determine why there are so many NA values in these columns. Add a comment to the code chunk below, explaining the reason for missing values in these columns.\n\n\n# Write code here\n\n# Add comment here\n\n\n\n\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nReferencing the College Scorecard data documentation, see if you can determine which students are included in calculations of earnings and debt. How might the data’s coverage bias the values that get reported? What might be the social consequences of these biases? Share your ideas on our sds-192-discussions Slack channel."
  },
  {
    "objectID": "labs/lab2.html",
    "href": "labs/lab2.html",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "",
    "text": "Designing effective data visualizations involves reviewing available data and then determining how best to map variables in the data onto a variety of visual cues. When we refer to visual cues, we are referring to those visual components of the plot that help us discern differences across data points. For instance, a plot might use different shapes or colors to represent different categories of data. A plot might also place points at different positions or bars at different heights to represent different numeric values. This week we will practice mapping variables in a dataset onto different plot aesthetics in order to tell different stories with the data. We will only be creating one type of plot today - a scatterplot. However, we are going to show how we can use different visual cues to plot a number of different variables onto a scatterplot.\n\n\n\nRead the ggplot cheatsheets\nMap variables onto plot aeshetics\nAdjust the attributes of a plot\nAdjust the scales of aeshetics on plots\nDeal with overplotting\nFacet plots into small multiples"
  },
  {
    "objectID": "labs/lab2.html#review-of-key-terms",
    "href": "labs/lab2.html#review-of-key-terms",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\nAesthetics\n\nVisual cues that we map variables in a dataset onto\n\nCartesian grid\n\nA 2-dimensional grid with an intersecting x and y axis\n\nSequential color\n\nA uni-directional ordering of shades\n\nQualitative color\n\nA discrete set of colors\n\nDivergent color\n\nA diverging ordering of color shades\n\nOverplotting\n\nInstances when visual representations of individual data points overlap on a plot making aspects of the plot illegible"
  },
  {
    "objectID": "labs/lab2.html#national-bridge-inventory-dataset",
    "href": "labs/lab2.html#national-bridge-inventory-dataset",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "National Bridge Inventory Dataset",
    "text": "National Bridge Inventory Dataset\nEvery year the U.S. Federal Highway Administration publishes a dataset listing every federally-regulated bridge and tunnel in the U.S., along with its location, design features, operational conditions, and inspection ratings. The data is used to review the safety of these transportation infrastructures for the traveling public. As you can imagine, for politicians promising to the improve the state of transportation infrastructure, this dataset is integral to determining where to allocate improvement funds:\n\n\nToday, we are going to look at a subset of 2022 NBI data for Massachusetts and for Hampshire County, MA, and we are going to focus solely on highway bridges (excluding pedestrian and railroad bridges). We’re going to look at what kinds of variables might contribute to poor bridge conditions, where there are poor bridge conditions, and which entities are responsible for maintaining them. The data documentation for this dataset is quite thick, so I will provide you with a data dictionary for today.\n\n\n\n\n\n\n\nVARIABLE NAME\nDESCRIPTION\n\n\n\n\nSTRUCTURE_NUMBER_008\nUnique ID for the bridge\n\n\nCOUNTY_CODE_003_L\nName of the county where the bridge is located\n\n\nROUTE_PREFIX_005B_L\nRoute signing prefix for the inventory route\n\n\nMAINTENANCE_021_L\nThe actual name(s) of the agency(s) responsible for the maintenance of the structure\n\n\nYEAR_BUILT_027\nThe year of construction of the structure\n\n\nADT_029\nThe average daily traffic volume for the inventory route based on most recent available data\n\n\nSTRUCTURE_KIND_043A_L\nThe kind of material and/or design of the structure\n\n\nSTRUCTURAL_EVAL_067\nA rating of the structural evaluation of the bridge based on inspections of its main structures, substructures, and/or its load ratings\n\n\nBRIDGE_IMP_COST_094\nEstimated costs for bridge improvements"
  },
  {
    "objectID": "labs/lab2.html#setting-up-your-environment",
    "href": "labs/lab2.html#setting-up-your-environment",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nInstall the RColorBrewer package by entering the following into your R Console: install.packages(\"RColorBrewer\")\nRun the code below to the import the bridge inventory for Massachusetts and for Hampshire County into R. Call me or one of the data assistants over if you get an error.\n\n\nlibrary(tidyverse)\nlibrary(RColorBrewer)\ncounties <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_counties.csv\")\nroute_prefixes <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_route_pre.csv\")\nmaintenance <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_maintenance.csv\")\nkinds <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_kind.csv\")\n\nnbi_ma <- read.delim(\"https://www.fhwa.dot.gov/bridge/nbi/2022/delimited/MA22.txt\", sep = \",\") |>\n  left_join(counties) |>\n  left_join(route_prefixes) |>\n  left_join(maintenance) |>\n  left_join(kinds) |>\n  filter(SERVICE_ON_042A == 1) |>\n  select(STRUCTURE_NUMBER_008, COUNTY_CODE_003_L, ROUTE_PREFIX_005B_L, MAINTENANCE_021_L, YEAR_BUILT_027, ADT_029, STRUCTURE_KIND_043A_L, STRUCTURAL_EVAL_067, BRIDGE_IMP_COST_094) |>\n  mutate(STRUCTURE_KIND_043A_L = \n           case_when(\n             STRUCTURE_KIND_043A_L == \"Concrete continuous\" ~ \"Concrete\",\n             STRUCTURE_KIND_043A_L == \"Steel continuous\" ~ \"Steel\",\n             STRUCTURE_KIND_043A_L == \"Prestressed concrete continuous\" ~ \"Prestressed concrete\",\n             TRUE ~ STRUCTURE_KIND_043A_L)) |>\n  mutate(BRIDGE_IMP_COST_094 = BRIDGE_IMP_COST_094 * 1000)\n\nnbi_hampshire <- nbi_ma |> filter(COUNTY_CODE_003_L == \"Hampshire\")\n\nrm(counties, kinds, maintenance, route_prefixes)"
  },
  {
    "objectID": "labs/lab2.html#visualization-aesthetics",
    "href": "labs/lab2.html#visualization-aesthetics",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Visualization Aesthetics",
    "text": "Visualization Aesthetics\nThe visual cues that we select to display on a plot largely depend on the kind of data that we have available. Last week we discussed the differences between categorical variables and numeric variables. Certain types of visual cues are more suited for representing categorical variables, while other types of visual cues are more suited for representing numeric variables. For instance, we wouldn’t use different shapes to represent different numeric values because there is no obvious ordering to a group of shapes (e.g. a triangle isn’t necessarily larger or greater than a circle; they’re just different). Because of this, shapes are much more appropriate for representing nominal categorical variables. Size is a much more effective visual cue for numeric variables because size can increase as the values in the data increase.\n\n\n\n\n\n\n\n\nCue\nEffective for what kinds of variables?\nExample where applied?\n\n\n\n\nShape\nCategorical\nPoints on scatterplots\n\n\nSize\nNumeric\nPoints on scatterplots\n\n\nArea\nNumeric\nBars in bar plots\n\n\nColor\nCategorical (Qualitative palette)\nNumeric (Sequential/Divergent)\nPoints on scatterplots;\nBars in bar plots;\nLines in a line plot\n\n\nPosition\nCategorical; Numeric\nPoints on scatterplots\n\n\nAngle\nNumeric\nSlices in pie chart\n\n\n\nYou’ll remember from lecture that we map variables onto these visual cues via the aesthetic function (aes()) in ggplot().\n\n\n\n\n\n\nTip\n\n\n\nYou may wish to reference this ggplot cheatsheet when completing this lab. Note how this cheatsheet is organized. There are headings for things like:\n\nBasics\nGeom functions\nScales\nCoordinate systems\n\nThe first tip to find what you’re looking for is to consider what heading the graphic element will likely fall under. Also note that, for many functions on this cheatsheet:\n\nan image is provided for how the plot will transform,\nthe function and its arguments are referenced in bold, and\na text description of what will happen when you apply the function is provided\n\nFinally note that most of these functions listed on this cheatsheet are appended to the ggplot() object with a + sign.\n\n\n\nPosition\nSince today we will be working with scatterplots, let’s start by talking about position. When we refer to position, we refer to the location of a point on a Cartesian plane (i.e. its position on an x-axis and its position on a y-axis). We can create scatterplots by mapping a variable in our dataset onto the x-axis and another variable onto the y-axis (aes(x = VARIABLE_NAME, y = VARIABLE_NAME)). Let’s take a look at what happens when we map the year a bridge was built onto the x aesthetic and the bridge’s structural evaluation onto the y aesthetic for all Hampshire County Massachusetts highway bridges.\n\nnbi_hampshire |>\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point() +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal()\n\n\n\n\nEach point on the plot corresponds to one observation (row) in the dataset. Since each row in this dataset is one Hampshire County, MA highway bridge, each point on this plot also represents one Hampshire County, MA highway bridge. The position of the point indicates to us the year that bridge was built and its structural evaluation. Zooming out to look at all of this data we can see that newer bridges tend to have higher structural evaluations.\n\nOverplotting\nYou’ll notice that parts of this plot can be challenging to read because some points overlap each other making it hard to distinguish one from the next. This is an example of overplotting, and there are a number of strategies we can take to addressing it. Most of these strategies involve revising attributes of the points on the plot (e.g. the size, transparency, and position of the points).\nNote that attributes are different than aesthetics. Recall that when we assign aesthetics in a plot, we are adjusting the visual cues on a plot according to the values in a variable. The visual cues will be different based on the values in that variable (e.g. size will be greater with greater values). On the other hand, when we adjust attributes, we are adjusting the visual cues on a plot in a fixed way (i.e. every point will be styled in the same way regardless of its value). Because of this attributes are assigned outside of the aes() (aesthetic) function. Let’s adjust the plot we created above by reducing the size of every point (size =), reducing the transparency of every point (alpha =), and adding \"jitter\" to the plot (position=). Recall that alpha is assigned between 0 and 1: 0 being transparent and 1 being opaque. Jitter means that we add a bit of random noise to the points on the plot in order to prevent overlapping points.\n\nQuestion\n\nIn the code below, adjust the size to 2 and the alpha to 0.5. Set the position to \"jitter\". Note how this changes the plot.\n\n\nnbi_hampshire |>\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(size = 1, alpha = 1, position = \"identity\") +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nColor\nColor can be used on plots to either distinguish between discrete values in a categorical variable or to represent the range of values in numeric variable. We use different kinds of color palettes for each of these scenarios. Palettes refer to a range of colors. We can have palettes with discrete colors (e.g. red, orange, and blue) or palettes with a gradient of colors (e.g. lightest red to darkest red).\n\n\n\n\n\n\n\n\nPalette\nEffective for what kinds of variables?\nExample\n\n\n\n\nQualitative\nCategorical\nRed, yellow, blue\n\n\nSequential\nNumeric\nLight blue to dark blue\n\n\nDivergent\nNumeric, extending in two directions (e.g. >1 and <1)\nBlue to purple to red\n\n\n\n\nQuestion\n\nIn the plot below, color the points on the plot with a qualitative palette by setting col= to the categorical variable ROUTE_PREFIX_005B_L. Remember that, in this case, color is an aesthetic, so this must be added inside of the aes() function. Add a label for the legend by setting col= in the labs() function to a phrase that describes the color variable.\n\n\nnbi_hampshire |>\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       col = \"ADD LEGED LABEL HERE\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nQuestion\n\nCopy the completed plot from the last exercise below but swap out the variable you mapped to the color aesthetic with a numeric variable from the dataset. Be sure to also adjust the legend label. What kind of palette gets created? What new information do you gain about Hampshire County bridges from the plot? Note your interpretations in your quarto file.\n\n\n\n\n\n\n\nShape\nA different way to differentiate data on a plot is to map the shape aesthetic onto the plot. In this case, rather than all observations in the dataset appearing as points on a plot, observations will appear as different shapes based on their associated values in a categorical variable.\n\nQuestion\n\nIn the plot below, assign the shape of the data on the plot by setting shape= to the categorical variable STRUCTURE_KIND_043A_L. Remember that, in this case, shape is an aesthetic, so this must be added inside of the aes() function. Add a label for the legend by setting shape= in the labs() function to a phrase that describes the shape variable.\n\n\nnbi_hampshire |>\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       shape = \"ADD LEGED LABEL HERE\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nSize\nWhile we might map a categorical variable onto the the shape aesthetic, we can alternatively map a numeric variable onto the size aesthetic. For instance, note what we learn when we map the variable for average daily traffic onto the size aesthetic below.\n\nnbi_hampshire |>\n  ggplot(aes(x = ROUTE_PREFIX_005B_L, \n             y = STRUCTURAL_EVAL_067, \n             size = ADT_029)) +\n  geom_point(alpha = 0.5, position = \"jitter\") +\n  coord_flip() +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Route Prefix\", \n       y = \"Structural Evaluation\",\n       size = \"Average Daily Traffic\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote how I moved my legend position to the bottom using + theme(legend.position = \"bottom\") in the code above.\n\n\n\nQuestion\n\nCopy one of your plots from above, and inside the aes() function, set size= to a numeric variable not already represented on the plot. Keep in mind that you’ll need to remove the size= attribute from geom_point() for the points to size correctly. Be sure to also adjust the legend labels. What new information do you gain about Hampshire County bridges from the plot?"
  },
  {
    "objectID": "labs/lab2.html#scales",
    "href": "labs/lab2.html#scales",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Scales",
    "text": "Scales\nWhen we map a variable onto an aesthetic, we are only indicating that the variable should be mapped. We are not indicating how the variable should be mapped. In order to indicate how we want a variable mapped to an aesthetic, we can adjust its scales. Scales are adjusted by tacking the following onto a ggplot() object: + scale_<aesthetic>_<type>(). For instance, let’s say that I wanted to adjust the scale of my x-axis to a log scale. I would attached + scale_x_log10() to my ggplot() object.\n\n\n\n\n\n\n\n\nScale\nDescription\nExample\n\n\n\n\nContinuous\nNumeric values are mapped along a continuum\n+ scale_y_continuous()\n\n\nDiscrete\nCategorical values are mapped into discrete buckets\n+ scale_color_discrete()\n\n\nBinned\nNumeric values are mapped into discrete bins\n+ scale_size_binned()\n\n\nLog\nNumeric values are mapped logarthmically\n+ scale_y_log10()\n\n\nDate-Time\nNumeric values are mapped along a timeline\n+ scale_x_datetime()\n\n\n\nLet’s talk about how we would adjust the scales for each of the aesthetics we’ve covered so far.\n\nPosition\nWe can adjust the scale of our x and y-axes by adding + scale_x_<type>() or + scale_y_<type>() to our plots. Note what happens when we attempt to create a scatterplot that shows the relationship between the year a bridge was built and the bridge improvement costs for all MA highway bridges.\n\nnbi_ma |>\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = BRIDGE_IMP_COST_094)) +\n  geom_point(size = 1, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Bridge Improvement Costs\") +\n  theme_minimal() + \n  scale_y_continuous(labels = scales::comma)\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nDue to huge disparities in costs for bridge improvements, this plot is difficult to interpret. Most bridge improvement costs are under $10,000,000, but with at least one bridge with costs just under a $1,000,000,000, the vast majority of the points on the plot appear at the very bottom of the y-axis scale and are largely indiscernible from one another. This is a case when it makes sense to apply a log scale to the y-axis.\n\nQuestion\n\nIn the plot below, change the y-axis scale from continuous to log10. You might reference the formula specified above or reference the ggplot cheatsheet for help. Note that your y-axis will appear in scientific notation. Convert it to comma notation by adding: labels = scales::comma as an argument in the scale function call.\n\n\nnbi_ma |>\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = BRIDGE_IMP_COST_094)) +\n  geom_point(size = 1, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Bridge Improvement Costs\") +\n  theme_minimal() + \n  scale_y_continuous(labels = scales::comma)\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nSometimes I might wish to group certain numeric values into bins on a plot. For instance, let’s say I just want to see how many bridges are structurally deficient in comparison to bridges that are operationally sound. Typically a bridge is considered structurally deficient when it scores 4 or lower. So I want to group the numeric values in STRUCTURAL_EVAL_067 into two bins: 0-4 and 4-9. To do this, I will set the scale to: + scale_y_binned() and add an argument to establish the bin breaks: breaks = c(4, 9) as well as an argument to label the bin breaks: labels = c(\"Structurally Deficent\", \"No Deficiencies\"). Check out what happens to the y-axis scale when I do this below.\n\nnbi_ma |>\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(size = 1, alpha = 0.2, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal() + \n  scale_y_binned(breaks = c(4, 9), labels = c(\"Structurally Deficent\", \"No Deficiencies\"))\n\n\n\n\n\n\n\nQuestion\n\nThe Interstate Highway Act of 1956 (signed under Dwight D. Eisenhower) established the U.S.’s interstate highway system and is considered the country’s largest public works project. Adjust the x-axis scale of the plot below to help tell this story. Specifically, bin years into before 1954 (when Eisenhower was elected) and 1954-2021. Set the label at 1954 to “Eisenhower Elected” and the label at 2021 to “Today.”\n\n\nnbi_ma |>\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = ROUTE_PREFIX_005B_L)) +\n  geom_point(size = 1, alpha = 0.2, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Route Prefix\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nColor\nWe’ve already talked about how both categorical and numeric variables can be mapped to the color aesthetic. However, sometimes, we want to be able to further customize which colors should appear on the plot and how they appear on the plot. The RColorBrewer package, which you installed earlier in the lab includes a number of palettes for coloring points on a plot. Check them out below:\n\nRColorBrewer::display.brewer.all() \n\n\n\n\nNote how the first set of palettes is sequential, the second set is categorical, and the third set is divergent. We can assign these palettes to our plots using one of two functions: + scale_color_brewer() for categorical data and + scale_color_distiller() for numeric data. Within this function, we can set the argument palette equal to one of the palettes specified in the image above.\n\nQuestion\n\nAdd `+ scale_color_brewer()` to the ggplot() object below and assign a categorical palette. Refer to the ggplot cheatsheet or the help pages for help with formatting the function call.\n\n\nnbi_hampshire |>\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067,\n             col = ROUTE_PREFIX_005B_L)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, Massachusetts, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       col = \"Route Prefix\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nQuestion\n\nAdd `+ scale_color_distiller()` to the ggplot() object below and assign a sequential palette. By default the colors will be ordered from darkest to lightest. Add an argument to the function call to reverse the direction of the colors. Refer to the ggplot cheatsheet or the help pages for help with formatting the function call.\n\n\nnbi_hampshire |>\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067,\n             col = ADT_029)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       col = \"Average Daily Traffic\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nSize and Shape\nThe following represents the values associated with ggplot() point shapes.\n\nWe can manually assign shapes using + scale_shape_manual(values = c(<shape_values>)).\n\nnbi_hampshire |>\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067, \n             shape = MAINTENANCE_021_L)) +\n  geom_point(alpha = 0.5, size = 2, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       shape = \"Maintainer\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",\n        legend.box=\"vertical\") +\n  guides(shape = guide_legend(nrow = 3, byrow = TRUE)) +\n  scale_shape_manual(values = c(15:17))\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that the guides() function above allows us to wrap the legend into three rows!\n\n\nAlso note how we bin point sizes just like we binned values on the x and y axis.\n\nnbi_hampshire |>\n  ggplot(aes(x = ROUTE_PREFIX_005B_L, \n             y = STRUCTURAL_EVAL_067, \n             size = ADT_029)) +\n  geom_point(alpha = 0.5, position = \"jitter\") +\n  coord_flip() +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Route Prefix\", \n       y = \"Structural Evaluation\",\n       size = \"Average Daily Traffic\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_size_binned(breaks = c(0, 500, 5000, 100000))"
  },
  {
    "objectID": "labs/lab2.html#faceting",
    "href": "labs/lab2.html#faceting",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Faceting",
    "text": "Faceting\nFaceting involves breaking out a single plot into multiple smaller plots based on the value in a variable. Faceting is very helpful when we have a categorical variable with many distinct values. If we tried to color by that variable, the colors would likely be indistinguishable from one another. For instance, check out what happens when we try to color by the COUNTY_CODE_003_L variable below.\n\nnbi_ma |>\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067, \n             col = COUNTY_CODE_003_L)) +\n  geom_point(alpha = 0.5, size = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       col = \"County\") +\n  theme_minimal()\n\n\n\n\nThere are so many counties that it’s extremely challenging to distinguish between colors on this plot. In this case, instead of using a color aesthetic, we facet the plot by adding + facet_wrap(vars(COUNTY_CODE_003_L)) to the ggplot() object. Check out what happens when we do that below.\n\nnbi_ma |>\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(alpha = 0.1, size = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal() +\n  facet_wrap(vars(COUNTY_CODE_003_L))\n\n\n\n\n\nQuestion\n\nFacet the plot below by STRUCTURE_KIND_043A_L to see the degree of structural deficiencies for different kinds of highway bridges of different ages in Massachusetts.\n\n\nnbi_ma |>\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(alpha = 0.1, size = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nIn response to concerns regarding domestic security, some stakeholders have questioned whether the Federal Highway Administration should be publicly disclosing information about the location and deficiencies of U.S. bridges and other forms of transportation infrastructure. What do you see as the value of making this data available to the public? Does its value outweigh national security concerns? Are these concerns legitimate? Share your ideas on our sds-192-discussions Slack channel."
  },
  {
    "objectID": "labs/lab3.html",
    "href": "labs/lab3.html",
    "title": "Lab 3: Plotting Frequencies and Distributions",
    "section": "",
    "text": "The goal of this lab is to provide you with practice in producing data visualizations that help to answer a research question.\n\n\n\nProduce and interpret univariate plots\nProduce and interpret multivariate plots\nSummarize and interpret variation and co-variation from observation of plots\nContextualize plots with descriptive labels and titles"
  },
  {
    "objectID": "labs/lab3.html#review-of-key-terms",
    "href": "labs/lab3.html#review-of-key-terms",
    "title": "Lab 3: Plotting Frequencies and Distributions",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\nMultivariate Plots\n\nPlots that summarize and visualize the distribution and relationship between multiple variables\n\nUnivariate Plots\n\nPlots that summarize and visualize the distribution of a single variable\n\nVariation\n\nThe degree to which categorical or numeric values vary across data"
  },
  {
    "objectID": "labs/lab3.html#spotify-dataset",
    "href": "labs/lab3.html#spotify-dataset",
    "title": "Lab 3: Plotting Frequencies and Distributions",
    "section": "Spotify Dataset",
    "text": "Spotify Dataset\nToday, we are prioritizing joy! Our research question will be: How joyful are popular Spotify playlists in my favorite music genre?\nThe music feature from Spotify’s data that serves as a measure of joy is called valence. This is the description from their API documentation for valence:\n\nA measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n\n(Pretty vague if you ask me, but today we’ll go with it.)"
  },
  {
    "objectID": "labs/lab3.html#setting-up-your-environment",
    "href": "labs/lab3.html#setting-up-your-environment",
    "title": "Lab 3: Plotting Frequencies and Distributions",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\n\n\n\n\n\nTip\n\n\n\nToday’s set-up is a little more complicated than usual, so be sure to take it slow and ask questions as they come up!\n\n\n\nInstall the Spotify R package by entering the following into your Console: install.packages(\"spotifyr\")\nLog-in to Spotify’s Developer Dashboard here. If you have a Spotify account, you can log-in with that account. Otherwise, you should create one.\nClick the ‘Create an App’ button to create an app named “SDS 192 Class Project”. You can indicate that this is a “Project for SDS 192 class”\n\n\n\nThis is where to click to create an app.\n\n\n\n\n\nThis is the form that you will fill out when you click ‘Create an app’\n\n\nClick Edit Settings. Under the heading Redirect URIs copy and paste this URL: http://localhost:1410/, and click Add. Scroll to the bottom of the window and click Save. This is going to allow us to authenticate our Spotify accounts through our local computers.\n\n\n\nClick here to Edit Settings\n\n\n\n\n\nThis is where you will paste the redirect link.\n\n\n\n\n\nBe sure to to click ‘Add’ after pasting the URL and to scroll down and click ‘Save’\n\n\nClick the Users and Access button. Scroll down to Add New User, and add your name and the email address associated with your Spotify account. Click Add.\n\n\n\nThis is where to click for Users and Access.\n\n\n\n\n\nThis is where to add a user.\n\n\n\n\n\nYou should see this if you’ve successfully added a user.\n\n\nClick “Show Client Secret”. Copy client id and secret into the code below, remove the comments in the code chunk, and then run the code chunk.\n\n\n\nThis is where you will see the client ID and secret. I’ve blocked it out here because these should be treated as passwords and not shared.\n\n\n\n\nlibrary(spotifyr)\n# DON'T FORGET TO REMOVE THE COMMENTS BELOW. THIS MEANS THAT YOU WILL NEED TO REMOVE THE '#' IN FRONT OF EACH OF THE FOLLOWING LINES.\n\n# id <- 'FILL CLIENT ID HERE'\n# secret <- 'FILL CLIENT SECRET HERE'\n# Sys.setenv(SPOTIFY_CLIENT_ID = id)\n# Sys.setenv(SPOTIFY_CLIENT_SECRET = secret)\n\n\nNavigate to https://open.spotify.com/. Below replace poiril with your Spotify username. This is the ID that appears in the upper right hand corner when you log into your Spotify account (not your developer account.)\n\n\n\nThis is where you will find the Spotify username.\n\n\nSearch Spotify for your favorite music genre and select three playlists from the search. Note that this code will only work for playlists, and playlists may be a ways down in the search results.\n\n\n\nMake sure to scroll down to Playlists when running your search!\n\n\nWhen you click on a playlist, notice the URL in the navigation bar of your web browser. It should look something like spotify.com/playlist/LONG_STRING_OF_CHARACTERS. Copy the long string of characters at the end of the URL, and paste it into the function below. Your characters should replace the example playlist I’ve added: 7ryj1GwAWYUY36VQd4uXoq.\n\n\n\nWhen you click on a playlist, you’ll see the long string of characters here.\n\n\nRepeat this for the other two playlists, replacing my other examples. Then run the code.\n\n\nlibrary(tidyverse)\nspotify_playlists <- get_playlist_audio_features(\n  username = \"poiril\",\n  playlist_uris = c(\"7ryj1GwAWYUY36VQd4uXoq\",\n                    \"3DYUw0nHB9o8tLZKQup4zp\",\n                    \"37i9dQZF1DXdOEFt9ZX0dh\"),\n  authorization = get_spotify_access_token()\n) |>\n  select(-c(track.artists, \n            track.available_markets, \n            track.album.artists, \n            track.album.available_markets, \n            track.album.images))"
  },
  {
    "objectID": "labs/lab3.html#data-analysis",
    "href": "labs/lab3.html#data-analysis",
    "title": "Lab 3: Plotting Frequencies and Distributions",
    "section": "Data Analysis",
    "text": "Data Analysis\nIn the exercises below, you will create a series of plots that will enable us to compare the joyfulness of the three playlists you’ve selected.\nI encourage you to take a look at the spotify_playlists data frame in your environment. You’ll note that each row in the dataset is a song/track from one of the the three playlists you selected (unique ID would be track.id), and columns provide information about that song (such as the track.name, the playlist it is a part of, its key, loudness, danceability, and acousticness). We can produce some pretty cool visualizations from this data. For instance check out how we might compare the relationship between the energy and acousticness of songs across the three selected playlists.\n\nspotify_playlists |>\n  ggplot(aes(x = acousticness, y = energy)) +\n  geom_point(alpha = 0.5, size = 0.5) +\n  coord_flip() +\n  facet_wrap(vars(playlist_name)) +\n  labs(title = \"Acousticness and Energy of Songs in Classic Rock Spotify Playlists, 2022\",\n       x = \"Acousticness\", \n       y = \"Energy\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou might use this plot as a starting template for the plots you are going to create below!\n\n\nWe’re going to start our analysis with univariate plotting. Specifically, we are going to produce data visualizations that count the number of observations in a dataset that fall into specific groupings. When grouping observations by a categorical variable, we will produce a bar plot. When grouping observations into intervals of a numeric variable, we will produce a histogram. Remember when labeling that these plots visualize frequency.\n\n\n\n\n\n\nWarning\n\n\n\nKeep in mind that all plots should have 5 contextual details represented in titles or labels:\n\nThe data’s unit of observation\nThe variables represented\nAny filters applied\nThe geographic scope of the data\nThe temporal scope of the data\n\n\n\n\nQuestion\n\nHow many songs are in each playlist? Create a plot to visualize this, and order the results by the number of songs. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n# Create plot here\n\n\n\n\n\n\nQuestion\n\nWhat is the distribution of valence across all of the songs (in intervals of 0.1 valence)? Create a histogram to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n# Create plot here\n\n\n\n\n\n\nLet’s move on to some multivariate plotting. Remember that we can add further variables to a plot via a number of different aesthetics (e.g. color: fill= or col=; size: size=; position: x= or y=, small multiples: + facet_wrap(vars(...)) ). Whenever we add further data to a plot, we should be on the lookout for overplotting.\n\n\n\nQuestion\n\nWhat is the distribution of valence across all of the songs (in intervals of 0.1 valence) in each playlist? Create a histogram to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n# Create plot here\n\n\n\n\n\n\nQuestion\n\nWhat are differences in the summary statistics (max, min, median, etc.) of the valence of songs in each playlist? Create grouped boxplot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n# Create plot here\n\n\n\n\n\n\nQuestion\n\nDo happier songs tend to be more danceable in each playlist? Create a plot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context. Also be sure to adjust your plot to address overplotting.\n\n\n# Create plot here\n\n\n\n\n\n\nQuestion\n\nDo songs composed with the minor or major mode_name tend to be happier in each playlist? Create a plot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n# Create plot here\n\n\n\n\n\n\nQuestion\n\nDo happier songs tend to have a higher tempo across all playlists? What role might the song’s mode play? Create a plot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context. Also be sure to adjust your plot to address overplotting.\n\n\n#Create plot here\n\n\n\n\n\n\nQuestion\n\nBased on all of your plots, summarize what you learned about the joyfulness of these three playlists below.\n\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nCheck out this article documenting ethical concerns regarding Spotify’s data collection practices. Should we be concerned about the assumptions that Spotify makes about us based on our music streaming habits? What about the way they curate music for us? What are some of the social consequences to this form of user surveillance? Share your ideas on our `sds-192-discussions` Slack channel."
  },
  {
    "objectID": "templates/lab5.html",
    "href": "templates/lab5.html",
    "title": "Lab 5: Exploratory Data Analysis",
    "section": "",
    "text": "sqf_2011 <- \n  sqf_2011 |> \n  select(pct, race, age, frisked, pistol, riflshot, asltweap, knifcuti, machgun, othrweap, sumissue, arstmade) |>\n  left_join(sqf_2011_race_cat, by = \"race\") |>\n  mutate(across(frisked:arstmade, \n         ~ case_when(. == \"Y\" ~ 1, . == \"N\" ~ 0)))\nrm(sqf_2011_race_cat)\n\n\nQuestion\n\nAdd two new columns. The first should indicate whether a weapon was found, and the second should indicate whether an arrest/summons was made.\n\n\nsqf_2011 <- \n  sqf_2011 |>\n  #Add a variable for weapon found\n  _____(wpnfound = case_when(pistol == 1 |\n                               riflshot == 1 | \n                               asltweap == 1 |\n                               knifcuti == 1 | \n                               machgun == 1 | \n                               othrweap == 1 ~ 1,\n                             TRUE ~ 0))\nsqf_2011 <- \n  sqf_2011 |>\n  #Add a variable for arrest made or summons issued\n  _____(arrestsumm = case_when(sumissue == 1 | \n                                arstmade == 1 ~ 1,\n                               TRUE ~ 0))\n\n\n\nQuestion\n\nSubset the dataset to the six variables listed in the data dictionary above.\n\n\nsqf_2011 <-\n  sqf_2011 |>\n  _____(pct, arrestsumm, _____, wpnfound, race_cat, _____)\n\n\n\nQuestion\n\nCalculate the number of stops in 2011. If you are not sure which function to use below, you may want to refer to the list of Summary functions in the the Data Wrangling cheatsheet. Remember that each row in the data frame is a stop.\n\n\ntotal_stops <-\n  sqf_2011 |>\n  summarize(Count = _____) |>\n  pull()\n\ntotal_stops\n\n\n\nQuestion\n\nHow many stops did not result in an arrest or summons in 2011? What percentage of stops did not result in an arrest or summons?\n\n\nsqf_2011 |>\n  #Subset to rows where suspect innocent\n  _____(arrestsumm _____ 0) |> \n  #Calculate number of observations\n  _____(total_innocent = n(), \n            percent_innocent = _____ / total_stops * 100)\n\n\n\nQuestion\n\nIn how many stops were the individuals aged 14-24? In what percentage of stops were the individuals aged 14-24?\n\n\nsqf_2011 |>\n  #Subset to rows where suspect age 14-24\n  _____(age _____ 14 & age _____ 24) |> \n  #Calculate number of observations and percentage of observations\n  _____(total_14_24 = _____, \n            percent_14_24 = n() / total_stops * 100)\n\n\n\nQuestion\n\nFix the code below to calculate the currect number of stops for individuals 14-24.\n\n\ntotal_stops_age_recorded <-\n  sqf_2011 |>\n  #Subset to rows where age is not 999\n  _____(age _____ 999) |> \n  summarize(Count = n()) |>\n  pull()\n\nsqf_2011 |>\n  filter(age >= 14 & age <= 24) |>\n  summarize(total_14_24 = n(), \n            percent_14_24 = n() / total_stops_age_recorded * 100)\n\nThis still doesn’t match the values we see on the website, but it does match the values we see in the NYCLU’s 2011 report on Stop, Question, and Frisk data. This is typically when I would reach out to a representative at the NYCLU to inquire about the discrepancy.\n\n\nQuestion\n\nHow many stops were there per race in 2011? What percentage of stops per race in 2011? Arrange by number of stops in descending order.\n\n\ntotal_stops_race_recorded <-\n  sqf_2011 |>\n  #Subset to rows where race_cat is not NA or \"OTHER\"\n  filter(_____(race_cat) & race_cat _____ \"OTHER\") |> \n  summarize(Count = n()) |>\n  pull()\n\nsqf_2011 |>\n  #Subset to rows where race_cat is not NA or \"OTHER\"\n  _____(_____(race_cat) & race_cat _____ \"OTHER\") |> \n  #Group by race\n  _____(race_cat) |> \n  #Calculate number of observations\n  _____(stops = n(), \n            percent_stops = n() / total_stops_race_recorded * 100) |>\n  #Sort by stops in descending order\n  _____(_____(stops)) \n\n\n\nQuestion\n\nIn how many stops were the individuals identified as Latinx (i.e. “WHITE-HISPANIC” or “BLACK-HISPANIC”)? In what percentage of stops were the individuals identified as Latinx?\n\n\nsqf_2011 |>\n  #Subset to rows where race_cat is \"WHITE-HISPANIC\" or \"BLACK-HISPANIC\"\n  _____(race_cat _____ c(\"WHITE-HISPANIC\", \"BLACK-HISPANIC\")) |> \n  #Calculate number of observations\n  _____(stops_Latinx = _____, \n            percent_Latinx = n() / total_stops_race_recorded * 100)\n\n\n\nQuestion\n\nWhat percentage of stops in 2011 resulted in a frisk per race? What percentage of stops in 2011 resulted in a weapon found per race? What percentage of stops in 2011 resulted in an arrest or summons per race? In your resulting data table, each row should be a race, and there should be columns for stops, percent_stops, percent_frisked, percent_wpnfound , and percent_arrestsumm.\n\n\n# Write code here. \n\n\n\nQuestion\n\nBelow, in 2-3 sentences, summarize what you learn from reviewing the summary statistics from the code above. What does this tell us about the constitutionality of 2011 stop and frisk activity in NYC?\n\n\n#If you finish early, I encourage you to attempt to plot some of this data below using `ggplot()`!"
  },
  {
    "objectID": "solutions/lab5.html",
    "href": "solutions/lab5.html",
    "title": "Lab 5: Exploratory Data Analysis",
    "section": "",
    "text": "Question\n\nAdd two new columns. The first should indicate whether a weapon was found, and the second should indicate whether an arrest/summons was made.\n\n\n\n\n\n\n\n\n\nQuestion\n\nSubset the dataset to the six variables listed in the data dictionary above.\n\n\n\n\n\n\n\n\n\nQuestion\n\nCalculate the number of stops in 2011. If you are not sure which function to use below, you may want to refer to the list of Summary functions in the the Data Wrangling cheatsheet. Remember that each row in the data frame is a stop.\n\n\n\n\n\n\n[1] 685724\n\n\n\n\nQuestion\n\nHow many stops did not result in an arrest or summons in 2011? What percentage of stops did not result in an arrest or summons?\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nQuestion\n\nIn how many stops were the individuals aged 14-24? In what percentage of stops were the individuals aged 14-24?\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nQuestion\n\nFix the code below to calculate the currect number of stops for individuals 14-24.\n\n\n\n\n\n\n\n\n  \n\n\n\nThis still doesn’t match the values we see on the website, but it does match the values we see in the NYCLU’s 2011 report on Stop, Question, and Frisk data. This is typically when I would reach out to a representative at the NYCLU to inquire about the discrepancy.\n\n\nQuestion\n\nHow many stops were there per race in 2011? What percentage of stops per race in 2011? Arrange by number of stops in descending order.\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nQuestion\n\nIn how many stops were the individuals identified as Latinx (i.e. “WHITE-HISPANIC” or “BLACK-HISPANIC”)? In what percentage of stops were the individuals identified as Latinx?\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nQuestion\n\nWhat percentage of stops in 2011 resulted in a frisk per race? What percentage of stops in 2011 resulted in a weapon found per race? What percentage of stops in 2011 resulted in an arrest or summons per race? In your resulting data table, each row should be a race, and there should be columns for stops, percent_stops, percent_frisked, percent_wpnfound , and percent_arrestsumm.\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nQuestion\n\nBelow, in 2-3 sentences, summarize what you learn from reviewing the summary statistics from the code above. What does this tell us about the constitutionality of 2011 stop and frisk activity in NYC?"
  },
  {
    "objectID": "templates/lab3.html",
    "href": "templates/lab3.html",
    "title": "Lab 3: Plotting Frequencies and Distributions",
    "section": "",
    "text": "library(tidyverse)\nspotify_playlists <- get_playlist_audio_features(\n  username = \"poiril\",\n  playlist_uris = c(\"7ryj1GwAWYUY36VQd4uXoq\",\n                    \"3DYUw0nHB9o8tLZKQup4zp\",\n                    \"37i9dQZF1DXdOEFt9ZX0dh\"),\n  authorization = get_spotify_access_token()\n) |>\n  select(-c(track.artists, \n            track.available_markets, \n            track.album.artists, \n            track.album.available_markets, \n            track.album.images))\n\n\nQuestion\n\nHow many songs are in each playlist? Create a plot to visualize this, and order the results by the number of songs. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n# Create plot here\n\n\n\nQuestion\n\nWhat is the distribution of valence across all of the songs (in intervals of 0.1 valence)? Create a histogram to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n# Create plot here\n\n\n\nQuestion\n\nWhat is the distribution of valence across all of the songs (in intervals of 0.1 valence) in each playlist? Create a histogram to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n# Create plot here\n\n\n\nQuestion\n\nWhat are differences in the summary statistics (max, min, median, etc.) of the valence of songs in each playlist? Create grouped boxplot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n# Create plot here\n\n\n\nQuestion\n\nDo happier songs tend to be more danceable in each playlist? Create a plot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context. Also be sure to adjust your plot to address overplotting.\n\n\n# Create plot here\n\n\n\nQuestion\n\nDo songs composed with the minor or major mode_name tend to be happier in each playlist? Create a plot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n# Create plot here\n\n\n\nQuestion\n\nDo happier songs tend to have a higher tempo across all playlists? What role might the song’s mode play? Create a plot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context. Also be sure to adjust your plot to address overplotting.\n\n\n#Create plot here\n\n\n\nQuestion\n\nBased on all of your plots, summarize what you learned about the joyfulness of these three playlists below."
  },
  {
    "objectID": "solutions/lab3.html",
    "href": "solutions/lab3.html",
    "title": "Lab 3: Plotting Frequencies and Distributions",
    "section": "",
    "text": "Question\n\nHow many songs are in each playlist? Create a plot to visualize this, and order the results by the number of songs. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\nWhat is the distribution of valence across all of the songs (in intervals of 0.1 valence)? Create a histogram to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\nWhat is the distribution of valence across all of the songs (in intervals of 0.1 valence) in each playlist? Create a histogram to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\nWhat are differences in the summary statistics (max, min, median, etc.) of the valence of songs in each playlist? Create grouped boxplot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\nDo happier songs tend to be more danceable in each playlist? Create a plot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context. Also be sure to adjust your plot to address overplotting.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\nDo songs composed with the minor or major mode_name tend to be happier in each playlist? Create a plot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\nDo happier songs tend to have a higher tempo across all playlists? What role might the song’s mode play? Create a plot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context. Also be sure to adjust your plot to address overplotting.\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\nBased on all of your plots, summarize what you learned about the joyfulness of these three playlists below."
  },
  {
    "objectID": "labs/lab1.html#submission",
    "href": "labs/lab1.html#submission",
    "title": "Lab 1: Understanding Datasets",
    "section": "Submission",
    "text": "Submission\n\nWhen you are done, save your .qmd file in RStudio.\nStage, commit, and push your file in the Git pane. Refer to step 12 in our Course Infrastructure setup for a reminder of how to do this.\nNavigate back to GitHub.com and click on the .qmd files to make sure you see your changes to the files there. If you don’t see the changes there, I won’t see them either!"
  },
  {
    "objectID": "labs/problem-solving.html#searching-the-web",
    "href": "labs/problem-solving.html#searching-the-web",
    "title": "Problem Solving",
    "section": "Searching the Web",
    "text": "Searching the Web\nI encourage you to search the web when you get errors in your code. Others have likely experienced that error before and gotten help from communities of data analysts and programmers. However, you should never copy and paste code directly from Stack Overflow. This violates the course policies on Academic Honesty. Instead you should use these resources to take notes and learn how to improve and revise code. Any time you reference Stack Overflow or any other Web resource to help you figure out an answer to a problem, you should cite that resource in your code. Here is how you would cite that post in APA format:\n\nUsername. (Year, Month Date). Title of page (Question/Topic). Stack Overflow. URL\n\n\nQuestion\n\nAdd a comment to the code chunk above, citing this Stack Overflow post. Be sure to cite the post properly. You can read through the post to double check your answers."
  },
  {
    "objectID": "labs/problem-solving.html#submission",
    "href": "labs/problem-solving.html#submission",
    "title": "Problem Solving",
    "section": "Submission",
    "text": "Submission\n\nWhen you are done, save your .qmd file in RStudio.\nStage, commit, and push your file in the Git pane. Refer to step 12 in our Course Infrastructure setup for a reminder of how to do this.\nNavigate back to GitHub.com and click on the .qmd files to make sure you see your changes to the files there. If you don’t see the changes there, I won’t see them either!"
  },
  {
    "objectID": "slides/Day3-RFundamentals.html#for-today",
    "href": "slides/Day3-RFundamentals.html#for-today",
    "title": "R Fundamentals",
    "section": "For Today",
    "text": "For Today\n\nNouns: Data objects in R\nVerbs: R Functions\nConjunctions: R Operators\nMissing Values and R Functions\nExercise"
  },
  {
    "objectID": "slides/Day3-RFundamentals.html#values-vs.-vectors-vs.-data-frames",
    "href": "slides/Day3-RFundamentals.html#values-vs.-vectors-vs.-data-frames",
    "title": "R Fundamentals",
    "section": "Values vs. Vectors vs. Data Frames",
    "text": "Values vs. Vectors vs. Data Frames\n\nValuesVectorsData Frame\n\n\n\n\na single data point\nR understands values to be of a certain type:\n\nnumeric: 3.29\ninteger: 3\ncharacter: “SDS 192”\nlogical: TRUE/FALSE\ndate-time: 3/12/92 01:23:01\n\n\n\n\n\n\n\na 1-dimensional data object, listing a series of values\nall objects in a vector share the same type\nvector defined by listing entries (separated by commas) in the function c() (shorthand for combine)\n\n\nvector_example <- c(1, 5, 6, 7)\nvector_example\n\n[1] 1 5 6 7\n\n\n\n\n\n\n\na two-dimensional (rectangular) data object\nEvery column in a data frame is a vector\nColumn names act as a variable name for that vector (access via the $ accessor)\nI (Lindsay) use df to denote a data frame.\n\n\n\n\n\ndf\n\n  col1  col2 col3\n1    1  TRUE    a\n2    5 FALSE    b\n3    6  TRUE    c\n4    7  TRUE    d\n\ndf$col1\n\n[1] 1 5 6 7"
  },
  {
    "objectID": "slides/Day3-RFundamentals.html#learning-check",
    "href": "slides/Day3-RFundamentals.html#learning-check",
    "title": "R Fundamentals",
    "section": "Learning check",
    "text": "Learning check\nWhat kind of object is this in R? What is its type?\n\ntemps <- c(47.3, 55.6, 48.3)"
  },
  {
    "objectID": "slides/Day3-RFundamentals.html#assigning-objects-to-variable-names",
    "href": "slides/Day3-RFundamentals.html#assigning-objects-to-variable-names",
    "title": "R Fundamentals",
    "section": "Assigning Objects to Variable Names",
    "text": "Assigning Objects to Variable Names\n\n<- symbol assigns a value to a variable\nVariable names should be descriptive! Poor or confusing variables names include:\n\na anddata1: Be descriptive!\nstudent.test.scores: Avoid periods!\nstudent test scores: Use separator characters!\n3rd_test: Variables can’t start with numbers!\n\nThis course: snake case (lower case with words separated by underscores)"
  },
  {
    "objectID": "slides/Day3-RFundamentals.html#where-can-i-find-these-data-objects-in-r",
    "href": "slides/Day3-RFundamentals.html#where-can-i-find-these-data-objects-in-r",
    "title": "R Fundamentals",
    "section": "Where can I find these data objects in R?",
    "text": "Where can I find these data objects in R?\n\n\n\nObjects in R will be listed in the Environment tab in the upper right hand corner of RStudio.\nRemoving unnecessary objects from the environment can free up space!\n\n\nrm(vector_example)"
  },
  {
    "objectID": "slides/Day3-RFundamentals.html#functions",
    "href": "slides/Day3-RFundamentals.html#functions",
    "title": "R Fundamentals",
    "section": "Functions",
    "text": "Functions\n\nThink of functions like imperative sentences (e.g. “go”, “stay”, or “sleep”)\nIndicate that you want it to take an action\nTypically immediately followed by open and closed parentheses\nWhat were some functions referenced this week’s reading?"
  },
  {
    "objectID": "slides/Day3-RFundamentals.html#arguments",
    "href": "slides/Day3-RFundamentals.html#arguments",
    "title": "R Fundamentals",
    "section": "Arguments",
    "text": "Arguments\n\nImagine I requested someone to “close” or “bring”\n\nThey’re next questions might be “close what” or “bring what”, and I might say back “close the door” or “bring dessert”\n\nSpecify the subject of the function, along with additional information needed to run the function\nListed inside of the parentheses\nSome arguments are required. Others are optional."
  },
  {
    "objectID": "slides/Day3-RFundamentals.html#learning-check-1",
    "href": "slides/Day3-RFundamentals.html#learning-check-1",
    "title": "R Fundamentals",
    "section": "Learning check",
    "text": "Learning check\nWhat would happen if I were to do the following in R?\n\nval <- 34\nval <- val + 1\n\n\nThis is called overwriting a variable."
  },
  {
    "objectID": "slides/Day3-RFundamentals.html#helpful-value-operations",
    "href": "slides/Day3-RFundamentals.html#helpful-value-operations",
    "title": "R Fundamentals",
    "section": "Helpful Value Operations",
    "text": "Helpful Value Operations\n\nNumeric ValuesCharacter Values\n\n\n\nR can work just like a calculator!\n\na <- 2\nb <- 3\n\nsum(a,b)\n\n[1] 5\n\n\nWhy does this produce an error?\n\nc <- \"3\"\nsum(c, c)\n\nError in sum(c, c): invalid 'type' (character) of argument\n\n\n\n\n\n\nR can concatenate strings!\n\nword1 <- \"Harry\"\nword2 <- \"Sally\"\npaste(\"When\", word1, \"Met\", word2, sep = \" \")\n\n[1] \"When Harry Met Sally\""
  },
  {
    "objectID": "slides/Day3-RFundamentals.html#helpful-vector-functions",
    "href": "slides/Day3-RFundamentals.html#helpful-vector-functions",
    "title": "R Fundamentals",
    "section": "Helpful Vector Functions",
    "text": "Helpful Vector Functions\n\nAll VectorsNumeric VectorsCategorical Vectors\n\n\n\n\nclass() returns the class of the values in a vector\nlength() returns the number of values in a vector\nis.na() for each value, returns whether the value is an NA value\n\n\n\n\n\n\nsum() returns the sum of the values in a vector\nmax() returns the maximum value in a vector\nrank() returns the ranking of a value in a vector\n\n\n\n\n\n\nunique() returns the unique values of a vector"
  },
  {
    "objectID": "slides/Day3-RFundamentals.html#helpful-data-frame-functions",
    "href": "slides/Day3-RFundamentals.html#helpful-data-frame-functions",
    "title": "R Fundamentals",
    "section": "Helpful Data Frame Functions",
    "text": "Helpful Data Frame Functions\n\nView(): Opens a tab to view the data frame as a table\nhead(): returns first six rows of dataset\nnames(): returns the dataset’s column names\nnrow(): returns the number of rows in the dataset\nncol(): returns the number of columns in the dataset"
  },
  {
    "objectID": "slides/Day3-RFundamentals.html#do-i-really-have-to-memorize-all-of-these-functions",
    "href": "slides/Day3-RFundamentals.html#do-i-really-have-to-memorize-all-of-these-functions",
    "title": "R Fundamentals",
    "section": "Do I really have to memorize all of these functions?!",
    "text": "Do I really have to memorize all of these functions?!\n\nNo. There are cheatsheets! See this cheatsheet for Base R."
  },
  {
    "objectID": "slides/Day3-RFundamentals.html#what-is-a-function",
    "href": "slides/Day3-RFundamentals.html#what-is-a-function",
    "title": "R Fundamentals",
    "section": "What is a function?",
    "text": "What is a function?\n\nThink of functions like imperative sentences (e.g. “go”, “stay”, or “sleep”)\nIndicate that you want it to take an action\nTypically immediately followed by open and closed parentheses\nWhat were some functions referenced this week’s reading?"
  },
  {
    "objectID": "slides/Day3-RFundamentals.html#finding-help",
    "href": "slides/Day3-RFundamentals.html#finding-help",
    "title": "R Fundamentals",
    "section": "Finding Help",
    "text": "Finding Help\n\nTyping ?FUNCTION_NAME in to the Console loads info about that function\n\n?round()\n\nWhat functions are required?\nWhat functions are optional?"
  },
  {
    "objectID": "slides/Day3-RFundamentals.html#learning-check-2",
    "href": "slides/Day3-RFundamentals.html#learning-check-2",
    "title": "R Fundamentals",
    "section": "Learning check",
    "text": "Learning check\nConvert the following variable name into something descriptive in snake case\na <- round(pi, digits = 2)\nRun the code in your Console. How can we find this variable in RStudio once we run this code?"
  },
  {
    "objectID": "slides/Day3-RFundamentals.html#operators-in-r",
    "href": "slides/Day3-RFundamentals.html#operators-in-r",
    "title": "R Fundamentals",
    "section": "Operators in R",
    "text": "Operators in R\n\nSymbols that communicate what operations to perform in R\nIncludes calculator symbols: +, -, *, /, ^\nIncludes relational symbols: <, <=, <, <=, ==, !=\nIncludes logical symbols: & (AND), | (OR), ! (NOT)"
  },
  {
    "objectID": "slides/Day3-RFundamentals.html#missing-values",
    "href": "slides/Day3-RFundamentals.html#missing-values",
    "title": "R Fundamentals",
    "section": "Missing Values",
    "text": "Missing Values\n\nRemember that missing values still have a position in rectangular datasets\nMissing values get recorded as NA in R\n…but sometimes analysts put words or numbers in their datasets to indicate missingness:\n\n“NONE”\n-999\n“” <- this is the most challenging to uncover!\n\n…but what happens when we try to perform functions on vectors that contain missing values?"
  },
  {
    "objectID": "slides/Day3-RFundamentals.html#missing-values-in-math-functions",
    "href": "slides/Day3-RFundamentals.html#missing-values-in-math-functions",
    "title": "R Fundamentals",
    "section": "Missing Values in Math Functions",
    "text": "Missing Values in Math Functions\nWe can use na.rm = TRUE to ignore NA values in math functions.\n\nvals <- c(1, 2, NA, 4, NA, 6)\nsum(vals)\n\n[1] NA\n\nsum(vals, na.rm = TRUE)\n\n[1] 13"
  },
  {
    "objectID": "slides/Day3-RFundamentals.html#things-to-know-right-up-front",
    "href": "slides/Day3-RFundamentals.html#things-to-know-right-up-front",
    "title": "R Fundamentals",
    "section": "Things to Know Right Up Front",
    "text": "Things to Know Right Up Front\n\nR is case-sensitive. df is different than DF"
  },
  {
    "objectID": "slides/Day3-RFundamentals.html#learning-check-3",
    "href": "slides/Day3-RFundamentals.html#learning-check-3",
    "title": "R Fundamentals",
    "section": "Learning Check",
    "text": "Learning Check\nHow would I find the sum of the third column in this data frame, which I have named df?\n\n\n  col1 col2 col3\n1    1    2    3\n2    5    4    6\n3    7    6    9"
  },
  {
    "objectID": "slides/Day3-RFundamentals.html",
    "href": "slides/Day3-RFundamentals.html",
    "title": "R Fundamentals",
    "section": "",
    "text": "Nouns: Data objects in R\nVerbs: R Functions\nConjunctions: R Operators\nMissing Values and R Functions\nExercise"
  },
  {
    "objectID": "slides/Day3-RFundamentals.html#operators-in-r-1",
    "href": "slides/Day3-RFundamentals.html#operators-in-r-1",
    "title": "R Fundamentals",
    "section": "Operators in R",
    "text": "Operators in R\n\nSymbols that communicate what operations to perform in R\nIncludes calculator symbols: +, -, *, /, ^\nIncludes relational symbols: <, <=, <, <=, ==, !=\nIncludes logical symbols: & (AND), | (OR), ! (NOT)"
  },
  {
    "objectID": "slides/Day3-RFundamentals.html#pipe-operator-in-r",
    "href": "slides/Day3-RFundamentals.html#pipe-operator-in-r",
    "title": "R Fundamentals",
    "section": "Pipe Operator in R",
    "text": "Pipe Operator in R\n\nSymbol is |> (old version is %>%)\n\n\nWithout Pipe\n\nFunctions are nested as arguments in R\nlength(unique(df$col1))\nPerform the innermost function to the outermost\n\n\nWith Pipe\n\nFunctions are sequenced in R\ndf$col1 |> unique() |> length()\nTake this data object, and then perform this function, and then perform this function"
  },
  {
    "objectID": "slides/Day4-Visualizations.html#for-today",
    "href": "slides/Day4-Visualizations.html#for-today",
    "title": "Visualizing Data",
    "section": "For Today",
    "text": "For Today\n\nLab debriefing\nWhat is a data visualization?\nTaxonomy of Data Visualizations\nVisualization Conventions and Critiques\nWork on Problem Solving Lab in Class"
  },
  {
    "objectID": "slides/Day4-Visualizations.html#elements-of-data-graphics",
    "href": "slides/Day4-Visualizations.html#elements-of-data-graphics",
    "title": "Visualizing Data",
    "section": "Elements of data graphics",
    "text": "Elements of data graphics\n\nvisual cues/aesthetics\nscale\ncontext\n\n\nFramework drawn from: Yau, Nathan. 2013. Data Points: Visualization That Means Something. 1st edition. Indianapolis, IN: Wiley."
  },
  {
    "objectID": "slides/Day4-Visualizations.html#what-is-data-visualization",
    "href": "slides/Day4-Visualizations.html#what-is-data-visualization",
    "title": "Visualizing Data",
    "section": "What is data visualization?",
    "text": "What is data visualization?\n\nthe translation of information into a graphical format\nhelps analysts summarize and identify patterns across large datasets\nalways involves critical judgment calls on the part of the designer"
  },
  {
    "objectID": "slides/Day4-Visualizations.html#visual-cues",
    "href": "slides/Day4-Visualizations.html#visual-cues",
    "title": "Visualizing Data",
    "section": "Visual Cues",
    "text": "Visual Cues\n\nWhere is the data positioned on the plot?\nWhat is the length of shapes on the plot?\nHow large is the angle between vectors?\nWhat shapes/symbols appear on the plot?\nHow much area do shapes take up on a plot?\nHow intense is the color presented on the plot?"
  },
  {
    "objectID": "slides/Day4-Visualizations.html#what-variables-mapped-onto-what-visual-cues",
    "href": "slides/Day4-Visualizations.html#what-variables-mapped-onto-what-visual-cues",
    "title": "Visualizing Data",
    "section": "What variables mapped onto what visual cues?",
    "text": "What variables mapped onto what visual cues?"
  },
  {
    "objectID": "slides/Day4-Visualizations.html#what-variables-mapped-onto-what-visual-cues-1",
    "href": "slides/Day4-Visualizations.html#what-variables-mapped-onto-what-visual-cues-1",
    "title": "Visualizing Data",
    "section": "What variables mapped onto what visual cues?",
    "text": "What variables mapped onto what visual cues?\n\n** This is the last time you will see me use a pie chart in this class!"
  },
  {
    "objectID": "slides/Day4-Visualizations.html#what-variables-mapped-onto-what-visual-cues-2",
    "href": "slides/Day4-Visualizations.html#what-variables-mapped-onto-what-visual-cues-2",
    "title": "Visualizing Data",
    "section": "What variables mapped onto what visual cues?",
    "text": "What variables mapped onto what visual cues?"
  },
  {
    "objectID": "slides/Day4-Visualizations.html#what-variables-mapped-onto-what-visual-cues-3",
    "href": "slides/Day4-Visualizations.html#what-variables-mapped-onto-what-visual-cues-3",
    "title": "Visualizing Data",
    "section": "What variables mapped onto what visual cues?",
    "text": "What variables mapped onto what visual cues?"
  },
  {
    "objectID": "slides/Day4-Visualizations.html#what-variables-mapped-onto-what-visual-cues-4",
    "href": "slides/Day4-Visualizations.html#what-variables-mapped-onto-what-visual-cues-4",
    "title": "Visualizing Data",
    "section": "What variables mapped onto what visual cues?",
    "text": "What variables mapped onto what visual cues?"
  },
  {
    "objectID": "slides/Day4-Visualizations.html#what-variables-mapped-onto-what-visual-cues-5",
    "href": "slides/Day4-Visualizations.html#what-variables-mapped-onto-what-visual-cues-5",
    "title": "Visualizing Data",
    "section": "What variables mapped onto what visual cues?",
    "text": "What variables mapped onto what visual cues?"
  },
  {
    "objectID": "slides/Day4-Visualizations.html#scale",
    "href": "slides/Day4-Visualizations.html#scale",
    "title": "Visualizing Data",
    "section": "Scale",
    "text": "Scale\n\nLinear: Numeric values are evenly spaced on axis.\nLogarithmic: Numeric interval are spaced by a factor of the base of the logarithm.\nCategorical: Categorical values are discretely placed on axis.\nOrdinal: Categorical values are ordered on axis.\nPercent: Percentages of a whole are evenly spaced on axis.\nTime: Date/time values are placed on axis in years, months, days, hours, etc."
  },
  {
    "objectID": "slides/Day4-Visualizations.html#examples",
    "href": "slides/Day4-Visualizations.html#examples",
    "title": "Visualizing Data",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "slides/Day4-Visualizations.html#context",
    "href": "slides/Day4-Visualizations.html#context",
    "title": "Visualizing Data",
    "section": "Context",
    "text": "Context\nIn every plot you submit for this class, I will be looking for five pieces of context.\n\nThe data’s unit of observation\nVariables represented on the plot\nFilters applied to the data\nGeographic context of the data\nTemporal (date/time range) context of the date"
  },
  {
    "objectID": "slides/Day4-Visualizations.html#data-visualization-conventions",
    "href": "slides/Day4-Visualizations.html#data-visualization-conventions",
    "title": "Visualizing Data",
    "section": "Data Visualization Conventions",
    "text": "Data Visualization Conventions\n\nEdward Tufte, American statistician sometimes considered “father of data visualization”\nIntroduced the concept of “graphical integrity”\nHow do we present data as honestly as possible?"
  },
  {
    "objectID": "slides/Day4-Visualizations.html#lie-factor",
    "href": "slides/Day4-Visualizations.html#lie-factor",
    "title": "Visualizing Data",
    "section": "Lie Factor",
    "text": "Lie Factor\n\nLie Factor = (size of effect in graphic)/(size of effect in data)\nLie factor is greater when variations on a graph fail to match variations in data\n\n\nTufte, Visual Display of Quantitative Information\n5.3-0.6 / 0.6 * 100= 783 (graph)\n27.5-18 /18 *100 = 53 (data)\n783/53 = 14.8"
  },
  {
    "objectID": "slides/Day4-Visualizations.html#inconsistent-scales",
    "href": "slides/Day4-Visualizations.html#inconsistent-scales",
    "title": "Visualizing Data",
    "section": "Inconsistent Scales",
    "text": "Inconsistent Scales\n\nExample from callingbullshit.org"
  },
  {
    "objectID": "slides/Day4-Visualizations.html#presenting-data-out-of-context",
    "href": "slides/Day4-Visualizations.html#presenting-data-out-of-context",
    "title": "Visualizing Data",
    "section": "Presenting Data out of Context",
    "text": "Presenting Data out of Context\n\nExample from mediamatters.org"
  },
  {
    "objectID": "slides/Day4-Visualizations.html#high-data-to-ink-ratio",
    "href": "slides/Day4-Visualizations.html#high-data-to-ink-ratio",
    "title": "`Visualizing Data",
    "section": "High Data-to-Ink Ratio",
    "text": "High Data-to-Ink Ratio\n\nEnsure that the ink used on the data match the amount of data presented\nData-to-ink ratio = (ink used to represent data)/(ink used to print graphic)\nShould be as close as possible to 1\nAnother way to think about it: How much of this graph could I erase without losing data?"
  },
  {
    "objectID": "slides/Day4-Visualizations.html#high-data-to-ink-ratio-1",
    "href": "slides/Day4-Visualizations.html#high-data-to-ink-ratio-1",
    "title": "`Visualizing Data",
    "section": "High Data-to-Ink Ratio",
    "text": "High Data-to-Ink Ratio"
  },
  {
    "objectID": "slides/Day4-Visualizations.html#deviating-from-norms",
    "href": "slides/Day4-Visualizations.html#deviating-from-norms",
    "title": "Visualizing Data",
    "section": "Deviating from Norms",
    "text": "Deviating from Norms\n\nExample from callingbullshit.org"
  },
  {
    "objectID": "slides/Day4-Visualizations.html#when-can-i-break-convention",
    "href": "slides/Day4-Visualizations.html#when-can-i-break-convention",
    "title": "Visualizing Data",
    "section": "When can I break convention??",
    "text": "When can I break convention??"
  },
  {
    "objectID": "slides/Day4-Visualizations.html#context-1",
    "href": "slides/Day4-Visualizations.html#context-1",
    "title": "Visualizing Data",
    "section": "Context",
    "text": "Context"
  },
  {
    "objectID": "slides/Day4-Visualizations.html#disproportionate-data-to-ink-ratio",
    "href": "slides/Day4-Visualizations.html#disproportionate-data-to-ink-ratio",
    "title": "Visualizing Data",
    "section": "Disproportionate Data-to-Ink Ratio",
    "text": "Disproportionate Data-to-Ink Ratio\n\nEnsure that the ink used on the data match the amount of data presented\nData-to-ink ratio = (ink used to represent data)/(ink used to print graphic)\nShould be as close as possible to 1\nAnother way to think about it: How much of this graph could I erase without losing data?"
  },
  {
    "objectID": "slides/Day4-Visualizations.html#disproportionate-data-to-ink-ratio-1",
    "href": "slides/Day4-Visualizations.html#disproportionate-data-to-ink-ratio-1",
    "title": "Visualizing Data",
    "section": "Disproportionate Data-to-Ink Ratio",
    "text": "Disproportionate Data-to-Ink Ratio"
  },
  {
    "objectID": "exercises/visualizations.html",
    "href": "exercises/visualizations.html",
    "title": "Day 4: Visualizations",
    "section": "",
    "text": "Navigate to the New York Times Grapics. Select a data visualization. …but no maps for today! Take no more than 2 minutes to for this! Drop a link to the data visualization in sds-192-discussions.\nIdentify the visual cues on the visualization, along with the scale of the visualization. Remember visual cues are used to represent variations in the data, while scale is the scale at which those visual cues are differentiated.\n\n\nVisual cues include: position, length, angle, shape, size, color\nScales include: numeric, logarithmic, categorical, ordinal, percentage date-time\n\n\nIdentify the context provided on the visualization. Are all five components of context represented?\n\n\nContext components include: units of observation, variables, filter, geographic context, temporal context\n\n\nAssess the graphical integrity of the visualization. Does the visualization violate any of Tufte’s principles? If so, does this violation serve a purpose?"
  },
  {
    "objectID": "slides/Day4-Visualizations.html",
    "href": "slides/Day4-Visualizations.html",
    "title": "Visualizing Data",
    "section": "",
    "text": "Lab debriefing\nWhat is a data visualization?\nTaxonomy of Data Visualizations\nVisualization Conventions and Critiques\nWork on Problem Solving Lab in Class"
  },
  {
    "objectID": "slides/Day5-ggplot.html",
    "href": "slides/Day5-ggplot.html",
    "title": "ggplot()",
    "section": "",
    "text": "Visualizations Exercise\nIntroduction to GGplot\nClass Activity"
  },
  {
    "objectID": "slides/Day5-ggplot.html#ggplot",
    "href": "slides/Day5-ggplot.html#ggplot",
    "title": "ggplot()",
    "section": "ggplot",
    "text": "ggplot\n\nMost plots we create in this course will rely on package called ggplot2\nggplot2 is included in the Tidyverse, which you installed in SDS 100\nLoad ggplot2 in your environment.\n\n\nlibrary(ggplot2)"
  },
  {
    "objectID": "slides/Day5-ggplot.html#elements-of-data-graphics",
    "href": "slides/Day5-ggplot.html#elements-of-data-graphics",
    "title": "ggplot()",
    "section": "Elements of data graphics",
    "text": "Elements of data graphics\n\nvisual cues/aesthetics\nscale\ncontext\n\n\nFramework drawn from: Yau, Nathan. 2013. Data Points: Visualization That Means Something. 1st edition. Indianapolis, IN: Wiley."
  },
  {
    "objectID": "slides/Day5-ggplot.html#visual-cues",
    "href": "slides/Day5-ggplot.html#visual-cues",
    "title": "ggplot()",
    "section": "Visual Cues",
    "text": "Visual Cues\n\nWhere is the data positioned on the plot?\nWhat is the length of shapes on the plot?\nHow large is the angle between vectors?\nWhat shapes/symbols appear on the plot?\nHow much area do shapes take up on a plot?\nHow intense is the color presented on the plot?"
  },
  {
    "objectID": "slides/Day5-ggplot.html#what-variables-mapped-onto-what-visual-cues",
    "href": "slides/Day5-ggplot.html#what-variables-mapped-onto-what-visual-cues",
    "title": "ggplot()",
    "section": "What variables mapped onto what visual cues?",
    "text": "What variables mapped onto what visual cues?\n\n\nWarning: package 'tidyverse' was built under R version 4.1.2\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.1 \n\n\nWarning: package 'ggplot2' was built under R version 4.1.2\n\n\nWarning: package 'tibble' was built under R version 4.1.2\n\n\nWarning: package 'readr' was built under R version 4.1.2\n\n\nWarning: package 'dplyr' was built under R version 4.1.2\n\n\nWarning: package 'stringr' was built under R version 4.1.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nRows: 2609 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (15): Date.and.Time.of.initial.call, Date.and.time.of.Ranger.response, B...\ndbl  (3): Duration.of.Response, X..of.Animals, Hours.spent.monitoring\nlgl  (4): PEP.Response, Animal.Monitored, Police.Response, ESU.Response\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "slides/Day5-ggplot.html#what-variables-mapped-onto-what-visual-cues-1",
    "href": "slides/Day5-ggplot.html#what-variables-mapped-onto-what-visual-cues-1",
    "title": "ggplot()",
    "section": "What variables mapped onto what visual cues?",
    "text": "What variables mapped onto what visual cues?\n\n\n\n\n\n** This is the last time you will see me use a pie chart in this class!"
  },
  {
    "objectID": "slides/Day5-ggplot.html#what-variables-mapped-onto-what-visual-cues-2",
    "href": "slides/Day5-ggplot.html#what-variables-mapped-onto-what-visual-cues-2",
    "title": "ggplot()",
    "section": "What variables mapped onto what visual cues?",
    "text": "What variables mapped onto what visual cues?"
  },
  {
    "objectID": "slides/Day5-ggplot.html#what-variables-mapped-onto-what-visual-cues-3",
    "href": "slides/Day5-ggplot.html#what-variables-mapped-onto-what-visual-cues-3",
    "title": "ggplot()",
    "section": "What variables mapped onto what visual cues?",
    "text": "What variables mapped onto what visual cues?\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union"
  },
  {
    "objectID": "slides/Day5-ggplot.html#what-variables-mapped-onto-what-visual-cues-4",
    "href": "slides/Day5-ggplot.html#what-variables-mapped-onto-what-visual-cues-4",
    "title": "ggplot()",
    "section": "What variables mapped onto what visual cues?",
    "text": "What variables mapped onto what visual cues?"
  },
  {
    "objectID": "slides/Day5-ggplot.html#what-variables-mapped-onto-what-visual-cues-5",
    "href": "slides/Day5-ggplot.html#what-variables-mapped-onto-what-visual-cues-5",
    "title": "ggplot()",
    "section": "What variables mapped onto what visual cues?",
    "text": "What variables mapped onto what visual cues?\n\n\n`summarise()` has grouped output by 'Year'. You can override using the\n`.groups` argument."
  },
  {
    "objectID": "slides/Day5-ggplot.html#scale",
    "href": "slides/Day5-ggplot.html#scale",
    "title": "ggplot()",
    "section": "Scale",
    "text": "Scale\n\nLinear: Numeric values are evenly spaced on axis.\nLogarithmic: Numeric interval are spaced by a factor of the base of the logarithm.\nCategorical: Categorical values are discretely placed on axis.\nOrdinal: Categorical values are ordered on axis.\nPercent: Percentages of a whole are evenly spaced on axis.\nTime: Date/time values are placed on axis in years, months, days, hours, etc."
  },
  {
    "objectID": "slides/Day5-ggplot.html#examples",
    "href": "slides/Day5-ggplot.html#examples",
    "title": "ggplot()",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "slides/Day5-ggplot.html#context",
    "href": "slides/Day5-ggplot.html#context",
    "title": "ggplot()",
    "section": "Context",
    "text": "Context\nIn every plot you submit for this class, I will be looking for five pieces of context.\n\nThe data’s unit of observation\nVariables represented on the plot\nFilters applied to the data\nGeographic context of the data\nTemporal (date/time range) context of the date"
  },
  {
    "objectID": "slides/Day5-ggplot.html#context-1",
    "href": "slides/Day5-ggplot.html#context-1",
    "title": "ggplot()",
    "section": "Context",
    "text": "Context"
  },
  {
    "objectID": "slides/Day5-ggplot.html#data-visualization-conventions",
    "href": "slides/Day5-ggplot.html#data-visualization-conventions",
    "title": "ggplot()",
    "section": "Data Visualization Conventions",
    "text": "Data Visualization Conventions\n\nEdward Tufte, American statistician sometimes considered “father of data visualization”\nIntroduced the concept of “graphical integrity”\nHow do we present data as honestly as possible?"
  },
  {
    "objectID": "slides/Day5-ggplot.html#lie-factor",
    "href": "slides/Day5-ggplot.html#lie-factor",
    "title": "ggplot()",
    "section": "Lie Factor",
    "text": "Lie Factor\n\nLie Factor = (size of effect in graphic)/(size of effect in data)\nLie factor is greater when variations on a graph fail to match variations in data\n\n\n\n\nTufte, Visual Display of Quantitative Information\n\n\n\n5.3-0.6 / 0.6 * 100= 783 (graph)\n27.5-18 /18 *100 = 53 (data)\n783/53 = 14.8"
  },
  {
    "objectID": "slides/Day5-ggplot.html#inconsistent-scales",
    "href": "slides/Day5-ggplot.html#inconsistent-scales",
    "title": "ggplot()",
    "section": "Inconsistent Scales",
    "text": "Inconsistent Scales\n\n\n\nExample from callingbullshit.org"
  },
  {
    "objectID": "slides/Day5-ggplot.html#presenting-data-out-of-context",
    "href": "slides/Day5-ggplot.html#presenting-data-out-of-context",
    "title": "ggplot()",
    "section": "Presenting Data out of Context",
    "text": "Presenting Data out of Context\n\n\n\nExample from mediamatters.org"
  },
  {
    "objectID": "slides/Day5-ggplot.html#disproportionate-data-to-ink-ratio",
    "href": "slides/Day5-ggplot.html#disproportionate-data-to-ink-ratio",
    "title": "ggplot()",
    "section": "Disproportionate Data-to-Ink Ratio",
    "text": "Disproportionate Data-to-Ink Ratio\n\nEnsure that the ink used on the data match the amount of data presented\nData-to-ink ratio = (ink used to represent data)/(ink used to print graphic)\nShould be as close as possible to 1\nAnother way to think about it: How much of this graph could I erase without losing data?"
  },
  {
    "objectID": "slides/Day5-ggplot.html#disproportionate-data-to-ink-ratio-1",
    "href": "slides/Day5-ggplot.html#disproportionate-data-to-ink-ratio-1",
    "title": "ggplot()",
    "section": "Disproportionate Data-to-Ink Ratio",
    "text": "Disproportionate Data-to-Ink Ratio"
  },
  {
    "objectID": "slides/Day5-ggplot.html#deviating-from-norms",
    "href": "slides/Day5-ggplot.html#deviating-from-norms",
    "title": "ggplot()",
    "section": "Deviating from Norms",
    "text": "Deviating from Norms\n\n\n\nExample from callingbullshit.org"
  },
  {
    "objectID": "slides/Day5-ggplot.html#when-can-i-break-convention",
    "href": "slides/Day5-ggplot.html#when-can-i-break-convention",
    "title": "ggplot()",
    "section": "When can I break convention??",
    "text": "When can I break convention??"
  },
  {
    "objectID": "slides/Day5-ggplot.html#for-today",
    "href": "slides/Day5-ggplot.html#for-today",
    "title": "ggplot()",
    "section": "For Today",
    "text": "For Today\n\nVisualizations Exercise\nIntroduction to ggplot()\nClass Activity"
  },
  {
    "objectID": "slides/Day5-ggplot.html#anatomy-of-the-ggplot-function",
    "href": "slides/Day5-ggplot.html#anatomy-of-the-ggplot-function",
    "title": "ggplot()",
    "section": "Anatomy of the ggplot() function",
    "text": "Anatomy of the ggplot() function\n\nggplot() takes two arguments:\n\ndata: the dataset used to produce the plot (in a data frame)\nmapping: the variables from the dataset we want mapped onto visual cues\n\nmappings are defined in a function called aes() (short for aesthetics)\nin Cartesian plots, we must supply the variables/columns that will appear on the axes (via x = and y =)"
  },
  {
    "objectID": "slides/Day5-ggplot.html#anatomy-of-the-ggplot-function-1",
    "href": "slides/Day5-ggplot.html#anatomy-of-the-ggplot-function-1",
    "title": "ggplot()",
    "section": "Anatomy of the ggplot() function",
    "text": "Anatomy of the ggplot() function\n\nggplot(data = hampshire_census_data, \n       aes(x = COMMUNITY, \n           y = CEN_EARLYED))"
  },
  {
    "objectID": "slides/Day5-ggplot.html#wheres-the-data",
    "href": "slides/Day5-ggplot.html#wheres-the-data",
    "title": "ggplot()",
    "section": "Where’s the data?",
    "text": "Where’s the data?\n\nIn previous plot, we told R what variables to plot, but we didn’t indicate how to plot them.\nTo do this, we need to add a geom function to our ggplot call. Examples:\n\nBar plots: geom_bar()\nScatterplots: geom_point()\n\nAppended to function call with a + sign"
  },
  {
    "objectID": "slides/Day5-ggplot.html#styling-plots-adjusting-the-scale",
    "href": "slides/Day5-ggplot.html#styling-plots-adjusting-the-scale",
    "title": "ggplot()",
    "section": "Styling Plots: Adjusting the Scale",
    "text": "Styling Plots: Adjusting the Scale\n\n# Adjust the Scale\nggplot(data = pioneer_valley_census_data, \n       aes(x = COUNTY,y = CEN_WORKERS)) +\n  geom_point() +\n  coord_flip() +\n  scale_y_log10() +\n  labs(title = \"Number of Workers Age 16+ in Pioneer Valley, MA Municipalities, 2018\", x = \"County\", y = \"Workers Age 16+\")"
  },
  {
    "objectID": "slides/Day5-ggplot.html#do-i-really-have-to-memorize-all-of-these-stylistic-functions",
    "href": "slides/Day5-ggplot.html#do-i-really-have-to-memorize-all-of-these-stylistic-functions",
    "title": "ggplot()",
    "section": "Do I really have to memorize all of these stylistic functions?!",
    "text": "Do I really have to memorize all of these stylistic functions?!\nNo. There are cheatsheets. The ggplot2() cheatsheet is linked here."
  },
  {
    "objectID": "slides/Day5-ggplot.html#lets-create-the-following-data-frame-to-motivate-todays-lecture.",
    "href": "slides/Day5-ggplot.html#lets-create-the-following-data-frame-to-motivate-todays-lecture.",
    "title": "ggplot()",
    "section": "Let’s create the following data frame to motivate today’s lecture.",
    "text": "Let’s create the following data frame to motivate today’s lecture.\n\nThis dataset comes from Pioneer Valley Data and documents estimates of population characteristics for each municipality in the Pioneer Valley.\n\n\nlibrary(tidyverse)\npioneer_valley_census_data <- read.csv(\"https://raw.githubusercontent.com/SDS-192-Intro/SDS-192-public-website/main/slides/datasets/pioneer_valley_census.csv\")\nhampshire_census_data <- pioneer_valley_census_data %>% \n  filter(COUNTY == \"Hampshire\")"
  },
  {
    "objectID": "slides/Day5-ggplot.html#adding-a-geom-function",
    "href": "slides/Day5-ggplot.html#adding-a-geom-function",
    "title": "ggplot()",
    "section": "Adding a geom function",
    "text": "Adding a geom function\n\nggplot(data = hampshire_census_data, \n       aes(x = COMMUNITY, \n           y = CEN_EARLYED)) +\n  geom_col()"
  },
  {
    "objectID": "slides/Day5-ggplot.html#styling-plots-flipping-coordinates",
    "href": "slides/Day5-ggplot.html#styling-plots-flipping-coordinates",
    "title": "ggplot()",
    "section": "Styling Plots: Flipping Coordinates",
    "text": "Styling Plots: Flipping Coordinates\n\nggplot(data = hampshire_census_data, \n       aes(x = COMMUNITY, \n           y = CEN_EARLYED)) +\n  geom_col() +\n  coord_flip() # Flipping the x and y coordinates here makes the labels more legible."
  },
  {
    "objectID": "slides/Day5-ggplot.html#styling-plots-changing-the-theme",
    "href": "slides/Day5-ggplot.html#styling-plots-changing-the-theme",
    "title": "ggplot()",
    "section": "Styling Plots: Changing the Theme",
    "text": "Styling Plots: Changing the Theme\n\nggplot(data = hampshire_census_data, \n       aes(x = COMMUNITY, \n           y = CEN_EARLYED)) +\n  geom_col() +\n  coord_flip() + # Flipping the x and y coordinates here makes the labels more legible. \n  theme_minimal()"
  },
  {
    "objectID": "slides/Day5-ggplot.html#styling-plots-adding-labels",
    "href": "slides/Day5-ggplot.html#styling-plots-adding-labels",
    "title": "ggplot()",
    "section": "Styling Plots: Adding Labels",
    "text": "Styling Plots: Adding Labels\n\nggplot(data = hampshire_census_data, \n       aes(x = COMMUNITY, \n           y = CEN_EARLYED)) +\n  geom_col() +\n  coord_flip() + # Flipping the x and y coordinates here makes the labels more legible. \n  theme_minimal() +\n  labs(title = \"Hampshire County Early Education Enrollment Rates, 2018\", \n       x = \"Enrollment Rate for 3-4 yr old\", \n       y = \"Municipality in Hampshire County, MA\")"
  },
  {
    "objectID": "slides/Day5-ggplot.html#aeshetics-vs.-attributes",
    "href": "slides/Day5-ggplot.html#aeshetics-vs.-attributes",
    "title": "ggplot()",
    "section": "Aeshetics vs. Attributes",
    "text": "Aeshetics vs. Attributes\n\nWe can adjust the way the data appears on plots in two ways:\n\nAccording to a variable:\n\nThis must be done inside of the aes() function\n\nIn a fixed way:\n\nThis must be done outside of the aes() function"
  },
  {
    "objectID": "slides/Day5-ggplot.html#adjusting-data-on-plots-via-aeshetics",
    "href": "slides/Day5-ggplot.html#adjusting-data-on-plots-via-aeshetics",
    "title": "ggplot()",
    "section": "Adjusting Data on Plots via Aeshetics",
    "text": "Adjusting Data on Plots via Aeshetics\n\nWe add visual cues to the plot in the aes() call\n\n\n# Add visual cue for size\nggplot(data = pioneer_valley_census_data, \n       aes(x = COUNTY, y = CEN_WORKERS, size = CEN_HOUSEHOLDS)) +\n  geom_point() +\n  coord_flip() +\n  labs(title = \"Number of Workers Age 16+ in Pioneer Valley, MA Municipalities, 2018\", x = \"County\", y = \"Workers Age 16+\", size = \"Households\")"
  },
  {
    "objectID": "slides/Day5-ggplot.html#adjusting-data-on-plots-via-attributes",
    "href": "slides/Day5-ggplot.html#adjusting-data-on-plots-via-attributes",
    "title": "ggplot()",
    "section": "Adjusting Data on Plots via Attributes",
    "text": "Adjusting Data on Plots via Attributes\n\n# Add visual cue for size and attribute for transparency\nggplot(data = pioneer_valley_census_data, \n       aes(x = COUNTY, y = CEN_WORKERS, size = CEN_HOUSEHOLDS)) +\n  geom_point(alpha = 0.2) +\n  coord_flip() +\n  labs(title = \"Number of Workers Age 16+ in Pioneer Valley, MA Municipalities, 2018\", x = \"County\", y = \"Workers Age 16+\", size = \"Households\")"
  },
  {
    "objectID": "slides/Day6-Frequency.html#for-today",
    "href": "slides/Day6-Frequency.html#for-today",
    "title": "Plotting Frequencies",
    "section": "For Today",
    "text": "For Today\n\nFill out CATME Survey\nHistograms\nBar plots\nMultivariate Frequencies"
  },
  {
    "objectID": "slides/Day6-Frequency.html#lets-create-the-following-data-frame-to-motivate-todays-lecture.",
    "href": "slides/Day6-Frequency.html#lets-create-the-following-data-frame-to-motivate-todays-lecture.",
    "title": "Plotting Frequencies",
    "section": "Let’s create the following data frame to motivate today’s lecture.",
    "text": "Let’s create the following data frame to motivate today’s lecture.\n\nlibrary(tidyverse)\ncounties <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_counties.csv\")\nroute_prefixes <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_route_pre.csv\")\nmaintenance <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_maintenance.csv\")\nkinds <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_kind.csv\")\n\nnbi_ma <- read.delim(\"https://www.fhwa.dot.gov/bridge/nbi/2022/delimited/MA22.txt\", sep = \",\") |>\n  left_join(counties) |>\n  left_join(route_prefixes) |>\n  left_join(maintenance) |>\n  left_join(kinds) |>\n  filter(SERVICE_ON_042A == 1) |>\n  select(STRUCTURE_NUMBER_008, COUNTY_CODE_003_L, ROUTE_PREFIX_005B_L, MAINTENANCE_021_L, YEAR_BUILT_027, ADT_029, STRUCTURE_KIND_043A_L, STRUCTURAL_EVAL_067, BRIDGE_IMP_COST_094) |>\n  mutate(STRUCTURE_KIND_043A_L = \n           case_when(\n             STRUCTURE_KIND_043A_L == \"Concrete continuous\" ~ \"Concrete\",\n             STRUCTURE_KIND_043A_L == \"Steel continuous\" ~ \"Steel\",\n             STRUCTURE_KIND_043A_L == \"Prestressed concrete continuous\" ~ \"Prestressed concrete\",\n             TRUE ~ STRUCTURE_KIND_043A_L)) |>\n  mutate(BRIDGE_IMP_COST_094 = BRIDGE_IMP_COST_094 * 1000)\n\nnbi_hampshire <- nbi_ma |> filter(COUNTY_CODE_003_L == \"Hampshire\")\n\nrm(counties, kinds, maintenance, route_prefixes)"
  },
  {
    "objectID": "slides/Day6-Frequency.html#ggplot",
    "href": "slides/Day6-Frequency.html#ggplot",
    "title": "Plotting Frequencies",
    "section": "ggplot",
    "text": "ggplot\n\nMost plots we create in this course will rely on package called ggplot2\nggplot2 is included in the Tidyverse, which you installed in SDS 100\nLoad ggplot2 in your environment.\n\n\nlibrary(ggplot2)"
  },
  {
    "objectID": "slides/Day6-Frequency.html#histogram",
    "href": "slides/Day6-Frequency.html#histogram",
    "title": "Plotting Frequencies",
    "section": "Histogram",
    "text": "Histogram\n\n\n\nVisualizes distribution of a numeric variable\n\nWhat are maximum and minimum values?\nHow spread out are the values?\nWhat is the center of the values."
  },
  {
    "objectID": "slides/Day6-Frequency.html#histogram-1",
    "href": "slides/Day6-Frequency.html#histogram-1",
    "title": "Plotting Frequencies",
    "section": "Histogram",
    "text": "Histogram\n\n\n\nCreate bins for numbers, each with the same range of values [i.e. 0-10, >10-20, >20-30, and so on]\n\nConverts the linear scale to a categorical scale\n\nCount the numbers in each bin\nSet the height of a bar for that bin to the count"
  },
  {
    "objectID": "slides/Day6-Frequency.html#bar-plot",
    "href": "slides/Day6-Frequency.html#bar-plot",
    "title": "Plotting Frequencies",
    "section": "Bar Plot",
    "text": "Bar Plot\n\n\n\nVisualizes counts of a categorical variable\n\nWhich value appears the most?\nWhich appears the least?\nHow evenly distributed are the counts?\n\n\n\n\n\n [1] \"a\" \"b\" \"c\" \"a\" \"c\" \"a\" \"a\" \"b\" \"c\" \"a\" \"b\" \"c\""
  },
  {
    "objectID": "slides/Day6-Frequency.html#histogram-2",
    "href": "slides/Day6-Frequency.html#histogram-2",
    "title": "Plotting Frequencies",
    "section": "Histogram",
    "text": "Histogram\n\n\n\nDetermine the unique values and places them on the x-axis\nCount the number of times each value appears\nSet the height of a bar for that category to the count\n\n\n\n\n [1] \"a\" \"b\" \"c\" \"a\" \"c\" \"a\" \"a\" \"b\" \"c\" \"a\" \"b\" \"c\""
  },
  {
    "objectID": "slides/Day6-Frequency.html#distribution-in-ma-bridges-years-built",
    "href": "slides/Day6-Frequency.html#distribution-in-ma-bridges-years-built",
    "title": "Plotting Frequencies",
    "section": "Distribution in MA Bridge’s Years Built",
    "text": "Distribution in MA Bridge’s Years Built\n\nggplot(nbi_ma, aes(x = YEAR_BUILT_027)) +\n  geom_histogram()  +\n  theme(text = element_text(size = 20))"
  },
  {
    "objectID": "slides/Day6-Frequency.html#distribution-in-ma-bridges-years-built-1",
    "href": "slides/Day6-Frequency.html#distribution-in-ma-bridges-years-built-1",
    "title": "Plotting Frequencies",
    "section": "Distribution in MA Bridge’s Years Built",
    "text": "Distribution in MA Bridge’s Years Built\n\n\n\nBinwidth indicates the width of the buckets we’d like to categorize our data into.\nBins indicates the number of bins to create.\nWe choose one or the other when creating histograms.\n\n\n\nggplot(nbi_ma, aes(x = YEAR_BUILT_027)) +\n  geom_histogram(binwidth = 10, color = \"white\")"
  },
  {
    "objectID": "slides/Day6-Frequency.html#how-would-we-describe-this-plot",
    "href": "slides/Day6-Frequency.html#how-would-we-describe-this-plot",
    "title": "Plotting Frequencies",
    "section": "How would we describe this plot?",
    "text": "How would we describe this plot?\n\nggplot(nbi_ma, aes(x = YEAR_BUILT_027)) +\n  geom_histogram(binwidth = 10, color = \"white\") +\n  labs(title = \"Distribution in the Years MA Bridges were Built, 2021\", \n       x = \"Year Built\",\n       y = \"Number of Bridges\")"
  },
  {
    "objectID": "slides/Day6-Frequency.html#faceting-a-histogram",
    "href": "slides/Day6-Frequency.html#faceting-a-histogram",
    "title": "Plotting Frequencies",
    "section": "Faceting a Histogram",
    "text": "Faceting a Histogram\n\nggplot(nbi_ma, aes(x = YEAR_BUILT_027)) +\n  geom_histogram(binwidth = 10, color = \"white\") +\n  labs(title = \"Distribution in the Years MA Bridges were Built, 2021\", \n       x = \"Year Built\",\n       y = \"Number of Bridges\") +\n  facet_wrap(vars(ROUTE_PREFIX_005B_L))"
  },
  {
    "objectID": "slides/Day6-Frequency.html#frequency-of-structure-kinds",
    "href": "slides/Day6-Frequency.html#frequency-of-structure-kinds",
    "title": "Plotting Frequencies",
    "section": "Frequency of Structure Kinds",
    "text": "Frequency of Structure Kinds\n\nggplot(nbi_ma, aes(x = STRUCTURE_KIND_043A_L)) +\n  geom_bar()"
  },
  {
    "objectID": "slides/Day6-Frequency.html#labels-for-this-plot",
    "href": "slides/Day6-Frequency.html#labels-for-this-plot",
    "title": "Plotting Frequencies",
    "section": "Labels for this Plot",
    "text": "Labels for this Plot\n\nggplot(nbi_ma, aes(x = STRUCTURE_KIND_043A_L)) +\n  geom_bar() +\n  coord_flip() +\n  labs(title = \"Frequency of Different Kinds of MA Bridge Structures, 2021\", \n       x = \"Structure Kind\",\n       y = \"Number of Bridges\")"
  },
  {
    "objectID": "slides/Day6-Frequency.html#stacked-bar-plot",
    "href": "slides/Day6-Frequency.html#stacked-bar-plot",
    "title": "Plotting Frequencies",
    "section": "Stacked Bar Plot",
    "text": "Stacked Bar Plot\n\nggplot(nbi_ma, aes(x = STRUCTURE_KIND_043A_L, fill = ROUTE_PREFIX_005B_L)) +\n  geom_bar() +\n  coord_flip() +\n  labs(title = \"Frequency of Different Kinds of MA Bridge Structures, 2021\", \n       x = \"Structure Kind\",\n       y = \"Number of Bridges\",\n       fill = \"Route Prefix\") +\n  scale_fill_brewer(palette = 'Set3')"
  },
  {
    "objectID": "slides/Day6-Frequency.html#learning-check-why-not-this",
    "href": "slides/Day6-Frequency.html#learning-check-why-not-this",
    "title": "Plotting Frequencies",
    "section": "Learning Check: Why not this?",
    "text": "Learning Check: Why not this?"
  },
  {
    "objectID": "slides/Day6-Frequency.html#dodging",
    "href": "slides/Day6-Frequency.html#dodging",
    "title": "Plotting Frequencies",
    "section": "Dodging",
    "text": "Dodging\n\nggplot(nbi_ma, aes(x = STRUCTURE_KIND_043A_L, fill = ROUTE_PREFIX_005B_L)) +\n  geom_bar(position = \"dodge\") +\n  coord_flip() +\n  labs(title = \"Frequency of Different Kinds of MA Bridge Structures, 2021\", \n       x = \"Structure Kind\",\n       y = \"Number of Bridges\",\n       fill = \"Route Prefix\") +\n  scale_fill_brewer(palette = 'Set3')"
  },
  {
    "objectID": "slides/Day6-Frequency.html#converting-the-percentages",
    "href": "slides/Day6-Frequency.html#converting-the-percentages",
    "title": "Plotting Frequencies",
    "section": "Converting the Percentages",
    "text": "Converting the Percentages\n\nggplot(nbi_ma, aes(x = STRUCTURE_KIND_043A_L, fill = ROUTE_PREFIX_005B_L)) +\n  geom_bar(position = \"fill\") +\n  coord_flip() +\n  labs(title = \"Frequency of Different Kinds of MA Bridge Structures, 2021\", \n       x = \"Structure Kind\",\n       y = \"Number of Bridges\",\n       fill = \"Route Prefix\") +\n  scale_fill_brewer(palette = 'Set3')"
  },
  {
    "objectID": "slides/Day6-Frequency.html#bar-plot-1",
    "href": "slides/Day6-Frequency.html#bar-plot-1",
    "title": "Plotting Frequencies",
    "section": "Bar Plot",
    "text": "Bar Plot\n\n\n\nDetermine the unique values and places them on the x-axis\nCount the number of times each value appears\nSet the height of a bar for that category to the count\n\n\n\n\n [1] \"a\" \"b\" \"c\" \"a\" \"c\" \"a\" \"a\" \"b\" \"c\" \"a\" \"b\" \"c\""
  },
  {
    "objectID": "slides/Day6-Frequency.html#bidwidth-vs.-bins",
    "href": "slides/Day6-Frequency.html#bidwidth-vs.-bins",
    "title": "Plotting Frequencies",
    "section": "Bidwidth vs. Bins",
    "text": "Bidwidth vs. Bins\n\n\n\nBinwidth indicates the width of the buckets we’d like to categorize our data into.\nBins indicates the number of bins to create.\nWe choose one or the other when creating histograms.\n\n\n\nggplot(nbi_ma, \n       aes(x = YEAR_BUILT_027)) +\n  geom_histogram(binwidth = 10, \n                 color = \"white\")"
  },
  {
    "objectID": "slides/Day6-Frequency.html#converting-to-percentages",
    "href": "slides/Day6-Frequency.html#converting-to-percentages",
    "title": "Plotting Frequencies",
    "section": "Converting to Percentages",
    "text": "Converting to Percentages\n\nggplot(nbi_ma, aes(x = STRUCTURE_KIND_043A_L, fill = ROUTE_PREFIX_005B_L)) +\n  geom_bar(position = \"fill\") +\n  coord_flip() +\n  labs(title = \"Frequency of Different Kinds of MA Bridge Structures, 2021\", \n       x = \"Structure Kind\",\n       y = \"Number of Bridges\",\n       fill = \"Route Prefix\") +\n  scale_fill_brewer(palette = 'Set3')"
  },
  {
    "objectID": "lab-almosts/joining.html",
    "href": "lab-almosts/joining.html",
    "title": "joining",
    "section": "",
    "text": "nabbs_ma -> nabbs routes (on state and route id) nabbs_ma -> nabbs_species (on AOU) nabbs_ma -> nabbs_weather (on routedataid) nabbs_ma -> nabbs_vehicle (on routedataid)\n\nlibrary(rtweet)\n\nWarning: package 'rtweet' was built under R version 4.1.2"
  },
  {
    "objectID": "lab-almosts/joining.html#setting-up-your-environment",
    "href": "lab-almosts/joining.html#setting-up-your-environment",
    "title": "joining",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.2\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.1 \n\n\nWarning: package 'ggplot2' was built under R version 4.1.2\n\n\nWarning: package 'tibble' was built under R version 4.1.2\n\n\nWarning: package 'readr' was built under R version 4.1.2\n\n\nWarning: package 'dplyr' was built under R version 4.1.2\n\n\nWarning: package 'stringr' was built under R version 4.1.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()  masks stats::filter()\n✖ purrr::flatten() masks rtweet::flatten()\n✖ dplyr::lag()     masks stats::lag()\n\nnabbs_ma <- read_csv(\"../data/nabbs/Massach.csv\")\n\nRows: 56311 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): Route, AOU\ndbl (12): RouteDataID, CountryNum, StateNum, RPID, Year, Count10, Count20, C...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnabbs_routes <- read_csv(\"../data/nabbs/routes.csv\")\n\nRows: 5756 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): StateNum, Route, RouteName\ndbl (8): CountryNum, Active, Latitude, Longitude, Stratum, BCR, RouteTypeID,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnabbs_weather <- read_csv(\"../data/nabbs/weather.csv\")\n\nRows: 129644 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (8): StateNum, Route, StartTemp, EndTemp, TempScale, StartTime, EndTime...\ndbl (14): RouteDataID, CountryNum, RPID, Year, Month, Day, ObsN, TotalSpp, S...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nspecies_colnames <- read_fwf(\"../data/nabbs/SpeciesList.txt\", skip = 9, n_max = 1)\n\nRows: 1 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\n\nchr (9): X1, X2, X3, X4, X5, X6, X7, X8, X9\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnabbs_species <- read_fwf(\"../data/nabbs/SpeciesList.txt\", skip = 11)\n\nRows: 756 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\n\nchr (8): X2, X3, X4, X5, X6, X7, X8, X9\ndbl (1): X1\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnames(nabbs_species) <- species_colnames[1,]\n\nWarning: The `value` argument of `names<-` must be a character vector as of\ntibble 3.0.0.\n\nrm(species_colnames)\n\n\nnabbs_ma %>%\n  filter(Year == 2019) %>%\n  left_join(nabbs_species, by = \"AOU\") %>%\n  group_by(AOU, English_Common_Name) %>%\n  summarize(SpeciesTotal = sum(SpeciesTotal)) %>% \n  arrange(desc(SpeciesTotal))\n\n`summarise()` has grouped output by 'AOU'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 128 × 3\n# Groups:   AOU [128]\n   AOU   English_Common_Name    SpeciesTotal\n   <chr> <chr>                         <dbl>\n 1 07610 American Robin                 1180\n 2 05600 Chipping Sparrow                730\n 3 07040 Gray Catbird                    676\n 4 06240 Red-eyed Vireo                  607\n 5 05930 Northern Cardinal               564\n 6 06740 Ovenbird                        510\n 7 07310 Tufted Titmouse                 508\n 8 04980 Red-winged Blackbird            500\n 9 07350 Black-capped Chickadee          491\n10 05810 Song Sparrow                    487\n# … with 118 more rows\n\nnabbs_ma %>%\n  filter(Year == 1966) %>%\n  left_join(nabbs_species, by = \"AOU\") %>%\n  group_by(AOU, English_Common_Name) %>%\n  summarize(SpeciesTotal = sum(SpeciesTotal)) %>% \n  arrange(desc(SpeciesTotal))\n\n`summarise()` has grouped output by 'AOU'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 76 × 3\n# Groups:   AOU [76]\n   AOU   English_Common_Name  SpeciesTotal\n   <chr> <chr>                       <dbl>\n 1 04930 European Starling             762\n 2 05110 Common Grackle                351\n 3 07610 American Robin                317\n 4 05870 Eastern Towhee                182\n 5 04980 Red-winged Blackbird          178\n 6 06882 House Sparrow                 159\n 7 04770 Blue Jay                      123\n 8 07040 Gray Catbird                  121\n 9 05810 Song Sparrow                  107\n10 00510 Herring Gull                   98\n# … with 66 more rows\n\n\n\nnabbs_ma %>%\n  left_join(nabbs_species, by = \"AOU\") %>%\n  filter(English_Common_Name == \"Least Tern\") %>%\n  group_by(Year) %>%\n  summarize(SpeciesTotal = sum(SpeciesTotal)) %>%\n  ggplot(aes(x = Year, y = SpeciesTotal, group = 1)) +\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"Annual NABBS Counts of Least Tern Population, MA\", x = \"Year\", y = \"Species Total\")\n\n\n\n\n\nnabbs_species %>%\n  anti_join(nabbs_ma, by = \"AOU\") %>%\n  distinct(English_Common_Name) %>% nrow()\n\n[1] 571\n\ntest <-nabbs_ma %>% filter(Route == \"019\")\n\nFiltering Joins\nAnti\nWhich bird species have never been counted in MA?\n\nnabbs_species %>%\n  anti_join(nabbs_ma, by = \"AOU\") %>%\n  select(English_Common_Name) %>%\n  unique()\n\n# A tibble: 571 × 1\n   English_Common_Name         \n   <chr>                       \n 1 Black-bellied Whistling-Duck\n 2 Fulvous Whistling-Duck      \n 3 Emperor Goose               \n 4 Snow Goose                  \n 5 (Blue Goose) Snow Goose     \n 6 Ross's Goose                \n 7 Greater White-fronted Goose \n 8 Brant                       \n 9 (Black Brant) Brant         \n10 Cackling Goose              \n# … with 561 more rows\n\n\n\nlibrary(leaflet)\n\nWarning: package 'leaflet' was built under R version 4.1.2\n\nlibrary(sf)\n\nLinking to GEOS 3.9.1, GDAL 3.3.1, PROJ 8.1.0; sf_use_s2() is TRUE\n\nroutes <-\nnabbs_routes %>%\n  filter(StateNum == 47) %>%\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4269) %>%\n  st_transform(4326)\nleaflet(width = \"100%\") %>%\n  setView(lat = 42.40, lng = -71.38, zoom = 10) %>%\n  addProviderTiles(providers$Stamen.Toner) %>%\n  addCircleMarkers(data = routes, label = ~RouteName)"
  },
  {
    "objectID": "labs/lab6.html#north-american-bird-breeding-survey-dataset",
    "href": "labs/lab6.html#north-american-bird-breeding-survey-dataset",
    "title": "Lab 6: Joining Datasets",
    "section": "North American Bird Breeding Survey Dataset",
    "text": "North American Bird Breeding Survey Dataset"
  },
  {
    "objectID": "slides/Day7-Boxplots.html#for-today",
    "href": "slides/Day7-Boxplots.html#for-today",
    "title": "Boxplots",
    "section": "For Today",
    "text": "For Today\n\nMeasures of Central Tendency and Dispersion\nBoxplots\nProject 1 Assigned"
  },
  {
    "objectID": "slides/Day7-Boxplots.html#lets-create-the-following-data-frame-to-motivate-todays-lecture.",
    "href": "slides/Day7-Boxplots.html#lets-create-the-following-data-frame-to-motivate-todays-lecture.",
    "title": "Boxplots",
    "section": "Let’s create the following data frame to motivate today’s lecture.",
    "text": "Let’s create the following data frame to motivate today’s lecture.\n\nlibrary(tidyverse)\ncounties <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_counties.csv\")\nroute_prefixes <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_route_pre.csv\")\nmaintenance <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_maintenance.csv\")\nkinds <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_kind.csv\")\n\nnbi_ma <- read.delim(\"https://www.fhwa.dot.gov/bridge/nbi/2022/delimited/MA22.txt\", sep = \",\") |>\n  left_join(counties) |>\n  left_join(route_prefixes) |>\n  left_join(maintenance) |>\n  left_join(kinds) |>\n  filter(SERVICE_ON_042A == 1) |>\n  select(STRUCTURE_NUMBER_008, COUNTY_CODE_003_L, ROUTE_PREFIX_005B_L, MAINTENANCE_021_L, YEAR_BUILT_027, ADT_029, STRUCTURE_KIND_043A_L, STRUCTURAL_EVAL_067, BRIDGE_IMP_COST_094) |>\n  mutate(STRUCTURE_KIND_043A_L = \n           case_when(\n             STRUCTURE_KIND_043A_L == \"Concrete continuous\" ~ \"Concrete\",\n             STRUCTURE_KIND_043A_L == \"Steel continuous\" ~ \"Steel\",\n             STRUCTURE_KIND_043A_L == \"Prestressed concrete continuous\" ~ \"Prestressed concrete\",\n             TRUE ~ STRUCTURE_KIND_043A_L)) |>\n  mutate(BRIDGE_IMP_COST_094 = BRIDGE_IMP_COST_094 * 1000)\n\nnbi_hampshire <- nbi_ma |> filter(COUNTY_CODE_003_L == \"Hampshire\")\n\nrm(counties, kinds, maintenance, route_prefixes)"
  },
  {
    "objectID": "slides/Day7-Boxplots.html#mean",
    "href": "slides/Day7-Boxplots.html#mean",
    "title": "Boxplots",
    "section": "Mean",
    "text": "Mean\n\n\n\nSum of values divided by number of values summed\nTakes every value into consideration\nModel of entire dataset\nHeavily influenced by outliers"
  },
  {
    "objectID": "slides/Day7-Boxplots.html#median",
    "href": "slides/Day7-Boxplots.html#median",
    "title": "Boxplots",
    "section": "Median",
    "text": "Median\n\n\n\nMiddle value(s) of the dataset when all values are lined from smallest to largest\nDoes not model entire dataset\nLimited influence from outliers"
  },
  {
    "objectID": "slides/Day7-Boxplots.html#normal-distributions",
    "href": "slides/Day7-Boxplots.html#normal-distributions",
    "title": "Boxplots",
    "section": "Normal Distributions",
    "text": "Normal Distributions\n\n\n\nMore values huddle around some center line and taper off as we move away from center\nHistogram is symmetrical with a perfectly normal distribution\nMedian and mean should be about the same; mean is a good measure of central tendency"
  },
  {
    "objectID": "slides/Day7-Boxplots.html#skew",
    "href": "slides/Day7-Boxplots.html#skew",
    "title": "Boxplots",
    "section": "Skew",
    "text": "Skew\n\n\n\nHistogram is non-symmetrical when there is skew\nLong trail to the right of center indicates a right skew\nMedian becomes more representative measure of central tendency than mean"
  },
  {
    "objectID": "slides/Day7-Boxplots.html#summarizing-data",
    "href": "slides/Day7-Boxplots.html#summarizing-data",
    "title": "Boxplots",
    "section": "Summarizing Data",
    "text": "Summarizing Data\n\nMeasures of central tendency summarize swaths of information into single value\nCan be reductionist\n\nExample: Measures of central tendency related to wealth in the US only tell us about those in the middle\nHide the experiences of the most impoverished communities.\n\nDegree of spread or dispersion is just as important as center"
  },
  {
    "objectID": "slides/Day7-Boxplots.html#range",
    "href": "slides/Day7-Boxplots.html#range",
    "title": "Boxplots",
    "section": "Range",
    "text": "Range\n\n\n\nMaximum value minus the minimum value\nEvaluates the spread of the entire dataset"
  },
  {
    "objectID": "slides/Day7-Boxplots.html#interquartile-range",
    "href": "slides/Day7-Boxplots.html#interquartile-range",
    "title": "Boxplots",
    "section": "Interquartile Range",
    "text": "Interquartile Range\n\n\n\n1st quartile is middle value between minimum and median\n3rd quartile is middle value between median and maximum\nIQR is the difference between the 1st and 3rd quartile\nRepresents the middle 50% of values"
  },
  {
    "objectID": "slides/Day7-Boxplots.html#boxplot",
    "href": "slides/Day7-Boxplots.html#boxplot",
    "title": "Boxplots",
    "section": "Boxplot",
    "text": "Boxplot"
  },
  {
    "objectID": "slides/Day7-Boxplots.html#grouped-boxplots",
    "href": "slides/Day7-Boxplots.html#grouped-boxplots",
    "title": "Boxplots",
    "section": "Grouped Boxplots",
    "text": "Grouped Boxplots\n\nggplot(nbi_hampshire, aes(x = ADT_029, y = ROUTE_PREFIX_005B_L)) +\n  geom_boxplot() +\n  labs(title = \"Distribution in the Average Daily Traffic of Hampshire County, MA Bridges, 2021\", \n       x = \"Average Daily Traffic\",\n       y = \"Route Prefix\") +\n  theme_minimal()"
  },
  {
    "objectID": "slides/Day7-Boxplots.html#interpreting-boxplots-step-1-check-for-outliers",
    "href": "slides/Day7-Boxplots.html#interpreting-boxplots-step-1-check-for-outliers",
    "title": "Boxplots",
    "section": "Interpreting Boxplots Step 1: Check for Outliers",
    "text": "Interpreting Boxplots Step 1: Check for Outliers\n\nHow many are there? What do they indicate? Do you assume they are errors in teh data? Or do they represent extremes that are important for us to take into consideration?"
  },
  {
    "objectID": "slides/Day7-Boxplots.html#interpreting-boxplots-step-2-compare-medians",
    "href": "slides/Day7-Boxplots.html#interpreting-boxplots-step-2-compare-medians",
    "title": "Boxplots",
    "section": "Interpreting Boxplots Step 2: Compare Medians",
    "text": "Interpreting Boxplots Step 2: Compare Medians\n\nDo the medians line up? If not, in which groups are the medians higher and in which are they lower?"
  },
  {
    "objectID": "slides/Day7-Boxplots.html#interpreting-boxplots-step-3-compare-range",
    "href": "slides/Day7-Boxplots.html#interpreting-boxplots-step-3-compare-range",
    "title": "Boxplots",
    "section": "Interpreting Boxplots Step 3: Compare Range",
    "text": "Interpreting Boxplots Step 3: Compare Range\n\nDo certain groups have a wider range of values represented than others? In other words, are the values more distributed for certain groups than for others? This might indicate a greater degree of disparity in some groups than others."
  },
  {
    "objectID": "slides/Day7-Boxplots.html#interpreting-boxplots-step-4-compare-iqr",
    "href": "slides/Day7-Boxplots.html#interpreting-boxplots-step-4-compare-iqr",
    "title": "Boxplots",
    "section": "Interpreting Boxplots Step 4: Compare IQR",
    "text": "Interpreting Boxplots Step 4: Compare IQR\n\nIn which groups do the middle 50% of values tend to huddle around a central value? In which are they more spread out from the center?"
  },
  {
    "objectID": "slides/Day7-Boxplots.html#interpreting-boxplots-step-5-compare-symmetry",
    "href": "slides/Day7-Boxplots.html#interpreting-boxplots-step-5-compare-symmetry",
    "title": "Boxplots",
    "section": "Interpreting Boxplots Step 5: Compare Symmetry",
    "text": "Interpreting Boxplots Step 5: Compare Symmetry\n\nDoes the median appear to be in the center of the range and IQR? Is the median closer to the minimum – or the bottom whisker? Or the top whisker?"
  },
  {
    "objectID": "labs/lab6.html#epas-echo",
    "href": "labs/lab6.html#epas-echo",
    "title": "Lab 6: Joining Datasets",
    "section": "EPA’s ECHO",
    "text": "EPA’s ECHO\nThe U.S. Environmental Protection Agency (EPA) is responsible for monitoring and regulating over 800,000 industrial facilities in the United States. This involves regularly inspecting facilities for their compliance with different environmental laws, issuing notices and taking enforcement actions when facilities are out of compliance, and issuing penalties for failures to address violations. A record of all of this activity is maintained online in a database known as ECHO - Enforcement and Compliance History Online. Specifically, ECHO maintains information about enforcement and compliance actions taken in relation to the Clean Air Act, the Clean Water Act, the Safe Drinking Water Act, and the Resource Conservation and Recovery Act.\nThere is so much information available to us via ECHO, and there is even an R package that makes it really simple to access the data in ECHO. However, because ECHO documents compliance with a number of different environmental laws, when we pull data from ECHO about regulated facilities, it is usually organized into separate tables for each law. Today we are going to pull a dataset documenting facilities’ compliance with the Clean Air Act and then pull a second dataset documenting facilities’ compliance with the Clean Water Act.\nThe Clean Air Act was first enacted in 1963 to regulate emissions from both stationary and mobile sources of air pollution. The Clean Water Act was enacted in 1972 regulate the polluting of U.S. waterways. When a facility violates one of these Acts, it typically means that they have released an excess amount of pollutants, neglected to implement the proper control technologies or standards, or failed to submit reports."
  },
  {
    "objectID": "labs/lab6.html#data-analysis",
    "href": "labs/lab6.html#data-analysis",
    "title": "Lab 6: Joining Datasets",
    "section": "Data Analysis",
    "text": "Data Analysis\n\nJoin Keys\nWhen joining data frames, we need to determine a join key. This will be a variable that exists in both data frames to uniquely identify the data frames’ units of observation. This key needs to be shared across the two data frames in order to be able to join the data frames together. This is because when joining data frames, R will check the value in that variable for one data frame and look for the corresponding value in the key variable for the other data frame in order to determine which units of observation match across the data frames.\nFor instance, let’s say we have two data frames documenting the same group of students - one documents their current courses, and another documents their living arrangements. For both data frames, the student ID might serve as a join key. This ID uniquely identifies each student, and we expect that that key will be the same in both data frames (i.e. the 99- that represents you in one data frame will be the same as the 99- number that represents you in another data frame.) Because that number matches across the two data frames, we can use that variable to determine which rows (i.e. students) in the first data frame are associated with which rows in the second data frame.\n\nQuestion\n\nReference the data dictionaries for these two data frames to determine the names of the variables that we will join on. Remember that these should be variables that we can use to uniquely identify each unit of observation across the data frames. Write code below to determine the number of unique values for the variables you identify in each of these data frames. (Hint: Remember how we counted unique values in a variable in lab 1?)\n\n\n# Write code here for echo_air_ca\n# Write code here for echo_water_ca\n\n\n\n\n\n\nIf you’ve done this correctly, you’ll learn that there are 2602 unique values echo_air_ca’s key variable. You’ll also learn that there are 28262 unique values in echo_water_ca’s key variable.\n…but if you look in your environment, there are 2794 rows in echo_air_ca, and there are 30829 rows in echo_water_ca. This means that the key variable must repeat, and that certain facilities are represented more than once in each of these data frames.\nThis happens because even the EPA defines a facility in different ways depending on what laws it is regulated by. What counts as one facility in the Clean Air Act may count as two or three facilities in the Clean Water Act, but there is only one ID to document the relationships between them. This means that the same ID may show up a few times for different parts of the same facility. Check out some of the duplicating rows via my code below. Notice how multiple facilities - sometimes with entirely different names can be associated with the same RegistryID?\n\necho_air_ca |>\n  group_by(RegistryID) |>\n  filter(n() > 1) |>\n  arrange(RegistryID) |>\n  head()\n\n\n\n  \n\n\n\nLater we will talk about what happens when we join two data frames that both have repeating keys, but for now, we’re going aggregate both of these data frames so that each Registry ID only appears once. Basically, we are going to write code to say: if any of the rows pertaining to this Registry ID document certain kinds of violations, set the violation flag for that facility to 1.\n\n\n\nQuestion\n\nIn the code below, for each of the data frames, you should group the data by RegistryID and then summarize by returning the max() value in CurrSvFlag, ViolFlag, CurrVioFlag, Insp5yrFlag for each group. Store the results in echo_air_ca_agg and echo_water_ca_agg respectively. Note how many rows in are in echo_air_ca_agg and echo_water_ca_agg. It should match the number of unique values in RegistryID that you calculated above.\n\n\necho_air_ca_agg <-\n  echo_air_ca |>\n  ______ |>             #group by RegistryID\n  _____(                #Summarize the following:\n    CurrSvFlag = _____, #Calculate max value in CurrSvFlag\n    ViolFlag = _____,   #Calculate max value in ViolFlag\n    CurrVioFlag = _____,#Calculate max value in CurrVioFlag\n    Insp5yrFlag = _____ #Calculate max value in Insp5yrFlag\n  )\n\necho_water_ca_agg <- #Copy and adjust code above to create echo_water_ca_agg\n\n\n\n\n\n\n\nJoins\nSo now we have two data frames with unique registry IDs and variables indicating whether the facility has a current significant violation, whether it has had a violation in the past three years, whether it has a current violation, and whether it has been inspected in the past 5 years.\nNow we want to join these two data frames together, so that we can check which facilities have violations to both the Clean Air Act and the Clean Water Act.\n\nQuestion\n\nWrite code below to perform four kinds of joins - a left join, a right join, and inner join, and a full join. echo_water_ca_agg should be in the first position, and echo_air_ca_agg should be joined onto it. I’ve started that for you in the commented code below.\n\n\n#joined_left <- echo_water_ca_agg |> \n\n#joined_right <- echo_water_ca_agg |> \n\n#joined_inner <- echo_water_ca_agg |> \n\n#joined_full <- echo_water_ca_agg |> \n\n\n\n\n\n\nCheck out the first six rows of joined_full below and note what happens when a RegistryID is present in one data frame but not in the other. We see data values in the columns associated with the data frame where the RegistryID was present, and NA values in the columns associated with the data frame where the RegistryID was not present. This is how we come to have so many extra rows in joined_full.\n\njoined_full |> head()\n\n\n\n  \n\n\n\nWith joined_inner, any rows associated with a RegistryID that doesn’t appear in both data frames get dropped. Check out the number of rows in joined_inner.\n\nnrow(joined_inner)\n\n[1] 739\n\n\nThis represents the number of Registry IDs that were present in both echo_water_ca_agg and echo_air_ca_agg.\n\n\n\nQuestion\n\nThe three statements below are incorrect. Correct my statements below about the remaining joins.\n\n\npaste(nrow(joined_full), \"represents the number of Registry IDs present in echo_air_ca_agg\")\n\npaste(nrow(joined_left), \"represents the number of Registry IDs present in either echo_air_ca_agg or echo_water_ca_agg\")\n\npaste(nrow(joined_right), \"represents the number of Registry IDs present in echo_water_ca_agg\")\n\n\n\n\n\n\nFrom here on out, since we are only interested in the facilities with violations to both Acts, we are going to focus on joined_inner. Let’s remove the rest of the joined data frames from our environment.\n\nrm(joined_full, joined_left, joined_right)\n\nYou might note at this point that it can be difficult to tell which columns are associated with which environmental laws. This is because originally in both data frames, the variables we are most interested in (CurrSvFlag, ViolFlag, CurrVioFlag, Insp5yrFlag) shared the same names. When we joined these two data frames together, R opted to distinguish between them by tacking a .x onto the variable names for the data frame in the first position in the join and a .y onto the variable names for the data frame in the second position in the join. Let’s give each of these variables more meaningful names. To do so, we can use the rename() function, which is included in the dplyr package in the tidyverse. When piped to a dataframe, the formula for rename() is simple: rename(new_name = old_name)\n\n\n\nQuestion\n\nIn my code below rename the remaining columns to differentiate between the column names from the two original data frames.\n\n\njoined_inner_renamed <-\n  joined_inner |>\n  rename(\n    CurrSvFlag_water = CurrSvFlag.x,\n    ViolFlag_water = ViolFlag.x \n    #Rename remaining columns here. Be sure to separate by comma. There are six more to go!\n  )\n\n\n\n\n\n\nQuestion\n\nNow that we have a cleaned up data frame, use a data wrangling verb to subset the data frame to the rows where a RegistryID has a current violation to both the Clean Water Act and Clean Air Act. Repeat these steps to determine which RegistryIDs have a significant violation to both Acts. I recommend running this on joined_inner_renamed.\nOnce you’ve run these codes, open both echo_air_ca and echo_water_ca by clicking on the data frames in your environment. Search for a few of the RegistryIDs that appeared in your analysis to identify the names and locations of the facilities.\n\n\n# Write wrangling code here for current violations. \n\n# Write wrangling code here for significant violations. \n\n\n\n\n\n\n\nMany-to-Many Joins\nWe don’t know much about these facilities because we lost a lot of critical information (e.g. the facility’s name and location) when aggregating our data by RegistryID above. At the time, we couldn’t aggregate by name because certain facilities sharing the same RegistryID had different names! Aggregating by name would have meant that we’d still have repeating RegistryIds in the data frame - one for each different version of the facility’s name.\nThe truth is though that we didn’t technically have to ensure that the RegistryID didn’t repeat. We can still join data frames in instances where the join key repeats in both data frames. This is called performing a many-to-many join because we are joining many of the same key in one data frame to many of the same key in another data frame. It’s important to pay attention to what happens when we make this join. Let’s look at an example: the facility with Registry ID 110001181186.\n\necho_air_ca |>\n  filter(RegistryID == 110001181186) \n\n\n\n  \n\n\necho_water_ca |>\n  filter(RegistryID == 110001181186)\n\n\n\n  \n\n\n\nIn echo_air_ca, this RegistryID appears three times in the data frame, and in echo_water_ca, this RegistryID appears twice in the data frame. In each instance, the facility has a different name. What happens to facility 110001181186 when we perform a many-to-many join?\n\necho_water_ca |> \n  inner_join(echo_air_ca, by = \"RegistryID\") |> \n  filter(RegistryID == 110001181186) |>\n  select(CWPName, AIRName, RegistryID)\n\n\n\n  \n\n\n\nThis inner join created six rows for facility 110001181186. This is because R matched each of the three rows for facility 110001181186 in echo_air_ca with the two rows for facility 110001181186 in echo_water_ca (and 3 rows * 2 rows = 6 rows).\nThis is one of the reasons why it is important to understand the context of datasets and their units of observation. Here we’ve joined these rows together as if they represented matching facilities. …but just by looking at this output we know that PLATFORM A is going to be physically different than DOS CUADRAS/SOUTH COUNTY/PLATFORM B, and PLATFORM B is going to be physically different than DOS CUADRAS/SOUTH COUNTY/PLATFORM C. The only thing that they all share is that they are sub-parts of the facility with the RegistryID 110001181186. Ultimately the units of observation don’t match across these two data frames because Clean Air Act and Clean Water Act delineate facilities differently. By aggregating to RegistryID above - something shared across the two Acts - we standardized the unit of observation, which made it possible to perform a more meaningful join.\n\nQuestion\n\nUsing my code as an example, determine how many times facility 110000483619 appears in echo_air_ca and how many times it appears in echo_water_ca. How many times would this facility show up if we were to perform a many-to-many inner join for these two data frames? Write your response as a comment in the code chunk.\n\n\n#Write code here for echo_air_ca!\n\n#Write code here for echo_water_ca!\n\n#Write comment here!\n\n\n\n\n\n\n\nFiltering Joins\nSometimes we want to know which observations that appear in one data frame don’t appear in another data frame. This might tell us where we have missing data. In this case, it will tell us which facilities are regulated by one environmental law and not the other.\n\nQuestion\n\nWrite code below to perform two anti_join()s. The first should tell me which facilities are regulated by the Clean Air Act and not the Clean Water Act, and the second should tell me which facilities are regulated by the Clean Water Act and not the Clean Air Act.\n\n\n#not_clean_water <- Write code here!\n\n#not_clean_air <- Write code here!\n\n\n\n\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nIn March 2020, the Environmental Protection Agency published a memo that permitted industrial facilities impacted by the Covid-19 pandemic to temporarily suspend mandated pollution monitoring. Research published by the Environmental Data Governance Initiative (EDGI) (Nost et al. 2020) has indicated that, during this time, reported violations to the Clean Air Act and the Clean Water Act dropped considerably. However, EDGI’s report goes on to argue that violations were likely being under-counted at this time. …first, because facilities were not being required to monitor and report data to the same degree during Covid-19, and second, because the EPA was conducting fewer inspections during Covid-19. In sum, as they say, “the absence of data should not be taken as the absence of pollution.” What social harms emerge in the wake of these data absences? Who benefits from these policies, and who faces the greatest risks? How should we as data scientists think and act as we anaylze and present this data?"
  },
  {
    "objectID": "solutions/lab6.html",
    "href": "solutions/lab6.html",
    "title": "Lab 6: Joining Datasets",
    "section": "",
    "text": "Question\n\nReference the data dictionaries for these two data frames to determine the names of the variables that we will join on. Remember that these should be variables that we can use to uniquely identify each unit of observation across the data frames. Write code below to determine the number of unique values for the variables you identify in each of these data frames. (Hint: Remember how we counted unique values in a variable in lab 1?)\n\n\n\n\n\n\n[1] 2604\n\n\n[1] 28264\n\n\n\n\nQuestion\n\nIn the code below, for each of the data frames, you should group the data by RegistryID and then summarize by returning the max() value in CurrSvFlag, ViolFlag, CurrVioFlag, Insp5yrFlag for each group. Store the results in echo_air_ca_agg and echo_water_ca_agg respectively. Note how many rows in are in echo_air_ca_agg and echo_water_ca_agg. It should match the number of unique values in RegistryID that you calculated above.\n\n\n\n\n\n\n\n\n\nQuestion\n\nWrite code below to perform four kinds of joins - a left join, a right join, and inner join, and a full join. echo_water_ca_agg should be in the first position, and echo_air_ca_agg should be joined onto it. I’ve started that for you in the commented code below.\n\n\n\n\n\n\n\n\n\nQuestion\n\nThe three statements below are incorrect. Correct my statements below about the remaining joins.\n\n\n\n\n\n\n[1] \"2604 represents the number of Registry IDs present in echo_air_ca_agg\"\n\n\n[1] \"30132 represents the number of Registry IDs present in either echo_air_ca_agg or echo_water_ca_agg\"\n\n\n[1] \"28264 represents the number of Registry IDs present in echo_water_ca_agg\"\n\n\n\n\n\n\n\nQuestion\n\nIn my code below rename the remaining columns to differentiate between the column names from the two original data frames.\n\n\n\n\n\n\n\n\n\nQuestion\n\nNow that we have a cleaned up data frame, use a data wrangling verb to subset the data frame to the rows where a RegistryID has a current violation to both the Clean Water Act and Clean Air Act. Repeat these steps to determine which RegistryIDs have a significant violation to both Acts. I recommend running this on joined_inner_renamed.\nOnce you’ve run these codes, open both echo_air_ca and echo_water_ca by clicking on the data frames in your environment. Search for a few of the RegistryIDs that appeared in your analysis to identify the names and locations of the facilities.\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\n\nQuestion\n\nUsing my code as an example, determine how many times facility 110000483619 appears in echo_air_ca and how many times it appears in echo_water_ca. How many times would this facility show up if we were to perform a many-to-many inner join for these two data frames? Write your response as a comment in the code chunk.\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\n\nQuestion\n\nWrite code below to perform two anti_join()s. The first should tell me which facilities are regulated by the Clean Air Act and not the Clean Water Act, and the second should tell me which facilities are regulated by the Clean Water Act and not the Clean Air Act."
  },
  {
    "objectID": "templates/lab6.html",
    "href": "templates/lab6.html",
    "title": "Lab 6: Joining Datasets",
    "section": "",
    "text": "echo_air_dd <- echoAirGetMeta() |>\n  filter(ColumnID %in% c(1,4,5,8,99,100,101,102)) |>\n  select(ColumnID, ObjectName, Description)\n\necho_water_dd <- echoWaterGetMeta() |>\n  filter(ColumnID %in% c(1,4,5,9,184,185,186,187)) |>\n  select(ColumnID, ObjectName, Description)\n\n\nQuestion\n\nReference the data dictionaries for these two data frames to determine the names of the variables that we will join on. Remember that these should be variables that we can use to uniquely identify each unit of observation across the data frames. Write code below to determine the number of unique values for the variables you identify in each of these data frames. (Hint: Remember how we counted unique values in a variable in lab 1?)\n\n\n# Write code here for echo_air_ca\n# Write code here for echo_water_ca\n\n\n\nQuestion\n\nIn the code below, for each of the data frames, you should group the data by RegistryID and then summarize by returning the max() value in CurrSvFlag, ViolFlag, CurrVioFlag, Insp5yrFlag for each group. Store the results in echo_air_ca_agg and echo_water_ca_agg respectively. Note how many rows in are in echo_air_ca_agg and echo_water_ca_agg. It should match the number of unique values in RegistryID that you calculated above.\n\n\necho_air_ca_agg <-\n  echo_air_ca |>\n  ______ |>             #group by RegistryID\n  _____(                #Summarize the following:\n    CurrSvFlag = _____, #Calculate max value in CurrSvFlag\n    ViolFlag = _____,   #Calculate max value in ViolFlag\n    CurrVioFlag = _____,#Calculate max value in CurrVioFlag\n    Insp5yrFlag = _____ #Calculate max value in Insp5yrFlag\n  )\n\necho_water_ca_agg <- #Copy and adjust code above to create echo_water_ca_agg\n\n\n\nQuestion\n\nWrite code below to perform four kinds of joins - a left join, a right join, and inner join, and a full join. echo_water_ca_agg should be in the first position, and echo_air_ca_agg should be joined onto it. I’ve started that for you in the commented code below.\n\n\n#joined_left <- echo_water_ca_agg |> \n\n#joined_right <- echo_water_ca_agg |> \n\n#joined_inner <- echo_water_ca_agg |> \n\n#joined_full <- echo_water_ca_agg |> \n\n\n\nQuestion\n\nThe three statements below are incorrect. Correct my statements below about the remaining joins.\n\n\npaste(nrow(joined_full), \"represents the number of Registry IDs present in echo_air_ca_agg\")\n\npaste(nrow(joined_left), \"represents the number of Registry IDs present in either echo_air_ca_agg or echo_water_ca_agg\")\n\npaste(nrow(joined_right), \"represents the number of Registry IDs present in echo_water_ca_agg\")\n\n\nrm(joined_full, joined_left, joined_right)\n\n\n\nQuestion\n\nIn my code below rename the remaining columns to differentiate between the column names from the two original data frames.\n\n\njoined_inner_renamed <-\n  joined_inner |>\n  rename(\n    CurrSvFlag_water = CurrSvFlag.x,\n    ViolFlag_water = ViolFlag.x \n    #Rename remaining columns here. Be sure to separate by comma. There are six more to go!\n  )\n\n\n\nQuestion\n\nNow that we have a cleaned up data frame, use a data wrangling verb to subset the data frame to the rows where a RegistryID has a current violation to both the Clean Water Act and Clean Air Act. Repeat these steps to determine which RegistryIDs have a significant violation to both Acts. I recommend running this on joined_inner_renamed.\nOnce you’ve run these codes, open both echo_air_ca and echo_water_ca by clicking on the data frames in your environment. Search for a few of the RegistryIDs that appeared in your analysis to identify the names and locations of the facilities.\n\n\n# Write wrangling code here for current violations. \n\n# Write wrangling code here for significant violations. \n\n\n\nQuestion\n\nUsing my code as an example, determine how many times facility 110000483619 appears in echo_air_ca and how many times it appears in echo_water_ca. How many times would this facility show up if we were to perform a many-to-many inner join for these two data frames? Write your response as a comment in the code chunk.\n\n\n#Write code here for echo_air_ca!\n\n#Write code here for echo_water_ca!\n\n#Write comment here!\n\n\n\nQuestion\n\nWrite code below to perform two anti_join()s. The first should tell me which facilities are regulated by the Clean Air Act and not the Clean Water Act, and the second should tell me which facilities are regulated by the Clean Water Act and not the Clean Air Act.\n\n\n#not_clean_water <- Write code here!\n\n#not_clean_air <- Write code here!"
  },
  {
    "objectID": "slides/Day8-Wrangling.html#for-today",
    "href": "slides/Day8-Wrangling.html#for-today",
    "title": "Data Wrangling",
    "section": "For Today",
    "text": "For Today\n\nQuiz 1\nSubsetting Data\nAggregating Data\nActivity"
  },
  {
    "objectID": "slides/Day8-Wrangling.html#select",
    "href": "slides/Day8-Wrangling.html#select",
    "title": "Data Wrangling",
    "section": "select()",
    "text": "select()\n\nselect() enables us to select variables (columns) of interest."
  },
  {
    "objectID": "slides/Day8-Wrangling.html#filter",
    "href": "slides/Day8-Wrangling.html#filter",
    "title": "Data Wrangling",
    "section": "filter()",
    "text": "filter()\n\nfilter() subsets observations (rows) according to a certain criteria that we provide."
  },
  {
    "objectID": "slides/Day8-Wrangling.html#arrange",
    "href": "slides/Day8-Wrangling.html#arrange",
    "title": "Data Wrangling",
    "section": "arrange()",
    "text": "arrange()\n\narrange() sorts rows according to values in a column\nDefaults to sorting from smallest to largest (numeric) or first character to last character (character)."
  },
  {
    "objectID": "slides/Day8-Wrangling.html#group_by",
    "href": "slides/Day8-Wrangling.html#group_by",
    "title": "Data Wrangling",
    "section": "group_by()",
    "text": "group_by()\n\ngroup_by() groups observations with a shared value in a variable\nGrouping only changes the metadata of a data frame; we combine group_by() with other functions to transform the data frame\nValues remain in groups unless we ungroup() it. This is important if we intend to run further operations on the resulting data."
  },
  {
    "objectID": "slides/Day8-Wrangling.html#summary-functions",
    "href": "slides/Day8-Wrangling.html#summary-functions",
    "title": "Data Wrangling",
    "section": "Summary functions",
    "text": "Summary functions"
  },
  {
    "objectID": "slides/Day8-Wrangling.html#summarize",
    "href": "slides/Day8-Wrangling.html#summarize",
    "title": "Data Wrangling",
    "section": "summarize()",
    "text": "summarize()\n\nsummarize() computes a value across a vector of values and stores it in a new data frame\n\n\n\nHow is this different than only applying a summary function to a vector?"
  },
  {
    "objectID": "slides/Day8-Wrangling.html#group_by-summarize",
    "href": "slides/Day8-Wrangling.html#group_by-summarize",
    "title": "Data Wrangling",
    "section": "group_by() |> summarize()",
    "text": "group_by() |> summarize()\n\ngroup_by() groups observations with a shared value in a variable\nWhen we combine group_by() and summarize() we can perform operations within groups"
  },
  {
    "objectID": "slides/Day8-Wrangling.html#mutate",
    "href": "slides/Day8-Wrangling.html#mutate",
    "title": "Data Wrangling",
    "section": "mutate()",
    "text": "mutate()\n\nmutate() creates a new variable (column) in a data frame and fills values according to criteria we provide"
  },
  {
    "objectID": "slides/Day8-Wrangling.html#group_by-filter",
    "href": "slides/Day8-Wrangling.html#group_by-filter",
    "title": "Data Wrangling",
    "section": "group_by() |> filter()",
    "text": "group_by() |> filter()\n\ngroup_by() groups observations with a shared value in a variable\nWhen we combine group_by() and filter() we can filter within groups"
  },
  {
    "objectID": "slides/Day8-Wrangling.html#group_by-filter-1",
    "href": "slides/Day8-Wrangling.html#group_by-filter-1",
    "title": "Data Wrangling",
    "section": "group_by() |> filter()",
    "text": "group_by() |> filter()\n\nspotify_playlists |>\n  group_by(playlist_name) |>\n  filter(valence == max(valence)) |>\n  select(playlist_name, track.name, valence) |>\n  head()"
  },
  {
    "objectID": "slides/Day8-Wrangling.html#group_by-mutate",
    "href": "slides/Day8-Wrangling.html#group_by-mutate",
    "title": "Data Wrangling",
    "section": "group_by() |> mutate()",
    "text": "group_by() |> mutate()\n\ngroup_by() groups observations with a shared value in a variable\nWhen we combine group_by() and mutate() we can perform operations within groups and add the resulting variable to the data frame"
  },
  {
    "objectID": "slides/Day8-Wrangling.html#whats-wrong-with-this-code",
    "href": "slides/Day8-Wrangling.html#whats-wrong-with-this-code",
    "title": "Data Wrangling",
    "section": "What’s wrong with this code?",
    "text": "What’s wrong with this code?\n\nWhich song has the duration that takes up the greatest percentage of time on any playlist?\n\n\nspotify_playlists |>\n  group_by(playlist_name) |>\n  mutate(TOTAL_DURATION = sum(track.duration_ms),\n         PERCENT_DURATION = track.duration_ms/TOTAL_DURATION * 100) |>\n  filter(PERCENT_DURATION == max(PERCENT_DURATION)) |>\n  select(playlist_name, track.name, track.duration_ms, TOTAL_DURATION, PERCENT_DURATION) |>\n  head()"
  },
  {
    "objectID": "slides/Day8-Wrangling.html#ungroup",
    "href": "slides/Day8-Wrangling.html#ungroup",
    "title": "Data Wrangling",
    "section": "ungroup()",
    "text": "ungroup()\n\nspotify_playlists |>\n  group_by(playlist_name) |>\n  mutate(TOTAL_DURATION = sum(track.duration_ms),\n         PERCENT_DURATION = track.duration_ms/TOTAL_DURATION * 100) |>\n  ungroup() |>\n  filter(PERCENT_DURATION == max(PERCENT_DURATION)) |>\n  select(playlist_name, track.name, track.duration_ms, TOTAL_DURATION, PERCENT_DURATION) |>\n  head()"
  },
  {
    "objectID": "labs/lab7.html",
    "href": "labs/lab7.html",
    "title": "Lab 7: Tidying Data",
    "section": "",
    "text": "In this lab, we will create a few data visualizations documenting point-in-time counts of homelessness in the United States. Specifically, we are going visualize data collected in 2020 through various Continuums of Care (CoCs) programs. In order to produce these data visualizations, you will need to join homelessness data with census population data and develop and execute a plan for how to wrangle the dataset into a “tidy” format.\n\n\n\nRecognize the differences between tidy and non-tidy data\nPivot datasets both longer and wider\nSeparate and unite columns\nConsider the ethical implications of analyzing homeless counts"
  },
  {
    "objectID": "labs/lab7.html#review-of-key-terms",
    "href": "labs/lab7.html#review-of-key-terms",
    "title": "Lab 7: Tidying Data",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\n\n\n\n\n\nTip\n\n\n\nYou may wish to reference this Tidy R Cheatsheet when completing this lab.\n\n\n\nTidy data\n\nA rectangular data table in which every row is an observation and every column is a variable describing something about that observation\n\nPivoting\n\nRotating data columns so that they are presented as rows or rotating data rows so that they are presented as columns"
  },
  {
    "objectID": "labs/lab7.html#huds-point-in-time-counts",
    "href": "labs/lab7.html#huds-point-in-time-counts",
    "title": "Lab 7: Tidying Data",
    "section": "HUD’s Point-in Time Counts",
    "text": "HUD’s Point-in Time Counts\nThe U.S. Department of Housing and Urban Development is responsible for monitoring and addressing housing affordability and homelessness throughout the country. One initiative that they oversee towards this end is the Continuum of Care Program. Continuums of Care (CoCs) are local planning organizations responsible for allocating resources and coordinating services to address homelessness in the United States. Every state has a number of CoCs that report to the Department of Housing and Urban Development.\nEvery year, on a single night in the last 10 days of January, CoCs required to conduct a “point-in-time” count of sheltered and unsheltered homeless individuals. To generate the sheltered point-in-time count, CoCs coordinate with transitional housing centers or emergency shelters to record the number of people housed in that location on the selected night. There are a few different approaches to generating an unsheltered point-in-time count:\n\n“Night of” street count: Volunteers are sent out to canvass either the entire geography of a random sample of areas in a CoC. While canvassing, they are expected to record the number of people they see currently residing in spaces not designed for sleeping accommodations.\nService-based count: In the 7 days following the night of the designated PIT count, volunteers are dispatched to food kitchens, shelters, libraries and other services identified as spaces that unhoused individuals frequent. There, they are expected to survey individuals to determine if they were sheltered on the night of the count.\n\nFollowing the night of the count, the collected data points are shipped off to HUD where they are aggregated into the dataset we will be working with today.\nThere are a number of limitations to this approach of estimating homelessness, which I encourage you to consider throughout this analysis and when responding to our Ethical Considerations question."
  },
  {
    "objectID": "labs/lab7.html#setting-up-your-environment",
    "href": "labs/lab7.html#setting-up-your-environment",
    "title": "Lab 7: Tidying Data",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nRun the code below to load today’s data frames into your environment.\n\n\nlibrary(tidyverse)\npit <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/main/website/data/pit_2015_2020.csv\") \n\n#Note for future reference how adding the following could remove unreliable counts:\n# mutate_at(vars(2:97), funs(case_when(. < 20 ~ as.numeric(NA), TRUE ~ .)))\n\ngender <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/main/website/data/gender_state_2015_2020.csv\")\nrace <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/main/website/data/race_state_2015_2020.csv\")"
  },
  {
    "objectID": "labs/lab7.html#data-analysis",
    "href": "labs/lab7.html#data-analysis",
    "title": "Lab 7: Tidying Data",
    "section": "Data Analysis",
    "text": "Data Analysis\n\nReview Data Frame\n\nQuestion\n\nReference the data dictionaries for these two data frames to determine the names of the variables that we will join on. Remember that these should be variables that we can use to uniquely identify each unit of observation across the data frames. Write code below to determine the number of unique values for the variables you identify in each of these data frames. (Hint: Remember how we counted unique values in a variable in lab 1?)\n\n\n# Write code here for echo_air_ca\n# Write code here for echo_water_ca\n\n\n\n\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nIn March 2020, the Environmental Protection Agency published a memo that permitted industrial facilities impacted by the Covid-19 pandemic to temporarily suspend mandated pollution monitoring. Research published by the Environmental Data Governance Initiative (EDGI) (Nost et al. 2020) has indicated that, during this time, reported violations to the Clean Air Act and the Clean Water Act dropped considerably. However, EDGI’s report goes on to argue that violations were likely being under-counted at this time. …first, because facilities were not being required to monitor and report data to the same degree during Covid-19, and second, because the EPA was conducting fewer inspections during Covid-19. In sum, as they say, “the absence of data should not be taken as the absence of pollution.” What social harms emerge in the wake of these data absences? Who benefits from these policies, and who faces the greatest risks? How should we as data scientists think and act as we anaylze and present this data?"
  },
  {
    "objectID": "slides/Day8-Wrangling.html#six-verbs-for-data-wrangling",
    "href": "slides/Day8-Wrangling.html#six-verbs-for-data-wrangling",
    "title": "Data Wrangling",
    "section": "Six “verbs” for data wrangling",
    "text": "Six “verbs” for data wrangling\n\narrange()\nselect()\nfilter()\nmutate()\nsummarize()\ngroup_by()"
  },
  {
    "objectID": "slides/Day8-Wrangling.html#subsetting-data-involves-selecting-relevant-variables-and-observations-for-analysis.-aggregating-data-involves-compiling-and-summarizing-data.",
    "href": "slides/Day8-Wrangling.html#subsetting-data-involves-selecting-relevant-variables-and-observations-for-analysis.-aggregating-data-involves-compiling-and-summarizing-data.",
    "title": "Data Wrangling",
    "section": "Subsetting data involves selecting relevant variables and observations for analysis. Aggregating data involves compiling and summarizing data.",
    "text": "Subsetting data involves selecting relevant variables and observations for analysis. Aggregating data involves compiling and summarizing data."
  },
  {
    "objectID": "slides/Day9-Joining.html#for-today",
    "href": "slides/Day9-Joining.html#for-today",
    "title": "Joining Datasets",
    "section": "For Today",
    "text": "For Today\n\nReminder: Quiz 1 and Group Projects!\nMSA Recap\nJoining Datasets\nMosaic Effect"
  },
  {
    "objectID": "slides/Day9-Joining.html#load-the-following-two-data-frames-into-rstudio.",
    "href": "slides/Day9-Joining.html#load-the-following-two-data-frames-into-rstudio.",
    "title": "Joining Datasets",
    "section": "Load the following two data frames into RStudio.",
    "text": "Load the following two data frames into RStudio.\n\nWhat do you notice about them? Why might they be useful to consider together?\n\n\nlibrary(tidyverse)\nct_school_attendance <- read.csv(\"https://data.ct.gov/resource/t4hx-jd4c.csv?$limit=3000\") %>%\n  filter(studentgroup == \"All Students\" & reportingdistrictname != \"Connecticut\")\nct_school_learning_model <-\n  read.csv(\"https://data.ct.gov/resource/5q7h-u2ac.csv?$where=update_date=%272020-09-17%27\") |>\n  rename(fully_remote = percent_students_fully_remote)"
  },
  {
    "objectID": "slides/Day9-Joining.html#joining-datasets",
    "href": "slides/Day9-Joining.html#joining-datasets",
    "title": "Joining Datasets",
    "section": "Joining Datasets",
    "text": "Joining Datasets\n\nUp until this point, we have been working with data in single tables.\nSometimes related data can be spread across multiple tables that we wish to bring together for different kinds of analysis."
  },
  {
    "objectID": "slides/Day9-Joining.html#keys",
    "href": "slides/Day9-Joining.html#keys",
    "title": "Joining Datasets",
    "section": "Keys",
    "text": "Keys\n\nA column shared across the tables that we can join on.\nWhat might be an issue with joining on this key?"
  },
  {
    "objectID": "slides/Day9-Joining.html#keys-1",
    "href": "slides/Day9-Joining.html#keys-1",
    "title": "Joining Datasets",
    "section": "Keys",
    "text": "Keys"
  },
  {
    "objectID": "slides/Day9-Joining.html#join",
    "href": "slides/Day9-Joining.html#join",
    "title": "Joining Datasets",
    "section": "Join",
    "text": "Join\n\nct_school_attendance |>\n  inner_join(ct_school_learning_model, \n             by = c(\"reportingdistrictcode\" = \"district_code\")) |>\n  select(reportingdistrictcode, attrate_202021, fully_remote) |>\n  head()"
  },
  {
    "objectID": "slides/Day9-Joining.html#different-kinds-of-joins",
    "href": "slides/Day9-Joining.html#different-kinds-of-joins",
    "title": "Joining Datasets",
    "section": "Different Kinds of Joins",
    "text": "Different Kinds of Joins\n\ninner_join()\nleft_join()\nright_join()\nfull_join()\nanti_join()"
  },
  {
    "objectID": "slides/Day9-Joining.html#inner-join",
    "href": "slides/Day9-Joining.html#inner-join",
    "title": "Joining Datasets",
    "section": "Inner Join",
    "text": "Inner Join"
  },
  {
    "objectID": "slides/Day9-Joining.html#outer-joins",
    "href": "slides/Day9-Joining.html#outer-joins",
    "title": "Joining Datasets",
    "section": "Outer Joins",
    "text": "Outer Joins"
  },
  {
    "objectID": "slides/Day9-Joining.html#one-to-many-relationships",
    "href": "slides/Day9-Joining.html#one-to-many-relationships",
    "title": "Joining Datasets",
    "section": "One-to-many relationships",
    "text": "One-to-many relationships"
  },
  {
    "objectID": "slides/Day9-Joining.html#many-to-many-relationships",
    "href": "slides/Day9-Joining.html#many-to-many-relationships",
    "title": "Joining Datasets",
    "section": "Many-to-many relationships",
    "text": "Many-to-many relationships"
  },
  {
    "objectID": "slides/Day9-Joining.html#anti-joins",
    "href": "slides/Day9-Joining.html#anti-joins",
    "title": "Joining Datasets",
    "section": "Anti-joins",
    "text": "Anti-joins\n\n\n\nct_school_attendance %>%\n  anti_join(ct_school_learning_model, by = c(\"reportingdistrictcode\" = \"district_code\")) %>%\n  select(reportingdistrictcode)"
  },
  {
    "objectID": "slides/Day9-Joining.html#ethics-of-joining-data",
    "href": "slides/Day9-Joining.html#ethics-of-joining-data",
    "title": "Joining Datasets",
    "section": "Ethics of Joining Data",
    "text": "Ethics of Joining Data"
  },
  {
    "objectID": "slides/Day9-Joining.html#timeline-of-healthcare-privacy-protections",
    "href": "slides/Day9-Joining.html#timeline-of-healthcare-privacy-protections",
    "title": "Joining Datasets",
    "section": "Timeline of Healthcare Privacy Protections",
    "text": "Timeline of Healthcare Privacy Protections\n\n1996: Healthcare Insurance Portability and Accountability Act (HIPAA) signed into law\n1999: HIPAA Privacy Rule put out for public comment; first issued in 2002\n\nEstablishes standards to secure “protected health information”\nLimits uses and disclosures of health information\nEstablishes individual rights over protected health information"
  },
  {
    "objectID": "slides/Day9-Joining.html#survey-1-link",
    "href": "slides/Day9-Joining.html#survey-1-link",
    "title": "Joining Datasets",
    "section": "Survey 1 Link",
    "text": "Survey 1 Link\n\nhttps://bit.ly/3wC3iAC"
  },
  {
    "objectID": "slides/Day9-Joining.html#survey-2-link",
    "href": "slides/Day9-Joining.html#survey-2-link",
    "title": "Joining Datasets",
    "section": "Survey 2 Link",
    "text": "Survey 2 Link\n\nhttps://bit.ly/3tBE4QM"
  },
  {
    "objectID": "labs/lab7.html#our-goal",
    "href": "labs/lab7.html#our-goal",
    "title": "Lab 7: Tidying Data",
    "section": "Our Goal",
    "text": "Our Goal\nToday’s goal is to produce just two plots:\n\nA timeseries of rates of both sheltered and unsheltered homelessness by gender for a given state.\nA timeseries of rates of both sheltered and unsheltered homelessness by race for a given state.\n\nThe first plot will look like this:\n\n…and the second plot will look like this.\n\nTo be able to produce these plots, we are going to have to do considerable amount of data wrangling and cleaning. This lab will walk you through those steps."
  },
  {
    "objectID": "labs/lab7.html#data-cleaning",
    "href": "labs/lab7.html#data-cleaning",
    "title": "Lab 7: Tidying Data",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nFirst, let’s consider each of our data frames. You have pit, which documents homelessness counts from 2015 to 2020 in each state along a number of categories (such as gender, race, sheltered vs. unsheltered, etc.). Note how in this data frame each row is a state, and each variable is a count associated with a particular category.\n\nhead(pit)\n\n\n\n  \n\n\n\nThis is an example of untidy data. The reason is that there is data we want to visualize (such as race, gender, year, and sheltered vs. unsheltered) that are stored in our column headers, not in cells of the data frame. To be able to visualize this data by race, by gender, or by year we need to pivot our data frames so that the values stored in column headers instead get restored in data cells.\n\nQuestion\n\nWrite code to pivot longer columns 2 through 97 (i.e. all columns except the first state column) in pit. Store the names in column called “Measure” and store the values in a column called “Value.” Store the resulting data frame in in pit_pivoted.\n\n\n# Uncomment below and write code to pivot dataset. Store the results in pit_pivoted\n\n# pit_pivoted <- pit |> pivot_longer(_____)\n\n\n\n\n\n\nIf you’ve done this correctly the first six rows of the resulting data frame should look like this.\n\nhead(pit_pivoted)\n\n\n\n  \n\n\n\nNow you’ll notice that that we have some more cleaning to do because many separate variables are all stored in the Measure column. We have: the year, the demographic, whether the count is for Sheltered or Unsheltered individuals all stored in the column. We need to separate all of these distinct variables into different columns. We’re going to do this in a few steps.\n\n\n\nQuestion\n\nWrite code to separate Measure into two columns. One column should be called Shel_Unshel, and the second should be called Demographic. Note what symbol separates these two pieces of data: space dash space (i.e. ” - “). You will need to specify this in the sep argument for separate(). Store the resulting data frame in in pit_separate_1.\n\n\n# Uncomment below and complete the code to separate the Measure column. Store the results in pit_separate_1\n\n# pit_separate_1 <- pit_pivoted |> separate(_____)\n\n\n\n\n\n\nIf you’ve done this correctly the first six rows of the resulting data frame should look like this.\n\nhead(pit_separate_1)\n\n\n\n  \n\n\n\nWe still have some cleaning to do because we have demographic data and year data are stored in the same column.\n\n\n\nQuestion\n\nWrite code to separate Demographic into two columns. One column should be called Demographic, and the second should be called Year. Note what symbol separates these two pieces of data: comma space (i.e. “,”). You will need to specify this in the sep argument for separate(). Store the resulting data frame in in pit_separate_2.\n\n\n# Uncomment below and complete the code to separate the Demographic column. Store the results in pit_separate_2\n\n# pit_separate_2 <- pit_separate_1 |> separate(_____)\n\n\n\n\n\n\nIf you’ve done this correctly the first six rows of the resulting data frame should look like this.\n\nhead(pit_separate_2)\n\n\n\n  \n\n\n\nAs a final cleaning step, let’s remove some unnecessary characters from the Shel_Unshel column. Specifically, let’s remove the string “Total” since we already know that these represent Total counts for these categories.\n\n\n\nQuestion\n\nRemove the string “Total” from the Shel_Unshel column. To do this you should use the str_replace() function. Replace the string “Total” with the empty string (i.e. ““) to remove these characters. Store the results in pit_str_cleaned.\n\n\n# Uncomment below and complete the code to replace the string \"Total \" in Shel_Unshel with an empty string. Store the results in pit_str_cleaned\n\n# pit_str_cleaned <- pit_separate_2 |> mutate(Shel_Unshel = _______)\n\n\n\n\n\n\nIf you’ve done this correctly the first six rows of the resulting data frame should look like this.\n\nhead(pit_str_cleaned)\n\n\n\n  \n\n\n\nThe next issue is that there are multiple kinds of demographics in the demographic column. Specifically, we have both race represented in that column and gender represented in that column. Remember the rules of tidy data, according to Wickham (2014):\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\nThis issue violates the third rule. In one table, we have two observational units - a count of unhoused individuals by race and a count of unhoused individuals by gender. To clean this up, we need to separate this into different tables.\n\n\n\nQuestion\n\nCreate two new data frames - one for pit_by_gender and one for pit_by_race. To do this you want to extract the rows with values in Demographic %in% the following vector of values: c(\"Female\", \"Male\", \"Transgender\"), and store the result in pit_by_gender. Then you want to extract the rows with values in Demographic %in% the following vector of values:\nc(\"Black or African American\", \n  \"Asian\", \n  \"American Indian or Alaska Native\", \n  \"Native Hawaiian or Other Pacific Islander\", \n  \"White\", \n  \"Multiple Races\")\nand store the result in pit_by_race.\n\n\n# Uncomment below to create two new tables.\n\n# pit_by_gender <- pit_str_cleaned |>\n# pit_by_race <- pit_str_cleaned |>\n\n\n\n\n\n\nIf you’ve done this correctly the first six rows of the resulting data frame should look like this.\n\nhead(pit_by_gender)\n\n\n\n  \n\n\nhead(pit_by_race)\n\n\n\n  \n\n\n\nNow we have two significantly cleaned data tables. Let’s go ahead and remove some of the data frames we no longer need from our environmennt.\n\nrm(pit, pit_pivoted, pit_separate_1, pit_separate_2, pit_str_cleaned)\n\nWith the final data, we could visualize counts of homelessness per state using each of these tables. Check out the plot below to see an example of what this might look like.\n\n\n\n\n\nThe problem now is that we don’t know if higher counts are a result of greater homelessness for that sub-group or a result of there being higher populations of that sub-group in each state. For instance, in the plot above, is the count of homelessness higher for unsheltered white individuals because white individuals are more likely to be homeless or because there is a higher population of white individuals in Florida? We ultimately want to consider the rates of homelessness per x number of people of that demographic in that state. To do that, we’re going to need to join this dataset with some census data documenting population.\nI’ve supplied you with two census tables:\n\ngender: documents population estimates and margin of error (moe) for each gender in each state from 2015-2020\n\n\nhead(gender)\n\n\n\n  \n\n\n\n\nrace: documents population estimates and margin of error (moe) for each race in each state from 2015-2020\n\n\nhead(race)\n\n\n\n  \n\n\n\nNote that in these two data frames - just like above - we have values stored in our column headers, and we need to pivot our data longer and then clean it up. We’re going to complete this in three parts.\n\n\n\nQuestion\n\nCreate two new tidy data frames - gender_final and race_final. For each, you should do this in 3 steps:\nStep 1: Pivot the columns estimate_2015:moe_2020 longer, storing the column names in “Measure” and the values in “Values”. Step 2: Separate the values in Measure into two columns: “Measure”, “Year”. Note that these are separated by an underscore (“_“). Step 3: Pivot the Measure column wider, taking the values from the Values column.\n\n\n# Fill in the blanks to clean up these data frames. \n\n# Step 1\n#gender_pivoted_longer <- gender |> pivot_longer(_____)\n\n# Step 2\n#gender_separated <- gender_pivoted |> separate(_____)\n\n# Step 3\n#gender_final <- gender_separated |> pivot_wider(_____)\n\n# Step 1\n#race_pivoted_longer <- race |> pivot_longer(_____)\n\n# Step 2\n#race_separated <- race_pivoted |> separate(_____)\n\n# Step 3\n#race_final <- race_separated |> pivot_wider(_____)\n\n\n\n\n\n\nIf you’ve done this correctly the first six rows of the resulting data frame should look like this.\n\nhead(gender_final)\n\n\n\n  \n\n\nhead(gender_final)\n\n\n\n  \n\n\n\nNow that we have cleaned up census data, let’s go ahead and remove some data frames that we no longer need in our environments\n\nrm(gender, gender_pivoted_longer, gender_separated, race, race_pivoted_longer, race_separated)\n\nOur next step is to join the census data with our point-in-time count data tables. Let’s just compare pit_by_gender to gender_final to discern the join key.\n\nhead(pit_by_gender)\n\n\n\n  \n\n\nhead(gender_final)\n\n\n\n  \n\n\n\nNotice that here, we need to join on three variables: the state, the year, and the demographic. …but you might have noticed a problem. In our census data table, states are written out, and in our point-in-time data table, states are abbreviated. The values need to match for the join to work, so we will need to create a new column with the full state name in our point-in-time count data table. I’ve written the code to do this below. You should run that code before moving on to the next step.\n\npit_by_gender <- \n  pit_by_gender |>\n  mutate(StateName = state.name[match(State, state.abb)])\n  \npit_by_race <- \n  pit_by_race |>\n  mutate(StateName = state.name[match(State, state.abb)])\n\nNow our data frames are formatted in such a way that we can join pit_by_gender to gender_final and pit_by_race to race_final.\n\n\n\nQuestion\n\nJoin pit_by_gender on the left to gender_final on the right and store the results in pit_gender. Join pit_by_race on the left to race_final on the right and store the results in pit_race. In both case, you will be joining by three variables: State, Demographic, and Year. We can set the left key variables to the right key variables by setting the by argument to this in the join: c(\"StateName\" = \"NAME\", \"Demographic\" = \"variable\", \"Year\" = \"Year\")\n\n\n# Uncomment below and write code to join pit_by_gender to gender_final and pit_by_race to race_final. Store the results in pit_gender and pit_race respectively.\n\n# pit_gender <- pit_by_gender |>\n# pit_race <- pit_by_race |>\n\n\n\n\n\n\nIf you’ve done this correctly the first six rows of the resulting data frames should look like this.\n\nhead(pit_gender)\n\n\n\n  \n\n\nhead(pit_race)\n\n\n\n  \n\n\n\nOne final step before we can create our plots! We need to create a new column that calculates the rate of homelessness for this sub-group per 10,000 population of that sub-group in that state and year. You have all of the pieces you need to do this now.\n\n\n\nQuestion\n\nUse a data wrangling verb to create a new column in both pit_gender and pit_race that calculates the rate of homelessness per 10,000 population. Set that column name to homeless_rate. Hint: When creating that column, you’ll need to divide the homeless count by the population estimate and then multiply by 10000. Store the resulting data frames in\n\n\n# Uncomment below and write code to create a new column for homelessness_rate in both pit_gender and pit_race\n\n# pit_gender_rates <- pit_gender |>\n# pit_race_rates <- pit_race |>\n\n\n\n\n\n\nIf you’ve done this correctly the first six rows of the resulting data frames should look like this.\n\nhead(pit_gender_rates)\n\n\n\n  \n\n\nhead(pit_race_rates)\n\n\n\n  \n\n\n\nIf you’ve done everything correctly, you should be able to run the following code to generate the plots presented at the beginning of the lab. Feel free to swap out the State filter for different states to see how the rates compare across the US.\n\noptions(scipen=999)\n\npit_gender_rates |>\n  filter(State == \"FL\") |>\n  ggplot(aes(x = Year, \n             y = homeless_rate, \n             col = Demographic, \n             group = Demographic)) +\n  geom_line() +\n  facet_wrap(vars(Shel_Unshel)) +\n  theme_bw() +\n  theme(legend.position = \"bottom\") +\n  labs(title = \"Point in Time Homeless Rates in FL, 2015-2020\",\n       x = \"Year\", \n       y = \"Homeless per 10,000 Population\",\n       col = \"Gender\")\n\n\n\npit_race_rates |>\n  filter(State == \"FL\") |>\n  ggplot(aes(x = Year, \n             y = homeless_rate, \n             col = Demographic, \n             group = Demographic)) +\n  geom_line() +\n  facet_wrap(vars(Shel_Unshel)) +\n  theme_bw() +\n  theme(legend.position = \"bottom\") +\n  guides(color = guide_legend(nrow = 3, byrow = TRUE)) +\n  labs(title = \"Point in Time Homeless Rates in FL, 2015-2020\",\n       x = \"Year\", \n       y = \"Homeless per 10,000 Population\",\n       col = \"Race\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nWhile this is one of the primary datasets used to direct resources for homelessness in the U.S., there are a number of reasons unhoused individual may go uncounted through point-in-time counts:\n\nOnly individuals that are visible to enumerators get counted. However, other sources (such as surveys in public schools) have shown that the majority of unhoused individuals are not on the streets or in shelters, but instead are temporarily residing with family/friends or are in staying in motels.\nAs living on the streets is increasingly criminalized in cities across the U.S., there are reasons why unhoused individuals may try to avoid being seen on a given night.\nOnly recently have counting protocols started to include separate categories for Transgender and Gender Non-Conforming individuals. When counting PIT data against census population data, however, these sub-groups won’t appear because the census only collects Sex as “Male” and “Female”.\nDifferent CoCs use different methods to generate PIT counts.\n\nFor more information see e National Law Center on Homelessness & Poverty (2017).\nTaking into consideration all of these factors, how should we, as data scientists, think about and present this data? Is there value in PIT count data? How should we go about communicating its shortcomings? How might we go about collecting this data in different ways? Share your ideas on our sds-192-discussions Slack channel."
  },
  {
    "objectID": "slides/Day10-EthicsJoining.html",
    "href": "slides/Day10-EthicsJoining.html",
    "title": "Ethics of Joining Datasets",
    "section": "",
    "text": "Reminder: Quiz 1 Due!\nMosaic Effect\nOur Data Bodies Activity\nGroups for Project 2"
  },
  {
    "objectID": "slides/Day10-EthicsJoining.html#ethics-of-joining-data",
    "href": "slides/Day10-EthicsJoining.html#ethics-of-joining-data",
    "title": "Ethics of Joining Datasets",
    "section": "Ethics of Joining Data",
    "text": "Ethics of Joining Data"
  },
  {
    "objectID": "slides/Day10-EthicsJoining.html#timeline-of-healthcare-privacy-protections",
    "href": "slides/Day10-EthicsJoining.html#timeline-of-healthcare-privacy-protections",
    "title": "Ethics of Joining Datasets",
    "section": "Timeline of Healthcare Privacy Protections",
    "text": "Timeline of Healthcare Privacy Protections\n\n1996: Healthcare Insurance Portability and Accountability Act (HIPAA) signed into law\n1999: HIPAA Privacy Rule put out for public comment; first issued in 2002\n\nEstablishes standards to secure “protected health information”\nLimits uses and disclosures of health information\nEstablishes individual rights over protected health information"
  },
  {
    "objectID": "slides/Day10-EthicsJoining.html#survey-1-link",
    "href": "slides/Day10-EthicsJoining.html#survey-1-link",
    "title": "Ethics of Joining Datasets",
    "section": "Survey 1 Link",
    "text": "Survey 1 Link\n\nhttps://bit.ly/3wC3iAC"
  },
  {
    "objectID": "slides/Day10-EthicsJoining.html#survey-2-link",
    "href": "slides/Day10-EthicsJoining.html#survey-2-link",
    "title": "Ethics of Joining Datasets",
    "section": "Survey 2 Link",
    "text": "Survey 2 Link\n\nhttps://bit.ly/3tBE4QM"
  },
  {
    "objectID": "slides/Day10-EthicsJoining.html#for-today",
    "href": "slides/Day10-EthicsJoining.html#for-today",
    "title": "Ethics of Joining Datasets",
    "section": "For Today",
    "text": "For Today\n\nReminder: Quiz 1 Due!\nMosaic Effect\nOur Data Bodies Activity\nGroups for Project 2"
  },
  {
    "objectID": "labs/lab8.html",
    "href": "labs/lab8.html",
    "title": "Lab 8: Programming with Data",
    "section": "",
    "text": "In this lab, we will program some custom R functions that allow us to analyze data related to medical conflicts of interest. Specifically, we will determine which ten Massachusetts-based doctors received the most money from pharmaceutical or medical device manufacturers in 2021. Then we will leverage our custom functions to produce a number of tables and plots documenting information about the payments made to each of these doctors. In doing so, we will update a similar analysis produced by ProPublica in 2018 called Dollars for Docs.\n\n\n\nWrite custom R functions\nIterate a function over the values in a vector, using the family of map functions\nPractice data cleaning and wrangling"
  },
  {
    "objectID": "labs/lab8.html#review-of-key-terms",
    "href": "labs/lab8.html#review-of-key-terms",
    "title": "Lab 8: Programming with Data",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\n\n\n\n\n\nTip\n\n\n\nYou may wish to reference this purrr Cheatsheet when completing this lab.\n\n\n\nFunction\n\na series of statements that returns a value or performs a task\n\nIteration\n\nrepeating a task over a series of values, vectors, or lists"
  },
  {
    "objectID": "slides/Day12-Cleaning.html#for-today",
    "href": "slides/Day12-Cleaning.html#for-today",
    "title": "Tidying Datasets",
    "section": "For Today",
    "text": "For Today\n\nFormatting up Columns and Values\nParsing Dates\nConditionals\nPivoting\nSeparating Columns"
  },
  {
    "objectID": "slides/Day12-Cleaning.html#converting-types",
    "href": "slides/Day12-Cleaning.html#converting-types",
    "title": "Tidying Datasets",
    "section": "Converting Types",
    "text": "Converting Types\n\nas.character(), as.numeric(), as.logical() all convert a variable from an original type to a new type\n\n\nBeforeCodeAfter\n\n\n\ntypeof(prisons$COUNTYFIPS)\n\n[1] \"character\"\n\n\n\n\n\nprisons <- \n  prisons |> \n  mutate(COUNTYFIPS = as.numeric(COUNTYFIPS))\n\n\n\n\ntypeof(prisons$COUNTYFIPS)\n\n[1] \"double\""
  },
  {
    "objectID": "slides/Day12-Cleaning.html#parsing-dates",
    "href": "slides/Day12-Cleaning.html#parsing-dates",
    "title": "Tidying Datasets",
    "section": "Parsing Dates",
    "text": "Parsing Dates\n\nDates can be converted to a date format using the lubridate package\n\nStep 1: Check how dates are formatted\nStep 2: Find corresponding conversion code on lubridate cheatsheet"
  },
  {
    "objectID": "slides/Day12-Cleaning.html#setting-dates",
    "href": "slides/Day12-Cleaning.html#setting-dates",
    "title": "Tidying Datasets",
    "section": "Setting Dates",
    "text": "Setting Dates\n\nymd_hms() will take a date formatted as year, month, day, hour, minute, second and convert it to a date time format\n\n\nBeforeCodeAfter\n\n\n\nprisons |> \n  select(NAME, SOURCEDATE) |> \n  head(3)\n\n\n\n  \n\n\n\n\n\n\nlibrary(lubridate)\n\nprisons <- \n  prisons |> \n  mutate(SOURCEDATE = ymd_hms(SOURCEDATE))"
  },
  {
    "objectID": "slides/Day12-Cleaning.html#setting-na-values",
    "href": "slides/Day12-Cleaning.html#setting-na-values",
    "title": "Tidying Datasets",
    "section": "Setting NA values",
    "text": "Setting NA values\n\nna_if() will take a variable and set specified values to NA\n\n\nBeforeCodeAfter\n\n\n\nprisons |> \n  select(NAME, POPULATION) |> \n  head(3)\n\n\n\n  \n\n\nsum(is.na(prisons$POPULATION))\n\n[1] 0\n\n\n\n\n\nprisons <- \n  prisons |> \n  mutate(POPULATION = na_if(POPULATION, -999))\n\n\n\n\nprisons |> \n  select(NAME, POPULATION) |> \n  head(3)\n\n\n\n  \n\n\nsum(is.na(prisons$POPULATION))\n\n[1] 132"
  },
  {
    "objectID": "slides/Day12-Cleaning.html#replacing-strings",
    "href": "slides/Day12-Cleaning.html#replacing-strings",
    "title": "Tidying Datasets",
    "section": "Replacing Strings",
    "text": "Replacing Strings\n\nstr_replace() will take a variable and replace an existing string with a new string\n\n\nBeforeCodeAfter\n\n\n\nprisons |> \n  select(NAME, ADDRESS) |> \n  head(3)\n\n\n\n  \n\n\n\n\n\n\nprisons <- \n  prisons |> \n  mutate(ADDRESS = str_replace(ADDRESS, \n                               \"AVENUE\", \n                               \"AVE\"))"
  },
  {
    "objectID": "slides/Day12-Cleaning.html#removing-strings",
    "href": "slides/Day12-Cleaning.html#removing-strings",
    "title": "Tidying Datasets",
    "section": "Removing Strings",
    "text": "Removing Strings\n\nstr_replace() will take a variable and replace an existing string with a new string\n\n\nBeforeCodeAfter\n\n\n\nprisons |> select(NAME, Creator) |> head(3)\n\n\n\n  \n\n\n\n\n\n\nprisons <- \n  prisons |> \n  mutate(Creator = str_replace(Creator, \n                               \"Hostedby\", \n                               \"\"))"
  },
  {
    "objectID": "slides/Day12-Cleaning.html#conditionals",
    "href": "slides/Day12-Cleaning.html#conditionals",
    "title": "Tidying Datasets",
    "section": "Conditionals",
    "text": "Conditionals\n\ncase_when() allows us to set values when conditions are met\n\n\nBeforeCodeAfter\n\n\n\nprisons |> \n  select(SECURELVL) |> \n  distinct()\n\n\n\n  \n\n\n\n\n\n\nprisons <- \n  prisons |> \n  mutate(JUVENILE = \n           case_when(\n             SECURELVL == \"JUVENILE\" ~ \"Juvenile\",\n             TRUE ~ \"Not Juvenile\"))"
  },
  {
    "objectID": "slides/Day12-Cleaning.html#section",
    "href": "slides/Day12-Cleaning.html#section",
    "title": "Cleaning Datasets",
    "section": ":::",
    "text": ":::"
  },
  {
    "objectID": "slides/Day12-Cleaning.html#what-is-tidy-data",
    "href": "slides/Day12-Cleaning.html#what-is-tidy-data",
    "title": "Tidying Datasets",
    "section": "What is tidy data?",
    "text": "What is tidy data?\n\nEvery observation has its own row.\nEvery variable has its own columns.\nEvery value has its own cell."
  },
  {
    "objectID": "slides/Day12-Cleaning.html#is-this-tidy",
    "href": "slides/Day12-Cleaning.html#is-this-tidy",
    "title": "Tidying Datasets",
    "section": "Is this tidy?",
    "text": "Is this tidy?\n\nWhat variables are displayed on this plot?"
  },
  {
    "objectID": "slides/Day12-Cleaning.html#what-will-it-look-like-when-tidy",
    "href": "slides/Day12-Cleaning.html#what-will-it-look-like-when-tidy",
    "title": "Tidying Datasets",
    "section": "What will it look like when tidy?",
    "text": "What will it look like when tidy?\n\ndf |> pivot_longer(-Date, \n                    names_to = \"City\", \n                    values_to = \"AQI\")"
  },
  {
    "objectID": "slides/Day12-Cleaning.html#pivoting-longer",
    "href": "slides/Day12-Cleaning.html#pivoting-longer",
    "title": "Tidying Datasets",
    "section": "Pivoting Longer",
    "text": "Pivoting Longer\n\nWe use pivot_longer() to pivot a datasets from wider to longer format:\npivot_longer() takes the following arguments:\n\n\ncols =: Identify a series of columns to pivot - The names of those columns will become repeated rows in the pivoted data frame, and the values in those columns will be stored in a new column.\nnames_to =: Identify a name for the column where the column names will be store\nvalues_to =: Identify a name for the column were the values associated with those names will be stored\nVarious arguments to support transformations to names"
  },
  {
    "objectID": "slides/Day12-Cleaning.html#example",
    "href": "slides/Day12-Cleaning.html#example",
    "title": "Tidying Datasets",
    "section": "Example",
    "text": "Example\n\nBeforeCodeAfter\n\n\n\n\n\n\n  \n\n\n\n\n\n\ndf |> pivot_longer(cols = ends_with(\"AQI\"), \n                    names_to = \"City\", \n                    values_to = \"AQI\") |>\n  mutate(City = str_replace(City, \"_AQI\", \"\"))"
  },
  {
    "objectID": "slides/Day12-Cleaning.html#pivoting-wider",
    "href": "slides/Day12-Cleaning.html#pivoting-wider",
    "title": "Tidying Datasets",
    "section": "Pivoting Wider",
    "text": "Pivoting Wider\n\nNote: I use this far less often than pivot_longer()\n\n\nWe use pivot_wider() to pivot a datasets from longer to wider format:\npivot_wider() takes the following arguments:\n\n\nnames_from =: Identify the column to get the new column names from\nvalues_from =: Identify the column to get the cell values from\nVarious arguments to support transformations to names"
  },
  {
    "objectID": "slides/Day12-Cleaning.html#example-1",
    "href": "slides/Day12-Cleaning.html#example-1",
    "title": "Tidying Datasets",
    "section": "Example",
    "text": "Example\n\nBeforeCodeAfter\n\n\n\n\n\n\n  \n\n\n\n\n\n\ndf |> pivot_wider(names_from = \"Date\", \n                   values_from = \"AQI\", \n                   names_repair = make.names)"
  },
  {
    "objectID": "slides/Day12-Cleaning.html#separating-columns",
    "href": "slides/Day12-Cleaning.html#separating-columns",
    "title": "Tidying Datasets",
    "section": "Separating Columns",
    "text": "Separating Columns\n\nWe use separate() to split a column into multiple columns:\nseparate() takes the following arguments:\n\n\ncol: Identify the existing column to separate\ninto = c(): Identify the names of the new columns\nsep =: Identify the characters or numeric position that indicate where to separate columns"
  },
  {
    "objectID": "slides/Day12-Cleaning.html#example-2",
    "href": "slides/Day12-Cleaning.html#example-2",
    "title": "Tidying Datasets",
    "section": "Example",
    "text": "Example\n\nBeforeCodeAfter\n\n\n\n\n\n\n  \n\n\n\n\n\n\ndf |> \n  pivot_longer(\n    cols = everything(), \n    names_to = \"Measure\",\n    values_to = \"AQI\") |>\n  separate(Measure, into = c(\"City\", \"Month\", \"Date\"), sep = \"_\")"
  },
  {
    "objectID": "slides/Day12-Cleaning.html",
    "href": "slides/Day12-Cleaning.html",
    "title": "Tidying Datasets",
    "section": "",
    "text": "Formatting up Columns and Values\nParsing Dates\nConditionals\nPivoting\nSeparating Columns"
  },
  {
    "objectID": "labs/lab8.html#cmss-open-payments-dataset",
    "href": "labs/lab8.html#cmss-open-payments-dataset",
    "title": "Lab 8: Programming with Data",
    "section": "CMS’s Open Payments Dataset",
    "text": "CMS’s Open Payments Dataset\nIn 2010, the the Physician Payments Sunshine Act (2010) was passed, requiring medical drug and device manufacturers to disclose payments and other transfers of value made to physicians, non-physician practitioners, and teaching hospitals. This law was put into place to promote transparency in our medical system - enabling the U.S. government and citizens to monitor for potential medical conflicts of interest.\nToday, every time a drug or medical device manufacturer makes a payment to a covered recipient, they must disclose the nature of that payment and the amount to the U.S. Centers for Medicare & Medicaid. Data about payments is then aggregated, reviewed by (and sometimes disputed by) recipients, corrected, and then published as an open government dataset.\nDefinitions for what counts as a reporting entity, a covered recipient, and a reportable activity have been expanding since the passing of the Physician Payments Sunshine Act as legislators have raised concerns over the degree of transparency of diversifying financial arrangements in the healthcare system. In 2020, the first settlement for violations to the Sunshine Act was announced, requiring Medtronic Inc. to pay $9.2 million to resolve allegations for failure to report. This served as a signal that enforcement is ramping up in the coming years. In 2022, the state of California passed a law requiring that medical practitioners disclose to patients that this data resource exists."
  },
  {
    "objectID": "labs/lab8.html#setting-up-your-environment",
    "href": "labs/lab8.html#setting-up-your-environment",
    "title": "Lab 8: Programming with Data",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nRun the code below to load today’s data frames into your environment.\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(RColorBrewer)\nopen_payments_original <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/main/website/data/open_payments_ma.csv\") |>\n  select(covered_recipient_npi,\n         covered_recipient_first_name:covered_recipient_last_name,\n         applicable_manufacturer_or_applicable_gpo_making_payment_id,\n         applicable_manufacturer_or_applicable_gpo_making_payment_name,\n         recipient_city,\n         recipient_state,\n         covered_recipient_specialty_1,\n         total_amount_of_payment_usdollars,\n         indicate_drug_or_biological_or_device_or_medical_supply_1,\n         product_category_or_therapeutic_area_1,\n         name_of_drug_or_biological_or_device_or_medical_supply_1,\n         date_of_payment,\n         nature_of_payment_or_transfer_of_value,\n         number_of_payments_included_in_total_amount,\n         form_of_payment_or_transfer_of_value,\n         dispute_status_for_publication,\n         payment_publication_date) |>\n  filter(!is.na(covered_recipient_npi))"
  },
  {
    "objectID": "labs/lab8.html#cleaning-up-this-data-frame",
    "href": "labs/lab8.html#cleaning-up-this-data-frame",
    "title": "Lab 8: Programming with Data",
    "section": "Cleaning up this Data Frame",
    "text": "Cleaning up this Data Frame\nEventually we are going to plot some timelines of payments to specific doctors, and we will need the date_of_payment column to be in a date-time format to do so. Right now however, these columns are strings. To get started with cleaning up this dataset, let’s convert the date columns in open_payments_original to a date-time format.\n\nQuestion\n\nWrite code to convert the date_of_payment and the payment_publication_date column to date-time format. You should first determine the format of the date in date_of_payment and payment_publication_date and then reference the lubridate to determine the corresponding function for parsing that date format. Finally, you will mutate the two columns.\n\nOptional challenge: Rather than mutating each column, see if you can mutate() across() the two columns to complete this step.\n\n\n\n# Uncomment below to write code to convert to date-time format here. \n\n# open_payments_dates_cleaned <- open_payments_original |>\n\n\n\n\n\n\nTo confirm that we’ve done this right, we can check whether both of the dates are in the date-time format.\n\n\n\nQuestion\n\nThe is.Date function returns TRUE if a vector is in date-time format, and FALSE if it is not. Below, I’ve selected the two columns in open_payments that contain the word “date” in the column header. Determine which map() function to use in order to return a vector that indicates whether these columns are in a date-time format. If you’ve done everything correctly, you should get the output below.\n\n\n# Select the appropriate map function below\n\nopen_payments_dates_cleaned |>\n  select(contains(\"date\")) |>\n  _____(is.Date)\n\n\n\n         date_of_payment payment_publication_date \n                    TRUE                     TRUE \n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nCheck out what happens if you swap out your map function above for map, map_chr, or map_int. Can you figure out the relationship between these values and the original values?\n\n\nIt’s important to note that the unit of observation in this dataset is not one medical practitioner, and it is not one manufacturer. Instead it is one payment from a manufacturer to a medical practitioner. That means that a medical practitioner can appear multiple times in the dataset if they’ve received multiple payments, and a medical drug or device manufacturer can appear multiple times in the dataset if they’ve disbursed multiple payments. We can identify medical practitioners with the covered_recipient_npi column and manufacturers with the applicable_manufacturer_or_applicable_gpo_making_payment_id column.\n…but we want to know more than just the ID of a medical practitioner. To identify which doctors are receiving the most money, we also want to know that practitioner’s name, location, specialty, etc. Because practitioners’ names are manually entered into a database every time a payment is made to them, sometimes the formatting of a practitioner’s name entered for one payment can differ from how that same practitioner’s name is formatted when entered for another payment. The same goes for other variables related to that practitioner. For instance, check out how the capitalization differs for the practitioner below. In some cases, there is a middle initial, while in others, there is a full middle name; in some cases, the practitioner’s name is all capitalized, and in other cases it is not.\n\nopen_payments_dates_cleaned |>\n  filter(covered_recipient_npi == 1003040676) |>\n  select(covered_recipient_first_name, covered_recipient_middle_name, covered_recipient_last_name)\n\n\n\n  \n\n\n\nThis issue with formatting exists across this entire dataset. To ensure that similar entities appear in the right buckets when we aggregate the data, we are going to standardize capitalization across the whole dataset. We’re also going to leave out the practitioner’s middle initial since it is not always included (or included in the same way).\n\n\n\nQuestion\n\nWrite code to mutate across all character columns such that strings in these columns are converted to title case. Title case refers to casing where the first letter in each word is capitalized and all other letters are lowercase. Strings can be converted to title case with the function str_to_title.\nAfter you’ve done this, mutate a new column called covered_recipient_full_name that concatenates (hint: i.e. paste()) together covered_recipient_first_name and covered_recipient_last_name.\nStore the resulting data frame in open_payments.\n\n\n# Uncomment below to clean up strings\n\n# open_payments <- open_payments_dates_cleaned |>\n\n\n\n\n\n\nAs we saw before, one covered_recipient_npi was associated with multiple names if the names were capitalized in some cases and not others. Now that we’ve standardized the formatting of these names, there ideally should be one full name associated with every covered_recipient_npi. Let’s compare the length of unique covered_recipient_npi values to the length of unique covered_recipient_full_name values to check whether this is the case.\n\n\n\nQuestion\n\nWrite a function below called num_unique. The function should calculate the length of unique values in the vector passed to the argument x.\nBelow, I’ve selected the two columns in open_payments that we want to iterate this function over. Determine which map() function to use in order to return a numeric vector that indicates the length of unique values in each of these columns. If you’ve done everything correctly, you should get the output below.\n\n\nnum_unique <- function(x) {\n # Write function here.\n}\n\nopen_payments |>\n  select(covered_recipient_npi, covered_recipient_full_name) |>\n  _____(_____) # Determine which map function to call here.\n\n\n\n      covered_recipient_npi covered_recipient_full_name \n                      11837                       11858 \n\n\n\n\nNotice that there are still more full names than covered_recipient_npis, which means that certain doctors have multiple names in this dataset. Below I’ve written some code to calculate the number unique full names listed for each covered_recipient_npi and filter to the rows with more than one name. Can you identify some reasons why we might have multiple names listed for this same medical practitioner in this data frame?\n\nopen_payments |>\n  group_by(covered_recipient_npi) |>\n  mutate(num_names = length(unique(covered_recipient_full_name))) |>\n  ungroup() |>\n  filter(num_names > 1) |>\n  select(covered_recipient_npi, covered_recipient_full_name) |>\n  distinct() |>\n  arrange(desc(covered_recipient_npi))\n\n\n\n  \n\n\n\nBecause of these issues, it is important that we use the covered_recipient_npi to identify doctors vs. the full name.\nNow that we have a final cleaned up open_payments data frame, let’s remove the other data frames from our environment.\n\nrm(open_payments_original, open_payments_dates_cleaned)\n\n…and on to analysis."
  },
  {
    "objectID": "labs/lab8.html#which-doctors-received-the-most-money-from-medical-drug-and-device-companies-in-2021",
    "href": "labs/lab8.html#which-doctors-received-the-most-money-from-medical-drug-and-device-companies-in-2021",
    "title": "Lab 8: Programming with Data",
    "section": "Which doctors received the most money from medical drug and device companies in 2021?",
    "text": "Which doctors received the most money from medical drug and device companies in 2021?\nUltimately, our aim is to produce a number of tables and plots for each of the ten MA-based doctors that received the most money from medical drug and device companies in 2021. This means that one of our first analysis steps is to identify those 10 doctors.\n\nWrite code to determine the 10 medical practitioners that received the most money from drug and device companies in 2021, and store your results in top_10_doctors. Your final data frame should have 10 rows and columns for covered_recipient_npi, covered_recipient_full_name, covered_recipient_specialty_1, recipient_city, sum_total_payments.\n\n\ntop_10_doctors <-\n  open_payments |>\n  group_by(covered_recipient_npi, \n           covered_recipient_full_name, \n           covered_recipient_specialty_1, \n           recipient_city) |>\n  summarize(sum_total_payments = sum(total_amount_of_payment_usdollars)) |>\n  arrange(desc(sum_total_payments)) |>\n  head(10)\n\nRight now the values that we will eventually want to iterate over in our analysis (such as the doctors’ id or specialty) are stored as columns in a dataframe. …but remember that the family of purrr functions allows us to apply a function to each element of a vector or list. We want to create a series of vectors from these columns that we can iterate over. We will use the pull() function to do this.\n\nCreate four vectors from top_10_doctors: top_10_doctors_ids, top_10_doctors_name, top_10_doctors_specialties, top_10_doctors_cities. I’ve completed the first one for you.\n\n\ntop_10_doctors_ids <-\n  top_10_doctors |>\n  pull(covered_recipient_npi)\n\ntop_10_doctors_names <-\n  top_10_doctors |>\n  pull(covered_recipient_full_name)\n\ntop_10_doctors_specialties <-\n  top_10_doctors |>\n  pull(covered_recipient_specialty_1)\n\ntop_10_doctors_cities <-\n  top_10_doctors |>\n  pull(recipient_city)\n\nNow that we have the vectors we want to iterate over, we are ready to start defining our first functions. To get started, let’s define a function that filters open_payments to a given doctor ID, and then calculates how much of each kind of payment has been paid to that doctor. Here is an example of what that data wrangling code would look like for a specific covered_recipient_npi:\n\nopen_payments |>\n  filter(covered_recipient_npi == 1194763482) |>\n  group_by(covered_recipient_full_name, \n           nature_of_payment_or_transfer_of_value) |>\n  summarize(num_payments = sum(number_of_payments_included_in_total_amount),\n            total_payments = sum(total_amount_of_payment_usdollars))\n\n\n\n  \n\n\n\n\nWrap the above code in a function named calculate_payment_type_amts. Rather than filtering to 1194763482, filter based on the value passed to an argument named doctor_id. Finally, use the map() function to apply calculate_payment_type_amts to each element in the top_10_doctors_ids vector. Running this code should return 10 data frames.\n\n\ncalculate_payment_type_amts <- function(doctor_id){\n  \n  open_payments |>\n    filter(covered_recipient_npi == doctor_id) |> \n    group_by(covered_recipient_full_name, \n             nature_of_payment_or_transfer_of_value) |>\n    summarize(num_payments = sum(number_of_payments_included_in_total_amount),\n              total_payments = sum(total_amount_of_payment_usdollars))\n  \n}\n\nmap_df(top_10_doctors_ids, calculate_payment_type_amts)\n\n\n\n  \n\n\n\nLet’s take a similar approach to define a function that filters open_payments to a given doctor ID and determines how much doctors have been paid in relation to specific products they may be asked to endorse. To do so, we will need to aggregate the data by name_of_drug_or_biological_or_device_or_medical_supply_1 and calculate the total payments associated with the mention of that product.\n\nWrite a function named calculate_drugs_and_devices_payments. The function should take a doctor_id, and filter open_payments to that ID. Then it should aggregate the filtered data by covered_recipient_full_name, name_of_drug_or_biological_or_device_or_medical_supply_1, and indicate_drug_or_biological_or_device_or_medical_supply_1, calculate the total amount of payments, and sort the resulting data frame in descending order by the total amount of payments. Finally, use the map() function to apply calculate_drugs_and_devices_payments to each element in the top_10_doctors_ids vector. Running this code should return 10 data frames.\n\n\ncalculate_drugs_and_devices_payments <- function(doctor_id){\n  \n  open_payments |>\n    filter(covered_recipient_npi == doctor_id) |> \n    group_by(covered_recipient_full_name, \n             name_of_drug_or_biological_or_device_or_medical_supply_1,\n             indicate_drug_or_biological_or_device_or_medical_supply_1) |>\n    summarize(total_amount_of_payment_usdollars = sum(total_amount_of_payment_usdollars)) |>\n    arrange(desc(total_amount_of_payment_usdollars))\n  \n}\n\nmap(top_10_doctors_ids, calculate_drugs_and_devices_payments)\n\n[[1]]\n# A tibble: 1 × 4\n# Groups:   covered_recipient_full_name,\n#   name_of_drug_or_biological_or_device_or_medical_supply_1 [1]\n  covered_recipient_full_name name_of_drug_or_biological_or_de…¹ indic…² total…³\n  <chr>                       <chr>                              <chr>     <dbl>\n1 David Friedman              <NA>                               <NA>     1.88e7\n# … with abbreviated variable names\n#   ¹​name_of_drug_or_biological_or_device_or_medical_supply_1,\n#   ²​indicate_drug_or_biological_or_device_or_medical_supply_1,\n#   ³​total_amount_of_payment_usdollars\n\n[[2]]\n# A tibble: 1 × 4\n# Groups:   covered_recipient_full_name,\n#   name_of_drug_or_biological_or_device_or_medical_supply_1 [1]\n  covered_recipient_full_name name_of_drug_or_biological_or_de…¹ indic…² total…³\n  <chr>                       <chr>                              <chr>     <dbl>\n1 Martin Pollak               <NA>                               <NA>     1.88e7\n# … with abbreviated variable names\n#   ¹​name_of_drug_or_biological_or_device_or_medical_supply_1,\n#   ²​indicate_drug_or_biological_or_device_or_medical_supply_1,\n#   ³​total_amount_of_payment_usdollars\n\n[[3]]\n# A tibble: 6 × 4\n# Groups:   covered_recipient_full_name,\n#   name_of_drug_or_biological_or_device_or_medical_supply_1 [6]\n  covered_recipient_full_name name_of_drug_or_biological_or_de…¹ indic…² total…³\n  <chr>                       <chr>                              <chr>     <dbl>\n1 Paul Tornetta               Evos                               Device   1.69e6\n2 Paul Tornetta               Trigen Femoral (Fan/Tan/Meta Nail) Device   3.73e5\n3 Paul Tornetta               Vlp Mini Mod                       Device   2.36e4\n4 Paul Tornetta               D-Rad Smart Pak                    Device   1.78e4\n5 Paul Tornetta               Peri-Loc                           Device   1.08e4\n6 Paul Tornetta               <NA>                               <NA>     3.57e2\n# … with abbreviated variable names\n#   ¹​name_of_drug_or_biological_or_device_or_medical_supply_1,\n#   ²​indicate_drug_or_biological_or_device_or_medical_supply_1,\n#   ³​total_amount_of_payment_usdollars\n\n[[4]]\n# A tibble: 1 × 4\n# Groups:   covered_recipient_full_name,\n#   name_of_drug_or_biological_or_device_or_medical_supply_1 [1]\n  covered_recipient_full_name name_of_drug_or_biological_or_de…¹ indic…² total…³\n  <chr>                       <chr>                              <chr>     <dbl>\n1 Thomas Thornhill            Attune                             Device   1.87e6\n# … with abbreviated variable names\n#   ¹​name_of_drug_or_biological_or_device_or_medical_supply_1,\n#   ²​indicate_drug_or_biological_or_device_or_medical_supply_1,\n#   ³​total_amount_of_payment_usdollars\n\n[[5]]\n# A tibble: 1 × 4\n# Groups:   covered_recipient_full_name,\n#   name_of_drug_or_biological_or_device_or_medical_supply_1 [1]\n  covered_recipient_full_name name_of_drug_or_biological_or_de…¹ indic…² total…³\n  <chr>                       <chr>                              <chr>     <dbl>\n1 Laurie Glimcher             <NA>                               <NA>    971882.\n# … with abbreviated variable names\n#   ¹​name_of_drug_or_biological_or_device_or_medical_supply_1,\n#   ²​indicate_drug_or_biological_or_device_or_medical_supply_1,\n#   ³​total_amount_of_payment_usdollars\n\n[[6]]\n# A tibble: 3 × 4\n# Groups:   covered_recipient_full_name,\n#   name_of_drug_or_biological_or_device_or_medical_supply_1 [3]\n  covered_recipient_full_name name_of_drug_or_biological_or_de…¹ indic…² total…³\n  <chr>                       <chr>                              <chr>     <dbl>\n1 Ali Nasseh                  <NA>                               <NA>     9.71e5\n2 Ali Nasseh                  Channels Obturator                 Device   1.08e2\n3 Ali Nasseh                  Cao Group Inc Precise Ltm Complet… Device   9.68e1\n# … with abbreviated variable names\n#   ¹​name_of_drug_or_biological_or_device_or_medical_supply_1,\n#   ²​indicate_drug_or_biological_or_device_or_medical_supply_1,\n#   ³​total_amount_of_payment_usdollars\n\n[[7]]\n# A tibble: 1 × 4\n# Groups:   covered_recipient_full_name,\n#   name_of_drug_or_biological_or_device_or_medical_supply_1 [1]\n  covered_recipient_full_name name_of_drug_or_biological_or_de…¹ indic…² total…³\n  <chr>                       <chr>                              <chr>     <dbl>\n1 Alan Garber                 <NA>                               <NA>    954938.\n# … with abbreviated variable names\n#   ¹​name_of_drug_or_biological_or_device_or_medical_supply_1,\n#   ²​indicate_drug_or_biological_or_device_or_medical_supply_1,\n#   ³​total_amount_of_payment_usdollars\n\n[[8]]\n# A tibble: 5 × 4\n# Groups:   covered_recipient_full_name,\n#   name_of_drug_or_biological_or_device_or_medical_supply_1 [5]\n  covered_recipient_full_name name_of_drug_or_biological_or_de…¹ indic…² total…³\n  <chr>                       <chr>                              <chr>     <dbl>\n1 James Bono                  Accolade                           Device  810243.\n2 James Bono                  Mako                               Device    5319.\n3 James Bono                  Triathlon                          Device    2500 \n4 James Bono                  <NA>                               <NA>      1415.\n5 James Bono                  Surpass Evolve                     Device     999.\n# … with abbreviated variable names\n#   ¹​name_of_drug_or_biological_or_device_or_medical_supply_1,\n#   ²​indicate_drug_or_biological_or_device_or_medical_supply_1,\n#   ³​total_amount_of_payment_usdollars\n\n[[9]]\n# A tibble: 4 × 4\n# Groups:   covered_recipient_full_name,\n#   name_of_drug_or_biological_or_device_or_medical_supply_1 [4]\n  covered_recipient_full_name name_of_drug_or_biological_or_de…¹ indic…² total…³\n  <chr>                       <chr>                              <chr>     <dbl>\n1 Stephen Murphy              Mpo Hip System                     Device  801074.\n2 Stephen Murphy              <NA>                               <NA>      1725 \n3 Stephen Murphy              Endurance                          Device     120 \n4 Stephen Murphy              Hips-None                          Device     112.\n# … with abbreviated variable names\n#   ¹​name_of_drug_or_biological_or_device_or_medical_supply_1,\n#   ²​indicate_drug_or_biological_or_device_or_medical_supply_1,\n#   ³​total_amount_of_payment_usdollars\n\n[[10]]\n# A tibble: 14 × 4\n# Groups:   covered_recipient_full_name,\n#   name_of_drug_or_biological_or_device_or_medical_supply_1 [14]\n   covered_recipient_full_name name_of_drug_or_biological_or_d…¹ indic…² total…³\n   <chr>                       <chr>                             <chr>     <dbl>\n 1 Michael Kaminer             <NA>                              <NA>     7.71e5\n 2 Michael Kaminer             Cosentyx                          Biolog…  1.23e2\n 3 Michael Kaminer             Cimzia                            Drug     9.51e1\n 4 Michael Kaminer             Humira                            Biolog…  7.59e1\n 5 Michael Kaminer             Enstilar                          Drug     6.77e1\n 6 Michael Kaminer             Dupixent                          Biolog…  5.64e1\n 7 Michael Kaminer             Taltz                             Drug     5.06e1\n 8 Michael Kaminer             Skyrizi                           Biolog…  4.88e1\n 9 Michael Kaminer             Ilumya                            Biolog…  4.71e1\n10 Michael Kaminer             Otezla                            Drug     4.08e1\n11 Michael Kaminer             Duobrii                           Drug     3.56e1\n12 Michael Kaminer             Tremfya                           Drug     2.77e1\n13 Michael Kaminer             Arazlo                            Drug     1.96e1\n14 Michael Kaminer             Enbrel                            Biolog…  1.79e1\n# … with abbreviated variable names\n#   ¹​name_of_drug_or_biological_or_device_or_medical_supply_1,\n#   ²​indicate_drug_or_biological_or_device_or_medical_supply_1,\n#   ³​total_amount_of_payment_usdollars\n\n\n\nmap_df(top_10_doctors_ids, calculate_drugs_and_devices_payments) |>\n  group_by(covered_recipient_full_name) |>\n  summarize(Count_Drugs = n())\n\n\n\n  \n\n\n\n\ndoctor_summary <- function(doctor_id){\n  open_payments |>\n    filter(covered_recipient_npi == doctor_id) |>\n    group_by(covered_recipient_npi, \n             covered_recipient_full_name) |>\n    summarize(num_payments = sum(number_of_payments_included_in_total_amount),\n              payment_total = sum(total_amount_of_payment_usdollars),\n              num_companies = length(unique(applicable_manufacturer_or_applicable_gpo_making_payment_id)))\n}\n\nmap_df(top_10_doctors_ids, doctor_summary)\n\n\n\n  \n\n\n\n\npayment_calendar <- function(doctor_id, doctor_name, doctor_specialty){\n  open_payments |>\n    filter(covered_recipient_npi == doctor_id) |>\n    ggplot(aes(x = day(date_of_payment), y = \"\", col = total_amount_of_payment_usdollars)) +\n    geom_jitter() +\n    theme_minimal() +\n    labs(title = doctor_name, \n         subtitle = doctor_specialty,\n         y = \"\", \n         x = \"Day\",\n         col = \"Payment Amount\") +\n    scale_y_discrete(limits = rev) +\n    scale_color_distiller(palette = \"BuPu\", direction = 1, labels = scales::comma) +\n    facet_wrap(vars(month(date_of_payment, label = TRUE)), nrow = 4) \n}\n\npmap(list(top_10_doctors_ids, top_10_doctors_names, top_10_doctors_specialties), payment_calendar)\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n[[6]]\n\n\n\n\n\n\n[[7]]\n\n\n\n\n\n\n[[8]]\n\n\n\n\n\n\n[[9]]\n\n\n\n\n\n\n[[10]]\n\n\n\n\n\n\n# Unused code\n# top_20_companies <-\n#   open_payments |>\n#   group_by(applicable_manufacturer_or_applicable_gpo_making_payment_name) |>\n#   summarize(total_payments = sum(total_amount_of_payment_usdollars)) |>\n#   arrange(desc(total_payments)) |>\n#   head(20) |>\n#   select(applicable_manufacturer_or_applicable_gpo_making_payment_name) |>\n#   pull()\n# \n# total_doctors_receiving_payment <- function(company){\n#   num_payments <-  \n#     open_payments |>\n#     filter(applicable_manufacturer_or_applicable_gpo_making_payment_name == company) |>\n#     select(covered_recipient_npi) |>\n#     n_distinct()\n#   return(num_payments)\n# }\n# \n# drugs_and_devices <- function(company){\n#   open_payments |>\n#     filter(applicable_manufacturer_or_applicable_gpo_making_payment_name == company) |>\n#     group_by(name_of_drug_or_biological_or_device_or_medical_supply_1) |>\n#     summarize(num_payments = n(),\n#               total_amount_of_payment_usdollars = sum(total_amount_of_payment_usdollars)) |>\n#     arrange(desc(total_amount_of_payment_usdollars)) |>\n#     head(10)\n# }\n# \n# doctors_receiving_payment <- function(company){\n#   open_payments |>\n#     filter(applicable_manufacturer_or_applicable_gpo_making_payment_name == company) |>\n#     mutate(covered_recipient_first_name = toupper(covered_recipient_first_name),\n#            covered_recipient_last_name = toupper(covered_recipient_last_name)) |>\n#     group_by(covered_recipient_npi, covered_recipient_first_name, covered_recipient_last_name, recipient_city) |>\n#     summarize(total_amount_of_payment_usdollars = sum(total_amount_of_payment_usdollars)) |>\n#     arrange(desc(total_amount_of_payment_usdollars)) |>\n#     head(10)\n# }\n# \n# set_names(top_20_companies) |> map_int(total_doctors_receiving_payment)\n# set_names(top_20_companies) |> map(drugs_and_devices)  \n# set_names(top_20_companies) |> map(doctors_receiving_payment)"
  },
  {
    "objectID": "slides/Day13-Functions.html#for-today",
    "href": "slides/Day13-Functions.html#for-today",
    "title": "Functions",
    "section": "For Today",
    "text": "For Today\n\nQuiz 2 Posted!\nDon’t forget about the Study Plan Assessment\nWriting Functions\nCode Along"
  },
  {
    "objectID": "slides/Day13-Functions.html#why-write-functions",
    "href": "slides/Day13-Functions.html#why-write-functions",
    "title": "Functions",
    "section": "Why write functions?",
    "text": "Why write functions?\n\n\n\n\npit %>%\n  filter(State == \"AK\" & Year == 2015) |>\n  summarize(Total = sum(Count))\n\n\n\n  \n\n\npit %>%\n  filter(State == \"AL\" & Year == 2015) |>\n  summarize(Total = sum(Count))\n\n\n\n  \n\n\npit %>%\n  filter(State == \"AR\" & Year == 2015) |>\n  summarize(Total = sum(Count))"
  },
  {
    "objectID": "slides/Day13-Functions.html#why-write-functions-1",
    "href": "slides/Day13-Functions.html#why-write-functions-1",
    "title": "Functions",
    "section": "Why write functions?",
    "text": "Why write functions?\n\nReduces the amount of code to write\nLowers the chances of errors in code\nSupports reproducibility"
  },
  {
    "objectID": "slides/Day13-Functions.html#functions",
    "href": "slides/Day13-Functions.html#functions",
    "title": "Functions",
    "section": "Functions",
    "text": "Functions\n\nStatements organized to perform a specific task\nTake arguments as inputs\n\nArguments can be required or optional\n\nReturn a value or set of values as outputs\n\nDefault value is output of last line of function if not specified"
  },
  {
    "objectID": "slides/Day13-Functions.html#calling-functions",
    "href": "slides/Day13-Functions.html#calling-functions",
    "title": "Functions",
    "section": "Calling Functions",
    "text": "Calling Functions\n\nThis is the same way you’d call functions built-in to R\n\n\nBasic formatExample\n\n\n\nfunction_name(value_for_arg1, value_for_arg2)\n\n\n\n\ncalculate_difference(7, 3)\n\n[1] 4"
  },
  {
    "objectID": "slides/Day13-Functions.html#user-defined-functions-1",
    "href": "slides/Day13-Functions.html#user-defined-functions-1",
    "title": "Functions",
    "section": "User-defined Functions",
    "text": "User-defined Functions\n\nstate_name is a required argument here.\n\n\ncalculate_state_total <- function(state_name) {\n  \n  x <- pit %>%\n    filter(State == state_name & Year == 2015) |>\n    summarize(Total = sum(Count))\n  \n  return(x)\n}\n\n\ncalculate_state_total(\"AL\")"
  },
  {
    "objectID": "slides/Day13-Functions.html#making-arguments-optional",
    "href": "slides/Day13-Functions.html#making-arguments-optional",
    "title": "Functions",
    "section": "Making Arguments Optional",
    "text": "Making Arguments Optional\n\nBecause we provide a deafult value for year, it is optional in the function call.\n\n\nSetting Variable DefaultsOverriding Variable Defaults\n\n\n\ncalculate_state_total <- function(state_name, year = 2020) {\n  \n  x <- pit %>%\n    filter(State == state_name & Year == year) |>\n    summarize(Total = sum(Count))\n  \n  return(x)\n}\n\n\ncalculate_state_total(\"AL\")\n\n\n\n  \n\n\n\n\n\n\ncalculate_state_total <- function(state_name, year = 2020) {\n  \n  x <- pit %>%\n    filter(State == state_name & Year == year) |>\n    summarize(Total = sum(Count))\n  \n  return(x)\n}\n\n\ncalculate_state_total(\"AL\", 2019)"
  },
  {
    "objectID": "slides/Day13-Functions.html#naming-arguments",
    "href": "slides/Day13-Functions.html#naming-arguments",
    "title": "Functions",
    "section": "Naming Arguments",
    "text": "Naming Arguments\n\nNaming helps to differentiate between arguments. Order matters if arguments aren’t named!\n\n\ncalculate_state_total <- function(state_name, year = 2020) {\n  \n  x <- pit %>%\n    filter(State == state_name & Year == year) |>\n    summarize(Total = sum(Count))\n  \n  return(x)\n}\n\n\ncalculate_state_total(state_name = \"AL\", year = 2019)"
  },
  {
    "objectID": "labs/lab8.html#data-analysis",
    "href": "labs/lab8.html#data-analysis",
    "title": "Lab 8: Programming with Data",
    "section": "Data Analysis",
    "text": "Data Analysis\nUltimately, our aim is to produce a number of tables and plots for each of the ten MA-based doctors that received the most money from medical drug and device manufacturers in 2021. This means that one of our first analysis steps is to identify those 10 medical practitioners.\n\nQuestion\n\nWrite code to determine the 10 medical practitioners that received the most money from drug and device manufacturers in 2021, and store your results in top_10_doctors. Your final data frame should have 10 rows and columns for covered_recipient_npi and sum_total_payments.\n\n\n# Uncomment below and write data wrangling code\n\n#top_10_doctors <- open_payments |>\n\n\n\n\n\n\nRight now the values that we will eventually want to iterate over in our analysis are stored as columns in a dataframe. …but remember that the family of purrr functions allows us to apply a function to each element of a vector or list. We want to create a series of vectors from these columns that we can iterate over. We will use the pull() function to do this.\n\n\n\nQuestion\n\nCreate a vector of top_10_doctors_ids from top_10_doctors, using the pull() function.\n\n\n# Uncomment and write code below to pull the top 10 doctor IDs into a vector\n\n# top_10_doctors_ids <- top_10_doctors |>\n\n\n\n\n\n\nWe also want a vector of doctor names associated with each of these IDs, but remember that there can be multiple names for a single doctor in this dataset. With this in mind, we are going to create a vector of the first listed name for a given covered_recipient_npi in the dataset. Taking the first listed name as the doctor’s name is an imperfect solution. The first listed name could be a misspelling. It could be a doctor’s maiden name that they have since changed. This is a temporary solution, and we would want to confirm that we have the correct name for each doctor before publishing any of these findings.\n\n\n\nQuestion\n\nCreate a vector containing the names of the doctors associated with the IDs in top_10_doctors_ids. First, define the function get_doctor_name. This function will:\n\ntake a doctor_id as an argument,\nfilter open_payments to that ID,\nsummarize the first() covered_recipient_full_name listed for that ID,\npull() the name value\n\nOnce this function has been defined, select the appropriate map() function to iterate top_10_doctors_ids through get_doctor_name and store the resulting character vector in top_10_doctors_names.\n\n\nget_doctor_name <- function(doctor_id){\n  # Write function code here\n}\n\n# Iterate the top_10_doctors_ids vector through get_doctor_name and store the results in a character vector\n\n# top_10_doctors_names <- \n\n\n\n\n\n\n\nNow that we have the vectors we want to iterate over, we are ready to start defining our first functions.\n\n\nWhat kind of payments did MA-based doctors receive in 2021?\nTo get started, let’s define a function that filters open_payments to a given doctor ID, and then calculates how much of each kind of payment has been paid to that doctor. Here is an example of what that data wrangling code would look like for a specific covered_recipient_npi:\n\nopen_payments |>\n  filter(covered_recipient_npi == 1194763482) |>\n  group_by(nature_of_payment_or_transfer_of_value) |>\n  summarize(num_payments = \n              sum(number_of_payments_included_in_total_amount),\n            total_payments = sum(total_amount_of_payment_usdollars))\n\n\n\n  \n\n\n\n\nQuestion\n\nWrap the above code in a function named calculate_payment_type_amts. Rather than filtering to 1194763482, filter based on the value passed to an argument named doctor_id.\nThen, use the map() function to apply calculate_payment_type_amts to each element in the top_10_doctors_ids vector. Running this code should return a list of 10 data frames.\nFinally, pipe in set_names(top_10_doctors_names) to set the names for each data frame in the list to the doctor’s name.\n\n\n# Write calculate_payment_type_amts function here\n\n# Iterate calculate_payment_type_amts over top_10_doctors_ids and set names to top_10_doctors_names\n\n\n\n\n\n\n\n\n\n\nWhen were payments made to each of these doctors in 2021?\nHere’s an example of a plot we could create to answer this question for one doctor.\n\n  open_payments |>\n    filter(covered_recipient_npi == 1194763482) |>\n    ggplot(aes(x = day(date_of_payment), \n               y = \"\", \n               fill = total_amount_of_payment_usdollars)) +\n    geom_jitter(pch = 21, size = 2, color = \"black\") +\n    theme_minimal() +\n    labs(title = \"David Friedman\", \n         y = \"\", \n         x = \"Day\",\n         fill = \"Payment Amount\") +\n    scale_y_discrete(limits = rev) +\n    scale_fill_distiller(palette = \"BuPu\", direction = 1, labels = scales::comma) +\n    facet_wrap(vars(month(date_of_payment, label = TRUE)), nrow = 4) \n\n\n\n\n\nQuestion\n\nWrite a function named payments_calendar. The function should:\n\nTake a doctor_id and doctor_name as arguments\nFilter open_payments to the doctor’s ID\nCreate payment calendar plot modeled after the one above.\nSet the title of the plot to the doctor’s name\n\nAfter you’ve written this function, select the appropriate map function to apply payments_calendar to each element in the top_10_doctors_ids vector and top_10_doctors_names vector.\n\nOptional Challenge: Extend your code to include the first listed specialty for each top 10 doctor as a subtitle in each plot.\n\n\n\n# Write payments_calendar function here\n\n# Iterate payments_calendar over top_10_doctors_ids and top_10_doctors_ids to create 10 plots\n\n\n\n\n\n\n\n\n\n\nWhich manufacturers paid MA-based doctors in 2021, and through what forms of payment?\nFinally, let’s define a function that filters open_payments to a given doctor ID and determines how much the doctor received in compensation from different manufacturers, along with the forms of payment from each manufacturer. To do so, we will need to aggregate the data by covered_recipient_npi, applicable_manufacturer_or_applicable_gpo_making_payment_name, and form_of_payment_or_transfer_of_value and calculate the total payments associated with each grouping.\n\nQuestion\n\nWrite a function named calculate_manufacturer_payments. The function should:\n\nTake a doctor_id as an argument\nFilter open_payments to that ID\nAggregate the filtered data by covered_recipient_npi, applicable_manufacturer_or_applicable_gpo_making_payment_name, and form_of_payment_or_transfer_of_value\nCalculate the total amount of payments for each grouping\nSort the resulting data frame in descending order by the total amount of payments.\n\nAfter you’ve written this function, use the map_df() function to apply calculate_manufacturer_payments to each element in the top_10_doctors_ids vector. Note how this returns one data frame rather than a list of 10 data frames.\nPlot your resulting data frame as a column plot, attempting (to the best of your ability) to match the formatting of the plot below.\n\nOptional Challenge: List the doctor’s full name in each facet band, rather than the the doctor’s ID.\n\n\n\n# Write calculate_manufacturer_payments function here\n\n# Iterate calculate_manufacturer_payments over top_10_doctors_ids here\n\n# Plot resulting data frame here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nOn September 29, 2022, California Governor Gavin Newsom signed a bill requiring that all physicians and surgeons notify patients about the Open Payments database on their initial visit. Specifically, patients will be given the following notice: “The Open Payments database is a federal tool used to search payments made by drug and device companies to physicians and teaching hospitals. It can be found at https://openpaymentsdata.cms.gov,” and will be prompted to sign and date that they have received the notice. This policy was developed in response to concerns that, while the Open Payments database includes a great deal of information that might impact how citizens make decisions about their healthcare, very few people knew of the database. Do you think this is a good solution to this problem? Should other states follow suit? Do you see any drawbacks to this policy? Share your ideas on our sds-192-discussions Slack channel."
  },
  {
    "objectID": "templates/lab8.html",
    "href": "templates/lab8.html",
    "title": "Lab 8: Programming with Data",
    "section": "",
    "text": "Question\n\nThe is.Date function returns TRUE if a vector is in date-time format, and FALSE if it is not. Below, I’ve selected the two columns in open_payments that contain the word “date” in the column header. Determine which map() function to use in order to return a vector that indicates whether these columns are in a date-time format. If you’ve done everything correctly, you should get the output below.\n\n\n# Select the appropriate map function below\n\nopen_payments_dates_cleaned |>\n  select(contains(\"date\")) |>\n  _____(is.Date)\n\n\n\nQuestion\n\nWrite code to mutate across all character columns such that strings in these columns are converted to title case. Title case refers to casing where the first letter in each word is capitalized and all other letters are lowercase. Strings can be converted to title case with the function str_to_title.\nAfter you’ve done this, mutate a new column called covered_recipient_full_name that concatenates (hint: i.e. paste()) together covered_recipient_first_name and covered_recipient_last_name.\nStore the resulting data frame in open_payments_cleaned.\n\n\n# Uncomment below to clean up strings\n\n# open_payments <- open_payments_dates_cleaned |>\n\n\n\nQuestion\n\nWrite a function below called num_unique. The function should calculate the length of unique values in the vector passed to the argument x.\nBelow, I’ve selected the two columns in open_payments that we want to iterate this function over. Determine which map() function to use in order to return a numeric vector that indicates the length of unique values in each of these columns. If you’ve done everything correctly, you should get the output below.\n\n\nnum_unique <- function(x) {\n # Write function here.\n}\n\nopen_payments |>\n  select(covered_recipient_npi, covered_recipient_full_name) |>\n  _____(_____) # Determine which map function to call here.\n\n\nrm(open_payments_original, open_payments_dates_cleaned)\n\n\n\nQuestion\n\nWrite code to determine the 10 medical practitioners that received the most money from drug and device manufacturers in 2021, and store your results in top_10_doctors. Your final data frame should have 10 rows and columns for covered_recipient_npi and sum_total_payments.\n\n\n# Uncomment below and write data wrangling code\n\n#top_10_doctors <- open_payments |>\n\n\n\nQuestion\n\nCreate a vector of top_10_doctors_ids from top_10_doctors, using the pull() function.\n\n\n# Uncomment and write code below to pull the top 10 doctor IDs into a vector\n\n# top_10_doctors_ids <- top_10_doctors |>\n\n\n\nQuestion\n\nCreate a vector containing the names of the doctors associated with the IDs in top_10_doctors_ids. First, define the function get_doctor_name. This function will:\n\ntake a doctor_id as an argument,\nfilter open_payments to that ID,\nsummarize the first() covered_recipient_full_name listed for that ID,\npull() the name value\n\nOnce this function has been defined, select the appropriate map() function to iterate top_10_doctors_ids through get_doctor_name and store the resulting character vector in top_10_doctors_names.\n\n\nget_doctor_name <- function(doctor_id){\n  # Write function code here\n}\n\n# Iterate the top_10_doctors_ids vector through get_doctor_name and store the results in a character vector\n\n# top_10_doctors_names <- \n\n\n\nQuestion\n\nWrap the above code in a function named calculate_payment_type_amts. Rather than filtering to 1194763482, filter based on the value passed to an argument named doctor_id.\nThen, use the map() function to apply calculate_payment_type_amts to each element in the top_10_doctors_ids vector. Running this code should return a list of 10 data frames.\nFinally, pipe in set_names(top_10_doctors_names) to set the names for each data frame in the list to the doctor’s name.\n\n\n# Write calculate_payment_type_amts function here\n\n# Iterate calculate_payment_type_amts over top_10_doctors_ids and set names to top_10_doctors_names\n\n\n\nQuestion\n\nWrite a function named payments_calendar. The function should:\n\nTake a doctor_id and doctor_name as arguments\nFilter open_payments to the doctor’s ID\nCreate payment calendar plot modeled after the one above.\nSet the title of the plot to the doctor’s name\n\nAfter you’ve written this function, select the appropriate map function to apply payments_calendar to each element in the top_10_doctors_ids vector and top_10_doctors_names vector.\n\nOptional Challenge: Extend your code to include the first listed specialty for each top 10 doctor as a subtitle in each plot.\n\n\n\n# Write payments_calendar function here\n\n# Iterate payments_calendar over top_10_doctors_ids and top_10_doctors_ids to create 10 plots\n\n\n\nQuestion\n\nWrite a function named calculate_manufacturer_payments. The function should:\n\nTake a doctor_id as an argument\nFilter open_payments to that ID\nAggregate the filtered data by covered_recipient_npi, applicable_manufacturer_or_applicable_gpo_making_payment_name, and form_of_payment_or_transfer_of_value\nCalculate the total amount of payments for each grouping\nSort the resulting data frame in descending order by the total amount of payments.\n\nAfter you’ve written this function, use the map_df() function to apply calculate_manufacturer_payments to each element in the top_10_doctors_ids vector. Note how this returns one data frame rather than a list of 10 data frames.\nPlot your resulting data frame as a column plot, attempting (to the best of your ability) to match the formatting of the plot below.\n\nOptional Challenge: List the doctor’s full name in each facet band, rather than the the doctor’s ID.\n\n\n\n# Write calculate_manufacturer_payments function here\n\n# Iterate calculate_manufacturer_payments over top_10_doctors_ids here\n\n# Plot resulting data frame here"
  },
  {
    "objectID": "slides/Day14-Iteration.html#for-today",
    "href": "slides/Day14-Iteration.html#for-today",
    "title": "Iteration",
    "section": "For Today",
    "text": "For Today\n\nQuiz 2 Posted!\nProject 2 Due Today\nFor Loops\nPerforming operations across() variables\nFamily of map functions"
  },
  {
    "objectID": "slides/Day14-Iteration.html#for-loops",
    "href": "slides/Day14-Iteration.html#for-loops",
    "title": "Iteration",
    "section": "For Loops",
    "text": "For Loops\n\n\n\n\n  \n\n\n\n\nfor (i in df$var_a) {\n  print(i + 1)\n}\n\n[1] 3\n[1] 4\n[1] 5\n[1] 6"
  },
  {
    "objectID": "slides/Day14-Iteration.html#for-loops-1",
    "href": "slides/Day14-Iteration.html#for-loops-1",
    "title": "Iteration",
    "section": "For Loops",
    "text": "For Loops\n\n\n\n\n  \n\n\n\n\nfor (i in df |> select(var_a:var_c)) {\n  print(sum(i))\n}\n\n[1] 14\n[1] 14\n[1] 18"
  },
  {
    "objectID": "slides/Day14-Iteration.html#across",
    "href": "slides/Day14-Iteration.html#across",
    "title": "Iteration",
    "section": "across()",
    "text": "across()\n\nApplies a function across multiple columns in a data frame\nTakes as arguments: the columns to perform the function across and the function name\n\n\nExample 1Example 2Example 3\n\n\n\ndf |>\n  summarize(across(var_a:var_c, sum))\n\n\n\n  \n\n\n\n\n\n\ndf |>\n  summarize(across(contains(\"var\"), mean))\n\n\n\n  \n\n\n\n\n\n\ndf |>\n  mutate(across(where(is.numeric), as.character))"
  },
  {
    "objectID": "slides/Day14-Iteration.html#purrr-package",
    "href": "slides/Day14-Iteration.html#purrr-package",
    "title": "Iteration",
    "section": "purrr package",
    "text": "purrr package\n\nIncluded in tidyverse\nPackage for working with functions and vectors\nProvides a family of map() functions\nmap() functions allow us to apply a function to each element of a list or vector"
  },
  {
    "objectID": "slides/Day14-Iteration.html#single-column-data-frames-vs.-vectors",
    "href": "slides/Day14-Iteration.html#single-column-data-frames-vs.-vectors",
    "title": "Iteration",
    "section": "Single Column Data Frames vs. Vectors",
    "text": "Single Column Data Frames vs. Vectors\n\nTo extract a column from a data frame, we use pull()\n\n\nData FrameVector ($)Vector (pull())\n\n\n\ndf |> select(var_a)\n\n\n\n  \n\n\n\n\n\n\ndf$var_a\n\n[1] 2 3 4 5\n\n\n\n\n\ndf |> select(var_a) |> pull()\n\n[1] 2 3 4 5"
  },
  {
    "objectID": "slides/Day14-Iteration.html#setting-names",
    "href": "slides/Day14-Iteration.html#setting-names",
    "title": "Iteration",
    "section": "Setting Names",
    "text": "Setting Names\n\nset_names() sets the names of elements in a vector\n\n\nadd_five <- function(x){\n  x + 5\n}\n\nmap(df$var_a, add_five) |>\n  set_names(df$name)\n\n$obs1\n[1] 7\n\n$obs2\n[1] 8\n\n$obs3\n[1] 9\n\n$obs1\n[1] 10"
  },
  {
    "objectID": "slides/Day14-Iteration.html#family-of-map-functions",
    "href": "slides/Day14-Iteration.html#family-of-map-functions",
    "title": "Iteration",
    "section": "Family of Map Functions",
    "text": "Family of Map Functions\n\nmap_int()map_chr()map_lgl()\n\n\n\nReturns a numeric vector\n\n\ndf |>\n  select(var_a:var_c) |>\n  map_int(is.numeric)\n\nvar_a var_b var_c \n    1     1     1 \n\n\n\n\n\nReturns a character vector\n\n\ndf |>\n  select(var_a:var_c) |>\n  map_chr(is.numeric)\n\n var_a  var_b  var_c \n\"TRUE\" \"TRUE\" \"TRUE\" \n\n\n\n\n\nReturns a logical vector\n\n\ndf |>\n  select(var_a:var_c) |>\n  map_lgl(is.numeric)\n\nvar_a var_b var_c \n TRUE  TRUE  TRUE"
  },
  {
    "objectID": "slides/Day14-Iteration.html#returning-a-data-frame",
    "href": "slides/Day14-Iteration.html#returning-a-data-frame",
    "title": "Iteration",
    "section": "Returning a Data Frame",
    "text": "Returning a Data Frame\n\nmap()map_df()\n\n\n\nReturns a list\n\n\ncreate_total_col <- function(x){\n  df |>\n    filter(name == x) |>\n    mutate(total = var_a + var_b + var_c)\n    \n}\n\nmap(unique(df$name), create_total_col)\n\n[[1]]\n  name var_a var_b var_c total\n1 obs1     2     4     4    10\n2 obs1     5     1     2     8\n\n[[2]]\n  name var_a var_b var_c total\n1 obs2     3     7     9    19\n\n[[3]]\n  name var_a var_b var_c total\n1 obs3     4     2     3     9\n\n\n\n\n\nReturns a data frame (binding rows of list)\n\n\ncreate_total_col <- function(x){\n  df |>\n    filter(name == x) |>\n    mutate(total = var_a + var_b + var_c)\n    \n}\n\nmap_df(unique(df$name), create_total_col)"
  },
  {
    "objectID": "slides/Day14-Iteration.html#iterating-over-multiple-vectors",
    "href": "slides/Day14-Iteration.html#iterating-over-multiple-vectors",
    "title": "Iteration",
    "section": "Iterating Over Multiple Vectors",
    "text": "Iterating Over Multiple Vectors\n\nadd_two_vectors <- function(x, y){\n  x + y\n}\n\nmap2(df$var_a, df$var_b, add_two_vectors)\n\n[[1]]\n[1] 6\n\n[[2]]\n[1] 10\n\n[[3]]\n[1] 6\n\n[[4]]\n[1] 6"
  },
  {
    "objectID": "labs/lab9.html",
    "href": "labs/lab9.html",
    "title": "Lab 9: Mapping with Leaflet",
    "section": "",
    "text": "In this lab, we will build a map that visualizes unaddressed housing violations in NYC in order to identify some of the city’s worst landlords. In doing so, we will gain practice in producing point maps in Leaflet. This analysis is based off of a similar analysis conducted by the NYC Public Advocate’s Office.\n\n\n\nTransform point data to an appropriate CRS\nMap point data in Leaflet\nCreate palettes for points on maps\nAdd legends and labels to a map"
  },
  {
    "objectID": "labs/lab9.html#review-of-key-terms",
    "href": "labs/lab9.html#review-of-key-terms",
    "title": "Lab 9: Mapping with Leaflet",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\n\n\n\n\n\nTip\n\n\n\nYou may wish to reference this reference guide when completing this lab.\n\n\n\nCoordinate Reference System (CRS)\n\na system used to locate geographic points on different spatial projections"
  },
  {
    "objectID": "labs/lab9.html#nyc-public-advocates-worst-landlords-watchlist",
    "href": "labs/lab9.html#nyc-public-advocates-worst-landlords-watchlist",
    "title": "Lab 9: Mapping with Leaflet",
    "section": "NYC Public Advocate’s Worst Landlords Watchlist",
    "text": "NYC Public Advocate’s Worst Landlords Watchlist\nThe NYC Public Advocate (currently Jumaane D. Willaims) publishes a list of NYC’s worst landlords every year by tracking the number of class B and C Housing Preservation and Development (HPD) violations in buildings owned by various people and companies in the City.\n\n\nFrom the Public Advocate’s website:\n\nExamples of Class B violations include: failing to provide self-closing public doors or adequate lighting in public areas, lack of posted Certificates of Occupancy, or failure to remove vermin. Class C violations include: immediately hazardous violations such as rodents, lead-based paint, and lack of heat, hot water, electricity, or gas.1 1See more here.\n\nI have already constructed a dataset documenting the 100 rental properties with the most average housing violations approved in 2021. If you want to check out the code to construct that dataset, you can review the code in data-import.R in this directory. Run the code below to import the dataset."
  },
  {
    "objectID": "labs/lab9.html#setting-up-your-environment",
    "href": "labs/lab9.html#setting-up-your-environment",
    "title": "Lab 9: Mapping with Leaflet",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nInstall the following packages in your Environment:\n\n\ninstall.packages(\"leaflet\")\ninstall.packages(\"sf\")\n\n\nRun the code below to load today’s data frames into your environment.\n\n\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(RColorBrewer)\nlibrary(sf)\n\nworst_buildings &lt;- read_csv(\"https://raw.githubusercontent.com/SDS-192-Intro/sds-192-labs/main/Day30-Leaflet/datasets/worst_buildings.csv\")"
  },
  {
    "objectID": "labs/lab9.html#mapping-in-leaflet",
    "href": "labs/lab9.html#mapping-in-leaflet",
    "title": "Lab 9: Mapping with Leaflet",
    "section": "Mapping in Leaflet",
    "text": "Mapping in Leaflet\nWe are going to work with the leaflet() package to design a map of worst 100 NYC rental properties in terms of housing violations. The map will color the points by the average number of violations and size the points by the number of units in the building. At any point during these exercises, you can reference the leaflet documentation to help you build out these maps."
  },
  {
    "objectID": "labs/lab9.html#set-geometry-and-transform-data-crs",
    "href": "labs/lab9.html#set-geometry-and-transform-data-crs",
    "title": "Lab 9: Mapping with Leaflet",
    "section": "Set Geometry and Transform Data CRS",
    "text": "Set Geometry and Transform Data CRS\nWe will be looking to convert our data into an object with coordinate-based geometry so that we can map it. We can use the st_as_sf() function to do this. st_as_sf() takes two arguments:\n\nThe names of the columns in our data frame containing geographic coodinates in the format coords = c(\"&lt;longitude column names&gt;\", \"&lt;latitude column name&gt;\")\nThe coordinate reference system that the dataset currently uses in the format crs = &lt;crs number&gt;.\n\nAfter adding this geometry column with st_as_sf(), we need to make sure that the CRS for our coordinates is consistent with the CRS of the basemap we will be placing the points on. The coordinates in worst_buildings data are in NAD 83 (EPSG:4269) and our basemap is in WGS 84 (EPSG:4326).\n\nQuestion\n\nIn the code block below, create a new geometry column with the given coordinates, using the function st_as_sf(). The column for longitude will go in the first coordinate position, and latitude will go in the second. Be sure to set the data’s current CRS (4269) in that function. Then transform the CRS to 4326 using st_transform().\n\n\n# Uncomment and fill in the blanks below. The column for longitude will go in the first coordinate position, and latitude will go in the second. \n\n#worst_buildings &lt;- worst_buildings %&gt;%\n#  st_as_sf(coords = c(\"____\", \"____\"), crs = ____) %&gt;%\n#  st_transform(____)"
  },
  {
    "objectID": "labs/lab9.html#add-layers",
    "href": "labs/lab9.html#add-layers",
    "title": "Lab 9: Mapping with Leaflet",
    "section": "Add Layers",
    "text": "Add Layers\nOnce we have our initial map, we can add layers to the map that display different forms of geospatial data. There are a number of different functions in leaflet that we can use to add layers. For instance, we can add markers to the map at a certain geo-coordinates using the addMarkers() function. We can also add polygons and rectangles to the map using the addRectangles() function or the addPolygons() function. Today we are going to work exclusively with the addCircleMarkers() function. This allows us to add a circle at the latitude and longitude for each row in our dataset. It also allows us to adjust the circle’s color and size according to values in our dataset.\n\nQuestion\n\nIn the code block below, pipe addCircleMarkers() onto the basemap and list data = worst_buildings as an argument in that function. It should look like my map below.\n\n\n# Uncomment and add the function\n\n#nyc_map %&gt;%\n\n\n\n\n\n\n\n\n\nMap isn’t so legible/beautiful at this point, right?"
  },
  {
    "objectID": "labs/lab9.html#styling-the-map",
    "href": "labs/lab9.html#styling-the-map",
    "title": "Lab 9: Mapping with Leaflet",
    "section": "Styling the Map",
    "text": "Styling the Map\n\nQuestion\n\nMap isn’t so legible/beautiful at this point, right? Copy the map you just created into the code chunk below, and pipe the addCircleMarkers() function onto your code. Check out the help pages for the addCircleMarkers() function, and add some arguments to help with the map’s legibility. At the very least, you should adjust the radius, weight, color, fillColor, and fillOpacity, and understand how each of these arguments will change the style of the map. For now you can set the color to “black” and the fillColor to “white”. See if you can get your map to match mine below.\n\n\n# Copy previous map here! \n\n\n\n\n\n\n\n\n\nCreating Color Palettes\nNow that our map is looking more legible, let’s color the circles by the 2021 average open violations at each property on the map. Remembering back to our lesson on Understanding Datasets and Visualization Aesthetics, we should keep in mind that avg_violations_weighted is a numeric variable, and therefore, we will create a sequential color palette to map it.\nThere are three functions in leaflet for creating a sequential color palette:\n\ncolorNumeric(): Creates a palette by assigning numbers to different colors on a spectrum\ncolorBin(): Creates a palette by grouping numbers into a specified number of equally-spaced intervals (e.g. 0-10, &gt;10-20, &gt;20-30)\ncolorQuantile(): Creates a palette by grouping numbers into a specified number of equally-sized quantiles\n\n\nNote that we would use colorFactor() to create a categorical palette.\n\nEach function takes both of the following arguments:\n\npalette: the colors we wish to map to the data. We will use preset palettes from the RColorBrewer. You can call display.brewer.all() to see the list of palettes or reference here: http://applied-r.com/rcolorbrewer-palettes/\ndomain: the values we wish to apply the palette to. Here we reference the column from the data frame we wish to color the points by using the accessor $.\n\nIn addition, the colorBin() function takes the argument bins, which indicates the number of color intervals to be created. The colorQuantile() functions takes the argument n, which indicates the number of quantiles to map data into.\n\nQuestion\n\nCreate three palettes below (one using each function colorNumeric(), colorBin(), and colorQuantile()), setting the palette to “YlOrRd”, and the domain to worst_buildings$avg_violations_weighted. For colorBin(), set the bins to 4, and for colorQuantile(), set n to 4.\n\n\npal_num &lt;- colorNumeric(palette=\"YlOrRd\", \n                        domain = worst_buildings$avg_violations_weighted)\n#Uncomment below to create the other two palettes.\n\n#pal_bin &lt;- \n\n#pal_quant &lt;- \n\n\n\nQuestion\n\nCopy and paste the last map you created into the code chunk below, three times. For each, we are going to adjust the fillColor by setting it to the variable from the dataset that we wish to color (avg_violations_weighted), colored by one of the palettes that we created. We can do this by setting the fill color argument equal to:\n\n~pal_num(avg_violations_weighted): for coloring the points according the pal_num() palette we created on the avg_violations_weighted variable\n~pal_bin(avg_violations_weighted): for coloring the points according the pal_bin() palette we created on the avg_violations_weighted variable\n~pal_quant(avg_violations_weighted): for coloring the points according the pal_quant palette we created on the avg_violations_weighted() variable\n\nYour maps should look like mine below.\n\n\n# Copy map first time here!\n\n# Copy map second time here!\n\n# Copy map third time here!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\nIn a comment below, explain the following: Why do the colors appear differently on each map? Which map best represents the distribution of values in avg_violations_weighted?\n\n\n# Add comment here. \n\n\n\n\nSizing the Circles\n\nQuestion\n\nCopy and paste the code from above that colors avg_violations_weighted in bins into the code chunk below. We are going to size each circle by the number of rental units in that property. Number of units is stored in the column legalclassa in our dataset, so we could size the circles by setting the radius to ~legalclassa. However, this would lead to some massive circles on our map as certain buildings have hundreds of units, and the value we supply to radius will determine the pixels of the circle on our map. To deal with this, we are going to take the square root of the units in each property using the sqrt() function. Set the radius to ~sqrt(legalclassa) below.\n\n\n# Copy map colored by bins here!\n\n\n\n\n\n\n\n\n\n\nLegends and Labels\n\nQuestion\n\nAs a final step, we are going to add labels and legends to the map. Copy and paste the code from the previous step into the code chunk below. Then do the following:\n\nAdd the label argument to the addCircleMarkers() function, and set the value to the column in our dataset that indicates the landlord’s full name: ~fullname. Run the code and check what happens when we hover over the circles.\nAdd a pipe to the end of the addCircleMarkers() and then add the function addLegend(). Consult the help pages for the addLegend() function to determine how to add a legend for the meaning of the colors represented on the map. At the very least, you will need an argument for data, pal, and values.\n\n\n\n# Copy previous map here!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nThe NYC Worst Landlord Watchlist determines worst landlords based on class B and C Housing Preservation and Development (HPD) violations. …but not every issue a tenant faces with respect to their landlord will be documented via an HPD violation. What kinds of issues might go ignored when we use these variables alone to determine worst landlords? How should we as data scientists communicate what this data shows and doesn’t show? Share your ideas on our sds-192-discussions Slack channel."
  },
  {
    "objectID": "labs/lab9.html#initialize-your-map",
    "href": "labs/lab9.html#initialize-your-map",
    "title": "Lab 9: Mapping with Leaflet",
    "section": "Initialize your Map",
    "text": "Initialize your Map\nThere are three steps to initializing a map in leaflet:\n\nCall the leaflet() function to create a map widget.\nCall the setView(lat = 0.00, lng = 0.00, zoom = 0). This function determines where the map will initially focus. We provide a set of coordinates that will be the map’s center point when we load it, and a number (from 1 to 16) to indicate the level at which to zoom in on the map.\n\nCall addProviderTiles() to load up provider tiles that constitute a base map. A number of different map providers have provider tiles that we can reference here. A few examples of the arguments we can supply to this function include include:\n\n\nproviders$OpenStreetMap\nproviders$Stamen.Toner\nproviders$CartoDB.Positron\nproviders$Esri.NatGeoWorldMap\n\nRun the code below to initialize a map.\n\nmap1 &lt;- leaflet(width = \"100%\") %&gt;%\n  setView(lat = 0.00, lng = 0.00, zoom = 0) %&gt;%\n  addProviderTiles(providers$Stamen.Toner)\n\nmap1\n\n\n\n\n\n\nQuestion\n\nAdjust the code below to center the map on NYC. You’ll need to look up the coordinates for NYC, keeping in mind that South and West coordinates will be negative. Adjust the zoom to keep the whole city in view (setting the zoom level between 1 and 16). When you are happy with the View, switch out the provider tiles to a base map that won’t distract from the data points we will layer on top of this map. Keep in mind our discussions regarding Visualization Aesthetics here.\n\n\nnyc_map &lt;- leaflet(width = \"100%\") %&gt;%\n  setView(lat = 0.00, lng = 0.00, zoom = 0) %&gt;%\n  addProviderTiles(providers$Stamen.Toner)\n\n# Uncomment below to view the map!\n# nyc_map"
  },
  {
    "objectID": "slides/Day15-PointMapping.html#for-today",
    "href": "slides/Day15-PointMapping.html#for-today",
    "title": "Point Mapping",
    "section": "For Today",
    "text": "For Today\n\nQuiz 2 Posted!\nProjections\nCoordinate Reference Systems\nPoint Mapping in Leaflet"
  },
  {
    "objectID": "slides/Day15-PointMapping.html#why-analyze-spatial-data",
    "href": "slides/Day15-PointMapping.html#why-analyze-spatial-data",
    "title": "Point Mapping",
    "section": "Why analyze spatial data?",
    "text": "Why analyze spatial data?\n\nHow are features distributed across geographies, and what does this tell us about potential disparities?\nWhere are certain events or features concentrated, and what other conditions might implicate these patterns?\nWhat kinds of features are proximate, and what impact might this have?\nWhat is the best way to get from point A to point B?"
  },
  {
    "objectID": "slides/Day15-PointMapping.html#geographic-comparisons",
    "href": "slides/Day15-PointMapping.html#geographic-comparisons",
    "title": "Point Mapping",
    "section": "Geographic Comparisons",
    "text": "Geographic Comparisons"
  },
  {
    "objectID": "slides/Day15-PointMapping.html#concentrations-of-features",
    "href": "slides/Day15-PointMapping.html#concentrations-of-features",
    "title": "Point Mapping",
    "section": "Concentrations of Features",
    "text": "Concentrations of Features\n\nWhere are the most were Missed Collections on October 1, 2021?"
  },
  {
    "objectID": "slides/Day15-PointMapping.html#proximity-analysis-carceral-ej-mapper",
    "href": "slides/Day15-PointMapping.html#proximity-analysis-carceral-ej-mapper",
    "title": "Point Mapping",
    "section": "Proximity Analysis: Carceral EJ Mapper",
    "text": "Proximity Analysis: Carceral EJ Mapper"
  },
  {
    "objectID": "slides/Day15-PointMapping.html#projections",
    "href": "slides/Day15-PointMapping.html#projections",
    "title": "Point Mapping",
    "section": "Projections",
    "text": "Projections\n\nMeans by which we convert curved surface of the globe to 2D rectangle\nNecessarily distorts the surface (e.g. area, distance)\nMany projections exist, serving different purposes"
  },
  {
    "objectID": "slides/Day15-PointMapping.html#orange-peel-example",
    "href": "slides/Day15-PointMapping.html#orange-peel-example",
    "title": "Point Mapping",
    "section": "Orange Peel Example",
    "text": "Orange Peel Example\n\n\n\nImagine that you peel an orange\n\nDatum is the original shape of the fruit (e.g. orange, lemon, apple, grapefruit)\nProjection is how we go about peeling and flattening the orange\n\n\n\n\n\nhttps://geohackweek.github.io/visualization/02-projections/"
  },
  {
    "objectID": "slides/Day15-PointMapping.html#coordinate-reference-system-crs",
    "href": "slides/Day15-PointMapping.html#coordinate-reference-system-crs",
    "title": "Point Mapping",
    "section": "Coordinate Reference System (CRS)",
    "text": "Coordinate Reference System (CRS)\n\nPoints are in different locations depending on how we flatten Earth’s surface into 2D map\nCRS is a system for locating features on a certain map projection via coordinates\nThousands of CRSs but some are more common than others (e.g. WGS 84 most common)\nFor locations to appear correctly on maps, geographic features and underlying maps need to share same CRS"
  },
  {
    "objectID": "slides/Day15-PointMapping.html#sf-package",
    "href": "slides/Day15-PointMapping.html#sf-package",
    "title": "Point Mapping",
    "section": "sf Package",
    "text": "sf Package\n\nEncodes spatial data into geometry objects\nLocates latitudes and longitudes according to a particular CRS\nEnables setting and transforming CRSs\n\n\nLat/LongX/Y Coordinates\n\n\n\nnyc_311_lat_long |>\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 3857) |> head(4)\n\n\n\n  \n\n\n\n\n\n\nnyc_311_xy |>\n  st_as_sf(coords = c(\"x_coordinate_state_plane\", \"y_coordinate_state_plane\"), crs = 2263) |>\n  head(4)"
  },
  {
    "objectID": "slides/Day15-PointMapping.html#how-do-i-know-which-to-use",
    "href": "slides/Day15-PointMapping.html#how-do-i-know-which-to-use",
    "title": "Point Mapping",
    "section": "How do I know which to use?",
    "text": "How do I know which to use?\n\nWe pray that it’s listed somewhere in data documentation.\nNot always the case."
  },
  {
    "objectID": "slides/Day15-PointMapping.html#leaflet",
    "href": "slides/Day15-PointMapping.html#leaflet",
    "title": "Point Mapping",
    "section": "Leaflet",
    "text": "Leaflet\n\nStart by calling leaflet() function, setting the original view, and adding basemap tiles\nNote that you need to look up the coordinates of the geography you wish to center in on.\n\n\n\n\nlibrary(leaflet)\nleaflet() |>\n  setView(lat = 40.7, lng = -100.0, zoom = 3) |> \n  addProviderTiles(\"OpenStreetMap\")"
  },
  {
    "objectID": "slides/Day15-PointMapping.html#transforming-crs",
    "href": "slides/Day15-PointMapping.html#transforming-crs",
    "title": "Point Mapping",
    "section": "Transforming CRS",
    "text": "Transforming CRS\n\nWhy didn’t points appear?\nLeaflet assumes that coordinates will be in CRS:4326 (WGS84)\nWe can use st_transform() to convert points to a different CRS.\n\n\nBeforeAfter\n\n\n\nnyc_311_xy |>\n  st_as_sf(coords = c(\"x_coordinate_state_plane\", \"y_coordinate_state_plane\"), crs = 2263) |>\n  head(4)\n\n\n\n  \n\n\n\n\n\n\nnyc_311_xy |>\n  st_as_sf(coords = c(\"x_coordinate_state_plane\", \"y_coordinate_state_plane\"), crs = 2263) |>\n  st_transform(crs = 4326) |>\n  head(4)"
  },
  {
    "objectID": "slides/Day15-PointMapping.html#leaflet-cont.",
    "href": "slides/Day15-PointMapping.html#leaflet-cont.",
    "title": "Point Mapping",
    "section": "Leaflet, cont.",
    "text": "Leaflet, cont.\n\nnyc_311_xy <- nyc_311_xy |>\n  st_transform(4326)\n\nleaflet() |>\n  setView(lat = 40.7, lng = -74.0, zoom = 10) |> \n  addProviderTiles(\"CartoDB.Positron\") |>\n  addCircleMarkers(data = nyc_311_xy)"
  },
  {
    "objectID": "slides/Day15-PointMapping.html#leaflet-cont.-1",
    "href": "slides/Day15-PointMapping.html#leaflet-cont.-1",
    "title": "Point Mapping",
    "section": "Leaflet, cont.",
    "text": "Leaflet, cont.\n\nnyc_311_xy <- nyc_311_xy |>\n  st_transform(4326)\n\nleaflet() |>\n  setView(lat = 40.7, lng = -74.0, zoom = 10) |> \n  addProviderTiles(\"CartoDB.Positron\") |>\n  addCircleMarkers(data = nyc_311_xy)"
  },
  {
    "objectID": "slides/Day15-PointMapping.html#layers",
    "href": "slides/Day15-PointMapping.html#layers",
    "title": "Point Mapping",
    "section": "Layers",
    "text": "Layers\n\nNYC Rodent and Litter Complaints on April 4, 2022\n\n\nleaflet() |>\n  setView(lat = 40.7, lng = -74.0, zoom = 10) |> \n  addProviderTiles(\"CartoDB.Positron\") |>\n  addCircleMarkers(data = nyc_311_rodent,\n                   fillColor = \"red\",\n                   stroke = FALSE) |>\n  addCircleMarkers(data = nyc_311_litter,\n                   fillColor = \"blue\",\n                   stroke = FALSE)"
  },
  {
    "objectID": "slides/Day15-PointMapping.html#adding-markers",
    "href": "slides/Day15-PointMapping.html#adding-markers",
    "title": "Point Mapping",
    "section": "Adding Markers",
    "text": "Adding Markers\n\naddCircleMarkers()\naddMarkers()\naddPolygons\n\n\nleaflet() |>\n  setView(lat = 40.7, lng = -74.0, zoom = 10) |> \n  addProviderTiles(\"CartoDB.Positron\") |>\n  addCircleMarkers(data = nyc_311_xy)"
  },
  {
    "objectID": "slides/Day15-PointMapping.html#creating-palettes-for-points",
    "href": "slides/Day15-PointMapping.html#creating-palettes-for-points",
    "title": "Point Mapping",
    "section": "Creating Palettes for Points",
    "text": "Creating Palettes for Points\n\ncolorNumeric(): Maps numbers to colors in a specified palette\ncolorBin(): Maps numbers into equally-spaced intervals (e.g. 0-10, >10-20, etc.)\ncolorQuantile(): Maps numbers into equally-sized intervals (same number of observations in each grouping)\ncolorFactor(): Maps categories into a specified number of categorical buckets"
  },
  {
    "objectID": "templates/lab9.html",
    "href": "templates/lab9.html",
    "title": "Lab 9: Mapping with Leaflet",
    "section": "",
    "text": "map1 <- leaflet(width = \"100%\") %>%\n  setView(lat = 0.00, lng = 0.00, zoom = 0) %>%\n  addProviderTiles(providers$Stamen.Toner)\n\nmap1\n\n\n\n\n\n\nQuestion\n\nAdjust the code below to center the map on NYC. You’ll need to look up the coordinates for NYC, keeping in mind that South and West coordinates will be negative. Adjust the zoom to keep the whole city in view (setting the zoom level between 1 and 16). When you are happy with the View, switch out the provider tiles to a base map that won’t distract from the data points we will layer on top of this map. Keep in mind our discussions regarding Visualization Aesthetics here.\n\n\nnyc_map <- leaflet(width = \"100%\") %>%\n  setView(lat = 0.00, lng = 0.00, zoom = 0) %>%\n  addProviderTiles(providers$Stamen.Toner)\n\n# Uncomment below to view the map!\n# nyc_map\n\n\n\nQuestion\n\nIn the code block below, create a new geometry column with the given coordinates, using the function st_as_sf(). The column for longitude will go in the first coordinate position, and latitude will go in the second. Be sure to set the data’s current CRS (4269) in that function. Then transform the CRS to 4326 using st_transform().\n\n\n# Uncomment and fill in the blanks below. The column for longitude will go in the first coordinate position, and latitude will go in the second. \n\n#worst_buildings <- worst_buildings %>%\n#  st_as_sf(coords = c(\"____\", \"____\"), crs = ____) %>%\n#  st_transform(____)\n\n\n\nQuestion\n\nIn the code block below, pipe addCircleMarkers() onto the basemap and list data = worst_buildings as an argument in that function. It should look like my map below.\n\n\n# Uncomment and add the function\n\n#nyc_map %>%\n\n\n\nQuestion\n\nMap isn’t so legible/beautiful at this point, right? Copy the map you just created into the code chunk below, and pipe the addCircleMarkers() function onto your code. Check out the help pages for the addCircleMarkers() function, and add some arguments to help with the map’s legibility. At the very least, you should adjust the radius, weight, color, fillColor, and fillOpacity, and understand how each of these arguments will change the style of the map. For now you can set the color to “black” and the fillColor to “white”. See if you can get your map to match mine below.\n\n\n# Copy previous map here! \n\n\n\nQuestion\n\nCreate three palettes below (one using each function colorNumeric(), colorBin(), and colorQuantile()), setting the palette to “YlOrRd”, and the domain to worst_buildings$avg_violations_weighted. For colorBin(), set the bins to 4, and for colorQuantile(), set n to 4.\n\n\npal_num <- colorNumeric(palette=\"YlOrRd\", \n                        domain = worst_buildings$avg_violations_weighted)\n#Uncomment below to create the other two palettes.\n\n#pal_bin <- \n\n#pal_quant <- \n\n\n\nQuestion\n\nCopy and paste the last map you created into the code chunk below, three times. For each, we are going to adjust the fillColor by setting it to the variable from the dataset that we wish to color (avg_violations_weighted), colored by one of the palettes that we created. We can do this by setting the fill color argument equal to:\n\n~pal_num(avg_violations_weighted): for coloring the points according the pal_num() palette we created on the avg_violations_weighted variable\n~pal_bin(avg_violations_weighted): for coloring the points according the pal_bin() palette we created on the avg_violations_weighted variable\n~pal_quant(avg_violations_weighted): for coloring the points according the pal_quant palette we created on the avg_violations_weighted() variable\n\nYour maps should look like mine below.\n\n\n# Copy map first time here!\n\n# Copy map second time here!\n\n# Copy map third time here!\n\n\n\nQuestion\n\nIn a comment below, explain the following: Why do the colors appear differently on each map? Which map best represents the distribution of values in avg_violations_weighted?\n\n\n# Add comment here. \n\n\n\nQuestion\n\nCopy and paste the code from above that colors avg_violations_weighted in bins into the code chunk below. We are going to size each circle by the number of rental units in that property. Number of units is stored in the column legalclassa in our dataset, so we could size the circles by setting the radius to ~legalclassa. However, this would lead to some massive circles on our map as certain buildings have hundreds of units, and the value we supply to radius will determine the pixels of the circle on our map. To deal with this, we are going to take the square root of the units in each property using the sqrt() function. Set the radius to ~sqrt(legalclassa) below.\n\n\n# Copy map colored by bins here!\n\n\n\nQuestion\n\nAs a final step, we are going to add labels and legends to the map. Copy and paste the code from the previous step into the code chunk below. Then do the following:\n\nAdd the label argument to the addCircleMarkers() function, and set the value to the column in our dataset that indicates the landlord’s full name: ~fullname. Run the code and check what happens when we hover over the circles.\nAdd a pipe to the end of the addCircleMarkers() and then add the function addLegend(). Consult the help pages for the addLegend() function to determine how to add a legend for the meaning of the colors represented on the map. At the very least, you will need an argument for data, pal, and values.\n\n\n\n# Copy previous map here!"
  },
  {
    "objectID": "slides/Day16-PolygonMapping.html#for-today",
    "href": "slides/Day16-PolygonMapping.html#for-today",
    "title": "Polygon Mapping",
    "section": "For Today",
    "text": "For Today\n\nQuiz 2 Posted!\nProject 3 Template Posted!\nReview: Projections and Coordinate Reference Systems\nMapping Geographic Boundaries\nImporting and Mapping Shapefiles\nChloropleth Maps"
  },
  {
    "objectID": "slides/Day16-PolygonMapping.html#mapping-polygons",
    "href": "slides/Day16-PolygonMapping.html#mapping-polygons",
    "title": "Polygon Mapping",
    "section": "Mapping Polygons",
    "text": "Mapping Polygons\n\nNot all cartographic data is encoded as a latitude and longitude! Some cartographic data is encoded as regularly or irregularly shaped polygons\nCan demarcate:\n\nAdministrative boundaries (e.g. census tracts, zip codes, states),\nFeature boundaries (e.g. buildings, bodies of water, etc.)\nBuffers (e.g. areas at a specified distance from a point source)"
  },
  {
    "objectID": "slides/Day16-PolygonMapping.html#administrative-boundaries-us-census",
    "href": "slides/Day16-PolygonMapping.html#administrative-boundaries-us-census",
    "title": "Polygon Mapping",
    "section": "Administrative Boundaries: US Census",
    "text": "Administrative Boundaries: US Census"
  },
  {
    "objectID": "slides/Day16-PolygonMapping.html#administrative-boundaries-us-census-1",
    "href": "slides/Day16-PolygonMapping.html#administrative-boundaries-us-census-1",
    "title": "Polygon Mapping",
    "section": "Administrative Boundaries: US Census",
    "text": "Administrative Boundaries: US Census"
  },
  {
    "objectID": "slides/Day16-PolygonMapping.html#administrative-boundaries-us-census-2",
    "href": "slides/Day16-PolygonMapping.html#administrative-boundaries-us-census-2",
    "title": "Polygon Mapping",
    "section": "Administrative Boundaries: US Census",
    "text": "Administrative Boundaries: US Census"
  },
  {
    "objectID": "slides/Day16-PolygonMapping.html#administrative-boundary-soup",
    "href": "slides/Day16-PolygonMapping.html#administrative-boundary-soup",
    "title": "Polygon Mapping",
    "section": "Administrative Boundary Soup",
    "text": "Administrative Boundary Soup\n\nNYC Boundaries Map"
  },
  {
    "objectID": "slides/Day16-PolygonMapping.html#feature-boundaries",
    "href": "slides/Day16-PolygonMapping.html#feature-boundaries",
    "title": "Polygon Mapping",
    "section": "Feature Boundaries",
    "text": "Feature Boundaries"
  },
  {
    "objectID": "slides/Day16-PolygonMapping.html#feature-boundaries-1",
    "href": "slides/Day16-PolygonMapping.html#feature-boundaries-1",
    "title": "Polygon Mapping",
    "section": "Feature Boundaries",
    "text": "Feature Boundaries"
  },
  {
    "objectID": "slides/Day16-PolygonMapping.html#shapefiles",
    "href": "slides/Day16-PolygonMapping.html#shapefiles",
    "title": "Polygon Mapping",
    "section": "Shapefiles",
    "text": "Shapefiles\n\nFile for storing geospatial feature data\nActually a series of files (.shp, .shx, and .dbf) that must all be present in the directory for the shapefile to import.\nImported file ends in .shp and contains feature geometry"
  },
  {
    "objectID": "slides/Day16-PolygonMapping.html#importing-shapefiles",
    "href": "slides/Day16-PolygonMapping.html#importing-shapefiles",
    "title": "Polygon Mapping",
    "section": "Importing Shapefiles",
    "text": "Importing Shapefiles\n\nFunction st_read() from sf package used to read in shapefiles\n\n\n\n\n\n\nlibrary(sf)\nnyc_cd <- st_read(\"../data/nyc_community_districts/nycd.shp\")\n\nReading layer `nycd' from data source \n  `/Users/lpoirier_1/Documents/GitHub/sds-192-public-website-quarto/website/data/nyc_community_districts/nycd.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 71 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 913175.1 ymin: 120128.4 xmax: 1067383 ymax: 272844.3\nProjected CRS: NAD83 / New York Long Island (ftUS)"
  },
  {
    "objectID": "slides/Day16-PolygonMapping.html#importing-shapefiles-1",
    "href": "slides/Day16-PolygonMapping.html#importing-shapefiles-1",
    "title": "Polygon Mapping",
    "section": "Importing Shapefiles",
    "text": "Importing Shapefiles\n\nnyc_cd |> head(4)"
  },
  {
    "objectID": "slides/Day16-PolygonMapping.html#mapping-polygons-with-leaflet",
    "href": "slides/Day16-PolygonMapping.html#mapping-polygons-with-leaflet",
    "title": "Polygon Mapping",
    "section": "Mapping Polygons with Leaflet",
    "text": "Mapping Polygons with Leaflet\n\nOutputCode\n\n\n\n\n\n\n\n\n\n\n\nlibrary(leaflet)\nnyc_cd <- nyc_cd |>\n  st_transform(4326)\nleaflet(width = \"100%\") |>\n  setView(lat = 40.7, lng = -74.0, zoom = 10) |> \n  addProviderTiles(\"CartoDB.Positron\") |>\n  addPolygons(data = nyc_cd)"
  },
  {
    "objectID": "slides/Day16-PolygonMapping.html#chloropleth-maps",
    "href": "slides/Day16-PolygonMapping.html#chloropleth-maps",
    "title": "Polygon Mapping",
    "section": "Chloropleth Maps",
    "text": "Chloropleth Maps\n\nPresents a numeric variable aggregated by a geospatial unit\nRepresents the value of the aggregated numeric variable via intensity of color\n\nValues presented via a sequential or diverging palette"
  },
  {
    "objectID": "slides/Day16-PolygonMapping.html#mapping-population-how-to-join",
    "href": "slides/Day16-PolygonMapping.html#mapping-population-how-to-join",
    "title": "Polygon Mapping",
    "section": "Mapping Population: How to Join?",
    "text": "Mapping Population: How to Join?\n\nPop DataCD Data\n\n\n\nlibrary(tidyverse)\ncd_pop <- read_csv(\"https://data.cityofnewyork.us/resource/xi7c-iiu2.csv?$select=borough,cd_number,_2010_population%20as%20population_2010\")\n\ncd_pop |> \n  select(borough,cd_number) |> \n  head(5)\n\n\n\n  \n\n\n\n\n\n\nnyc_cd |> \n  select(BoroCD) |> \n  head(5)"
  },
  {
    "objectID": "slides/Day16-PolygonMapping.html#cleaning-geographic-id-fields",
    "href": "slides/Day16-PolygonMapping.html#cleaning-geographic-id-fields",
    "title": "Polygon Mapping",
    "section": "Cleaning Geographic ID Fields",
    "text": "Cleaning Geographic ID Fields\n\ncd_pop <- cd_pop |>\n  mutate(borough_num = case_when( \n    borough == \"Manhattan\" ~ 1,\n    borough == \"Bronx\" ~ 2,\n    borough == \"Brooklyn\" ~ 3,\n    borough == \"Queens\" ~ 4,\n    borough == \"Staten Island\" ~ 5)) |>\n  mutate(cd = str_pad(cd_number, 2, side=\"left\", \"0\")) |>\n  mutate(BoroCD = paste0(borough_num, cd) |> as.numeric())\n\nnyc_cd <- \n  nyc_cd |>\n  left_join(cd_pop, by = c(\"BoroCD\" = \"BoroCD\"))"
  },
  {
    "objectID": "slides/Day16-PolygonMapping.html#nyc-commmunity-district-chloropleth",
    "href": "slides/Day16-PolygonMapping.html#nyc-commmunity-district-chloropleth",
    "title": "Polygon Mapping",
    "section": "NYC Commmunity District Chloropleth",
    "text": "NYC Commmunity District Chloropleth\n\nCodeOutput\n\n\n\nlibrary(RColorBrewer)\npal_num <- colorNumeric(palette = \"YlOrRd\", \n                        domain = nyc_cd$population_2010)\n\nleaflet(width = \"100%\") |>\n  setView(lat = 40.7, lng = -74.0, zoom = 10) |> \n  addProviderTiles(\"CartoDB.Positron\") |>\n  addPolygons(data = nyc_cd,\n              fillColor = ~pal_num(population_2010), \n              stroke = FALSE,\n              fillOpacity = 0.5)"
  },
  {
    "objectID": "slides/Day16-PolygonMapping.html#nyc-commmunity-district-chloropleth-numeric",
    "href": "slides/Day16-PolygonMapping.html#nyc-commmunity-district-chloropleth-numeric",
    "title": "Polygon Mapping",
    "section": "NYC Commmunity District Chloropleth: Numeric",
    "text": "NYC Commmunity District Chloropleth: Numeric\n\nOutputCode\n\n\n\n\n\n\n\n\n\n\n\nlibrary(RColorBrewer)\npal_num <- colorNumeric(palette = \"YlOrRd\", \n                        domain = nyc_cd$population_2010)\n\nleaflet(width = \"100%\") |>\n  setView(lat = 40.7, lng = -74.0, zoom = 10) |> \n  addProviderTiles(\"CartoDB.Positron\") |>\n  addPolygons(data = nyc_cd,\n              fillColor = ~pal_num(population_2010), \n              stroke = FALSE,\n              fillOpacity = 0.5) |>\n  addLegend(data = nyc_cd, \n            values = ~population_2010,\n            pal = pal_num,\n            title = \"Population\")"
  },
  {
    "objectID": "slides/Day16-PolygonMapping.html#nyc-commmunity-district-chloropleth-bin",
    "href": "slides/Day16-PolygonMapping.html#nyc-commmunity-district-chloropleth-bin",
    "title": "Polygon Mapping",
    "section": "NYC Commmunity District Chloropleth: Bin",
    "text": "NYC Commmunity District Chloropleth: Bin\n\nOutputCode\n\n\n\n\n\n\n\n\n\n\n\nlibrary(RColorBrewer)\npal_bin <- colorBin(palette = \"YlOrRd\", \n                        domain = nyc_cd$population_2010, n = 4)\n\nleaflet(width = \"100%\") |>\n  setView(lat = 40.7, lng = -74.0, zoom = 10) |> \n  addProviderTiles(\"CartoDB.Positron\") |>\n  addPolygons(data = nyc_cd,\n              fillColor = ~pal_bin(population_2010), \n              stroke = FALSE,\n              fillOpacity = 0.5) |>\n  addLegend(data = nyc_cd, \n            values = ~population_2010,\n            pal = pal_bin,\n            title = \"Population\")"
  },
  {
    "objectID": "slides/Day16-PolygonMapping.html#nyc-commmunity-district-chloropleth-quantile",
    "href": "slides/Day16-PolygonMapping.html#nyc-commmunity-district-chloropleth-quantile",
    "title": "Polygon Mapping",
    "section": "NYC Commmunity District Chloropleth: Quantile",
    "text": "NYC Commmunity District Chloropleth: Quantile\n\nOutputCode\n\n\n\n\n\n\n\n\n\n\n\nlibrary(RColorBrewer)\npal_quant <- colorQuantile(palette = \"YlOrRd\", \n                        domain = nyc_cd$population_2010, n = 4)\n\nleaflet(width = \"100%\") |>\n  setView(lat = 40.7, lng = -74.0, zoom = 10) |> \n  addProviderTiles(\"CartoDB.Positron\") |>\n  addPolygons(data = nyc_cd,\n              fillColor = ~pal_quant(population_2010), \n              stroke = FALSE,\n              fillOpacity = 0.5) |>\n  addLegend(data = nyc_cd, \n            values = ~population_2010,\n            pal = pal_quant,\n            title = \"Population\")"
  },
  {
    "objectID": "slides/Day17-HowToLie.html#for-today",
    "href": "slides/Day17-HowToLie.html#for-today",
    "title": "How to Lie with Maps",
    "section": "For Today",
    "text": "For Today\n\nQuiz 2 Posted!\nProject 3 Template Posted!\nStrategies for Lying with Maps"
  },
  {
    "objectID": "slides/Day17-HowToLie.html#all-maps-lie",
    "href": "slides/Day17-HowToLie.html#all-maps-lie",
    "title": "How to Lie with Maps",
    "section": "All maps lie!",
    "text": "All maps lie!"
  },
  {
    "objectID": "slides/Day17-HowToLie.html#acknowledgements",
    "href": "slides/Day17-HowToLie.html#acknowledgements",
    "title": "How to Lie with Maps",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nToday’s lecture is almost entirely structured based on:\nDeluca, Eric, and Sara Nelson. 2017. “7. Lying With Maps.” In Mapping, Society, and Technology, edited by Steven Manson. Minneapolis, Minnesota: University of Minnesota Libraries Publishing. https://open.lib.umn.edu/mapping/chapter/7-lying-with-maps/."
  },
  {
    "objectID": "slides/Day17-HowToLie.html#projections",
    "href": "slides/Day17-HowToLie.html#projections",
    "title": "How to Lie with Maps",
    "section": "Projections",
    "text": "Projections"
  },
  {
    "objectID": "slides/Day17-HowToLie.html#symbolization",
    "href": "slides/Day17-HowToLie.html#symbolization",
    "title": "How to Lie with Maps",
    "section": "Symbolization",
    "text": "Symbolization"
  },
  {
    "objectID": "slides/Day17-HowToLie.html#cartograms",
    "href": "slides/Day17-HowToLie.html#cartograms",
    "title": "How to Lie with Maps",
    "section": "Cartograms",
    "text": "Cartograms\n\n\nBenjamin Hennig, https://www.viewsoftheworld.net/"
  },
  {
    "objectID": "slides/Day17-HowToLie.html#standardization",
    "href": "slides/Day17-HowToLie.html#standardization",
    "title": "How to Lie with Maps",
    "section": "Standardization",
    "text": "Standardization"
  },
  {
    "objectID": "slides/Day17-HowToLie.html#classification",
    "href": "slides/Day17-HowToLie.html#classification",
    "title": "How to Lie with Maps",
    "section": "Classification",
    "text": "Classification\nClassification\n\n\nBinning creates equal interval breaks, and classifies the data into the intervals.\n\n\n\n\n\n\n\nQuantiles creates breaks so that there are the same number of observations classified into each bin."
  },
  {
    "objectID": "slides/Day17-HowToLie.html#aggregation",
    "href": "slides/Day17-HowToLie.html#aggregation",
    "title": "How to Lie with Maps",
    "section": "Aggregation",
    "text": "Aggregation\n\nEcological fallacy: Assuming that a value calculated for a group is equivalent to the values for individual members of the group"
  },
  {
    "objectID": "slides/Day17-HowToLie.html#zonation",
    "href": "slides/Day17-HowToLie.html#zonation",
    "title": "How to Lie with Maps",
    "section": "Zonation",
    "text": "Zonation\n\n\n\nModifiable Aerial Unit Problem\nThe boundaries we aggregate data into are often arbitrary (i.e. not meaningful for the analysis)\n\nIf I’m studying air quality effects of a facility on surrounding populations, air pollution doesn’t stop at a zip code even if I aggregate my data by zip code.\n\nDrawing boundaries in a different way would produce different results."
  },
  {
    "objectID": "slides/Day17-HowToLie.html#gerrymandering",
    "href": "slides/Day17-HowToLie.html#gerrymandering",
    "title": "How to Lie with Maps",
    "section": "Gerrymandering",
    "text": "Gerrymandering"
  },
  {
    "objectID": "slides/Day18-APIs.html#for-today",
    "href": "slides/Day18-APIs.html#for-today",
    "title": "APIs",
    "section": "For Today",
    "text": "For Today\n\nQuiz 2 Due Today at 5PM!\nHTTP and Web Protocols\nWriting API Queries\nSQL"
  },
  {
    "objectID": "slides/Day18-APIs.html#api-application-programming-interface",
    "href": "slides/Day18-APIs.html#api-application-programming-interface",
    "title": "APIs",
    "section": "API (Application Programming Interface)",
    "text": "API (Application Programming Interface)\n\nAllows programmers or other systems (or users) to communicate with an online data service\nClients (other programmers) expose part of the data service they’ve used to construct their databases\n\nThis is called an endpoint\nClients also publish documentation about how to communicate with the endpoint\n\nUsers build URLs or HTTP services to request computer-readable data from the endpoint"
  },
  {
    "objectID": "slides/Day18-APIs.html#http-and-get-requests",
    "href": "slides/Day18-APIs.html#http-and-get-requests",
    "title": "APIs",
    "section": "HTTP and GET Requests",
    "text": "HTTP and GET Requests\n\nHypertext Transfer Protocol (HTTP) is what enables communication between servers hosting web pages and browsers\nGET requests enable us to access a resource from a server (only receives data; doesn’t change it on the server)\nEntering https://smith.edu into a browser issues a GET request to access the home page of the Smith website"
  },
  {
    "objectID": "slides/Day18-APIs.html#api-calls",
    "href": "slides/Day18-APIs.html#api-calls",
    "title": "APIs",
    "section": "API Calls",
    "text": "API Calls\n\n\n\nSends an HTTP request URI for a certain resource to a server\n\nURI includes parameters about what data we wish receive and in what format (e.g. all colleges in MA in the format CSV)\n\nServers send that information back via HTTP via response\n\n\n\n\nFigure: REST API - Author: Seobility - License: CC BY-SA 4.0]"
  },
  {
    "objectID": "slides/Day18-APIs.html#api-keys",
    "href": "slides/Day18-APIs.html#api-keys",
    "title": "APIs",
    "section": "API Keys",
    "text": "API Keys\n\nMany services require you to request and reference an API key before accessing data from their API\nAllows systems to track abuse of the service and sometimes limit requests\nUsually free\nAPI key gets included in call"
  },
  {
    "objectID": "slides/Day18-APIs.html#motivating-example-nyc-311-service-requests",
    "href": "slides/Day18-APIs.html#motivating-example-nyc-311-service-requests",
    "title": "APIs",
    "section": "Motivating Example: NYC 311 Service Requests",
    "text": "Motivating Example: NYC 311 Service Requests"
  },
  {
    "objectID": "slides/Day18-APIs.html#constructing-a-query",
    "href": "slides/Day18-APIs.html#constructing-a-query",
    "title": "APIs",
    "section": "Constructing a Query",
    "text": "Constructing a Query\nBase URL is the API Endpoint:https://data.cityofnewyork.us/resource/erm2-nwe9.csv"
  },
  {
    "objectID": "slides/Day18-APIs.html#basic-filtering",
    "href": "slides/Day18-APIs.html#basic-filtering",
    "title": "APIs",
    "section": "Basic Filtering",
    "text": "Basic Filtering\nhttps://data.cityofnewyork.us/resource/erm2-nwe9.csv\n\nFilters appended after a ?\nMultiple filters combined with &\n$limit= limits the number of rows downloaded to a certain number\n\nhttps://data.cityofnewyork.us/resource/erm2-nwe9.csv?unique_key=10693408\nhttps://data.cityofnewyork.us/resource/erm2-nwe9.json?complaint_type=Obstruction&$limit=100"
  },
  {
    "objectID": "slides/Day18-APIs.html#api-documentation",
    "href": "slides/Day18-APIs.html#api-documentation",
    "title": "APIs",
    "section": "API Documentation",
    "text": "API Documentation\n\nIndicates how to sign up for an API key\nIndicates possible output formats (e.g. CSV, JSON, XML, etc.)\nLists field names and descriptions\nProvides example API calls\nOutlines error messages and solutions\n\n\nhttps://dev.socrata.com/foundry/data.cityofnewyork.us/erm2-nwe9"
  },
  {
    "objectID": "slides/Day18-APIs.html#spaces-and-special-characters",
    "href": "slides/Day18-APIs.html#spaces-and-special-characters",
    "title": "APIs",
    "section": "Spaces and Special Characters",
    "text": "Spaces and Special Characters\nInternet protocols don’t know how to interpret spaces or other special characters (i.e. non-ASCII), so we replace those characters with special codes that they do recognize:\n\n\n\nspace : %20\n!: %21\n\": %22\n\n\n\n%: %25\n': %27\n-: %2D\n\n\n\nThere are many resources online for identifying these."
  },
  {
    "objectID": "slides/Day18-APIs.html#example-request-with-special-characters",
    "href": "slides/Day18-APIs.html#example-request-with-special-characters",
    "title": "APIs",
    "section": "Example Request with Special Characters",
    "text": "Example Request with Special Characters\nhttps://data.cityofnewyork.us/resource/erm2-nwe9.csv?complaint_type=Noise%20%2D%20Commercial&$limit=200"
  },
  {
    "objectID": "slides/Day18-APIs.html#reading-api-output-into-r",
    "href": "slides/Day18-APIs.html#reading-api-output-into-r",
    "title": "APIs",
    "section": "Reading API Output into R",
    "text": "Reading API Output into R\n\nWhen API data can be output as a CSV, the URL can be provided directly into read_csv()\n\n\nlibrary(readr)\nnyc_recent_noise <- read_csv(\"https://data.cityofnewyork.us/resource/erm2-nwe9.csv?complaint_type=Noise%20%2D%20Commercial&$limit=200\")\nhead(nyc_recent_noise)"
  },
  {
    "objectID": "slides/Day18-APIs.html#what-is-a-relational-database",
    "href": "slides/Day18-APIs.html#what-is-a-relational-database",
    "title": "APIs",
    "section": "What is a relational database?",
    "text": "What is a relational database?\n\nSeries of tables (or rectangular datasets!)\nEvery table has an ID field for each unit of observation (row)\nIDs are referenced to map relations across related tables\nAccess data from individual tables or across multiple tables with Structured Query Language (SQL)"
  },
  {
    "objectID": "slides/Day18-APIs.html#dplyr",
    "href": "slides/Day18-APIs.html#dplyr",
    "title": "APIs",
    "section": "dplyr",
    "text": "dplyr\n\nselect()\nfilter()\ngroup_by()\narrange()\nhead()"
  },
  {
    "objectID": "slides/Day18-APIs.html#sql",
    "href": "slides/Day18-APIs.html#sql",
    "title": "APIs",
    "section": "SQL",
    "text": "SQL\n\nSELECT\nWHERE\nGROUP BY\nORDER BY\nLIMIT"
  },
  {
    "objectID": "slides/Day18-APIs.html#sql-in-apis",
    "href": "slides/Day18-APIs.html#sql-in-apis",
    "title": "APIs",
    "section": "SQL in APIs",
    "text": "SQL in APIs\n\nDifferent flavors of SQL can be written in the URLs constructed for API calls\nSocrata’s API provides functionality for the Socrata Query Language (SoQL) - a flavor of SQL"
  },
  {
    "objectID": "slides/Day18-APIs.html#soql",
    "href": "slides/Day18-APIs.html#soql",
    "title": "APIs",
    "section": "SoQL",
    "text": "SoQL"
  },
  {
    "objectID": "slides/Day18-APIs.html#dplyr-vs.-sql",
    "href": "slides/Day18-APIs.html#dplyr-vs.-sql",
    "title": "APIs",
    "section": "dplyr vs. SQL",
    "text": "dplyr vs. SQL\n\n\n\nselect()\nfilter()\ngroup_by()\narrange()\nhead()\n\n\n\nSELECT\nWHERE\nGROUP BY\nORDER BY\nLIMIT"
  },
  {
    "objectID": "slides/Day18-APIs.html#response-codes",
    "href": "slides/Day18-APIs.html#response-codes",
    "title": "APIs",
    "section": "Response Codes",
    "text": "Response Codes\n\n\n\n200: Success!\n403: Forbidden\n404: Not Found\n500: Internal Server Error\n502: Bad Gateway\n\n\n\n\nhttps://learn.onemonth.com/what-is-a-404-page/"
  },
  {
    "objectID": "slides/Day18-APIs.html#soql-example-1",
    "href": "slides/Day18-APIs.html#soql-example-1",
    "title": "APIs",
    "section": "SoQL Example 1",
    "text": "SoQL Example 1\n\ndplyrSQLSoQL\n\n\n\ndf |>\n  select(unique_key, created_date, incident_address) |>\n  filter(descriptor == 'Pothole') |>\n  head(100)\n\n\n\nSELECT unique_key, created_date, incident_address \nWHERE descriptor = 'Pothole'\nLIMIT 100\n\n\n\nnyc_recent_potholes <- read_csv(\"https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$select=unique_key,created_date,incident_address&$where=descriptor='Pothole'&$limit=100\")\nhead(nyc_recent_potholes)"
  },
  {
    "objectID": "slides/Day18-APIs.html#soql-example-2",
    "href": "slides/Day18-APIs.html#soql-example-2",
    "title": "APIs",
    "section": "SoQL Example 2",
    "text": "SoQL Example 2\n\ndplyrSQLSoQL\n\n\n\ndf |>\n  filter(complaint_type == 'Traffic') |>\n  group_by(descriptor) |>\n  summarize(count = n()) |>\n  arrange(desc(count))\n\n\n\nWHERE complaint_type = 'Traffic'\nSELECT descriptor, count(*)\nGROUP BY descriptor\nORDER BY count DESC\n\n\n\n\nlibrary(readr)\nnyc_traffic_complaints <- read_csv(\"https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$where=complaint_type='Traffic'&$select=descriptor,count(*)&$group=descriptor&$order=count%20DESC\")\nhead(nyc_traffic_complaints)"
  },
  {
    "objectID": "labs/lab10.html",
    "href": "labs/lab10.html",
    "title": "Lab 10: APIs",
    "section": "",
    "text": "In this lab, we will write queries to access subsets of a very large dataset on the NYC Open Data Portal. We will practice all of the standards we have learned in the course so far in visualizing and wrangling the resulting data.\n\n\n\nWrite API Queries\nRecognize different HTTP Response Codes"
  },
  {
    "objectID": "labs/lab10.html#review-of-key-terms",
    "href": "labs/lab10.html#review-of-key-terms",
    "title": "Lab 10: APIs",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\n\n\n\n\n\nTip\n\n\n\nYou may wish to reference this reference guide when completing this lab.\n\n\n\nApplication Programming Interface (API)\n\nan mechanism that allows programmers or other systems (or users) to access resources from or post data to an online data service\n\nHTTP\n\na Web protocol for transmitting documents (e.g. Web pages, images, datasets, etc.) between web servers and browsers\n\nEndpoint\n\nAn exposed portion of an online data service that clients can assess via API queries"
  },
  {
    "objectID": "labs/lab10.html#nyc-311-service-requests",
    "href": "labs/lab10.html#nyc-311-service-requests",
    "title": "Lab 10: APIs",
    "section": "NYC 311 Service Requests",
    "text": "NYC 311 Service Requests\n311 is a dialing code set aside in many cities across the US to field calls about requests for municipal service – fixing potholes, reporting noise complaints, reporting parking infractions, etc. NYC has one of the most comprehensive 311 programs in the country, fielding calls to be routed to one of the city’s 77 agencies. Unlike in many other cities, New Yorkers can call 311 to report sexual harassment in taxi cabs or to report tenant rights issues (like a landlord failing to address a lack of heat or hot water). In October 2011, data documenting each request for service New Yorkers had made to 311 began to be published daily to the city’s public-facing open data portal. Since then, the dataset has become one of the largest and most complex archived and maintained on the portal, which includes datasets covering topics from restaurant health inspections, to crime statistics, to construction permits. Each row in the 311 dataset details one anonymized and geocoded request - the type of request that was made, when it was made, the location where the incident occurred, and how the relevant NYC agency responded. As of November 2022, the dataset has over 31 million rows, representing service requests made to 311 since 2010.\nWhen the 311 program was introduced, it was celebrated for its bottom-up approach to producing empirical evidence about problems facing New Yorkers; the program buttressed a persuasive techno-liberal imaginary, suggesting that fair and unbiased representation of quality of life concerns would emanate through the crowd-sourced data. When wrangled and visualized, the data can produce persuasive narratives about the state of equity and quality of life in communities across NYC, and thus a number of communities now leverage the data to legitimate claims. Community boards regularly cite 311 statistics about noise, infrastructure, and rodent issues when preparing district budget requests. Activists present the data to city legislators when advocating for or against new housing and transportation laws. Cyclists have produced apps that track complaints about the blocked bicycle lanes in New York. Journalists regularly reference the dataset to report which communities have received the most noise complaints, have had the most restaurants not following Covid-19 orders, or have had the most rat sightings. In 2018, eight pieces of legislation introduced by City Council required data about various issues – from rodent complaints, to noise complaints, to complaints about sexual harassment in taxis – be reported to the city agencies responsible for addressing those issues.\nYet, while representatives acknowledge 311 data to be a useful form of evidence for quality of life issues, they can be hesitant to rely on 311 statistics alone to measure urban problems. Communities that regularly leverage 311 open data warn that the data does not “represent” the saturation of problems in the city but that they represent “where people are complaining in the city.”\nMapping the data shows that, for almost all reported issues, 311 complaints are disproportionately high in gentrifying communities. The data under-represents problems in areas where individuals do not know to report, do not have the capacity to report, or fear harassment (from their landlords or bosses) for reporting. Complaints about NYC Housing Authority (NYCHA) are entirely excluded from the data. The data also over-represents problems when weaponized against minority communities and small businesses to consistently report minor and sometimes false noise issues or legal infractions. While this highlights how the dataset is being used to propagate inequity across the city, activists have also recognized that they can leverage 311 in liberatory ways. I’ve interviewed tenant rights groups that will run 311 calling campaigns in housing complexes, which can significantly increase the number of complaints made at a particular address - not necessarily because it is experiencing more issues than the building next to it, but because there has been advocacy around reporting the issues at this location."
  },
  {
    "objectID": "labs/lab10.html#setting-up-your-environment",
    "href": "labs/lab10.html#setting-up-your-environment",
    "title": "Lab 10: APIs",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nInstall the httr package.\nRun the code below to load the packages for today’s lab\n\n\nlibrary(httr)\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(sf)"
  },
  {
    "objectID": "labs/lab10.html#writing-api-queries",
    "href": "labs/lab10.html#writing-api-queries",
    "title": "Lab 10: APIs",
    "section": "Writing API Queries",
    "text": "Writing API Queries\nAn API query can be parsed into component parts:\n\nAn endpoint to the data resource\nThe format we want the data returned in (e.g. CSV, JSON, XML)\nParameters indicating what subsets/aggregates of the data that we want\nA reference to an API key that grants us access to the data\n\nDifferent APIs have different syntax rules for formatting API queries, so it is important to reference the API documentation to learn how these component parts come together.\nToday we are going to use the API that is made available from Socarata. Socrata is a Web hosting platform that is very commonly used by state and municipal governments, along with federal agencies, to host open government datasets. For example, the City of New York, the City of Chicago, the US Center for Disease Control, and the state of Texas all use the Socrata platform to host datasets they wish to make available to the public. Because Socrata is managing the data, Socrata also defines the API to access the data. A Socrata API query will follow this basic format:\n<endpoint>.<format>?<optional-parameters>\n\nNote that most calls to Socrata do not require an API key.\n\n\n\n\n\n\n\nTip\n\n\n\nNYC 311 is updated very often, and APIs allow us to access the most recent data available at a service. This means that your results for this lab will probably look different from mine. My results are based on running these codes on the evening of November 29, 2022.\n\n\n\nQuestion\n\nWrite an API query to access data at the endpoint https://data.cityofnewyork.us/resource/erm2-nwe9 in a CSV format. Let’s not worry about adding parameters yet. The resulting data frame should look something like my data frame below.\n\n\n# Code below!\n\n#nyc_311 <- read_csv(<URL HERE!>)\n#head(nyc_311)\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n$limit\nNote that this will only return the first 1000 rows of data.\n…but we know from examining the “311 Service Requests from 2010 to Present” metadata that this dataset has over 30 million rows. By default, the API limits the amount of data that gets sent to us so as not to overwhelm our systems.\nWe can manually adjust how much data gets sent by setting the limit parameter to a specific number of rows we wish to have returned. To add a parameter to a Socrata query, first, we put a ? at the end of the query. This indicates that we’re about to list a series of parameters. We then reference parameter names we wish to set behind a $ (e.g. ...?$limit). Finally, we set values to those parameters by adding =<some-value> after the parameter name. (e.g. ...?$limit=10).\n\nQuestion\n\nLet’s say I wanted to access data at the endpoint https://data.cityofnewyork.us/resource/erm2-nwe9 in a CSV format. Let’s not worry about adding parameters yet. Adjust your call above to limit the results to 20 rows of 311 data. The resulting data frame should look something like my data frame below.\n\n\n# Code below!\n\n#nyc_311 <- read_csv(<URL HERE!>)\n#head(nyc_311)\n\n\n\n\n\n  \n\n\n\n\n\nAdditional parameters are available to subset and aggregate our data in various ways. Socrata has it’s own query language called SoQL, which codifies these parameters. SoQL is modeled after SQL - a language for querying data stored in large relational databases. Many SoQL parameters have a direct translation to SQL parameters. …and many of the parameters also have a direct translation to our dplyr data wrangling verbs.\n\n\n\ndplyr\nSoQL\nSQL\n\n\n\n\nhead()\n$limit\nLIMIT\n\n\nselect()\n$select\nSELECT\n\n\nfilter()\n$where\nWHERE\n\n\ngroup_by()\n$group\nGROUP BY\n\n\narrange()\n$order\nORDER BY\n\n\n\n\n\n\n\n$select\nThis dataset has 41 columns, and many of the variables in this dataset will not be immediately relevant to a data science task at hand. By pulling all of the columns, we are requesting much more data (and space on our computers!) than we actually need. We can request that only specific columns be returned via the $select= parameter. We assign the names of the fields we wish to have returned to this parameter. We can find the names of the fields at our data documentation here.\n\n\n\n\n\n\nTip\n\n\n\nNote that we use %>% or |> to string together multiple data wrangling verbs in dplyr. When writing SoQL, we use & to string together multiple parameters (e.g. ...?$limit=<value>&$select=<value>.\n\n\n\nQuestion\n\nCreate an API call to return the unique keys and complaint types for the latest 30 entries in NYC’s 311 open dataset. In other words, translate the following dplyr call into a SoSQL query:\nnyc_311 %>% \n  select(unique_key, complaint_type) %>% \n  head(30)\nPlot the counts of complaint types as a barplot. It should look something like my plot below.\n\n\n# Code below!\n# nyc_311_unique_keys <- read_csv(<URL HERE!>)\n# Plot here!\n\n\n\n\n\n\n\n\n\n$where\nIt’s most likely that I don’t want just 30 random rows of data, but instead that I want specific subsets of the data. We can use the $where= parameter to filter the data to relevant rows. Just like with filter() in dplyr, we can return cases where the values in a row are equal to (=), not equal to (!=), greater than (>), or less than (<) a value we supply (e.g. $where=<field>=<value> or $where=<field>><value>)\n\nPercent Encoding\nWhen determining the value that we are going to supply to the $where parameter, it’s important to keep in mind that not all characters are safe to include in URLs. Some of our keyboard characters serve special purposes in URLs. For instance, the / is used to indicate a sub-directory, and (as we’ve just learned) ? are used to indicate that we are about to string a series of parameters to the end of the URL. Because these characters serve special purposes in a URL, we can’t use them when writing out our value in the ?where parameter… So what do we do when we want to supply a value that includes one of these special characters?\nThis is where percent encoding comes in. Whenever we would normally use the special characters listed below on the left in a URL, we would replace it with the percent encoding listed on the right.\n\nspace : %20\n!: %21\n\": %22\n%: %25\n': %27\n-: %2D\n\nSo for instance, the following: 'Noise - Commercial' includes five reserved characters:\n\nan apostrophe\na space\na dash\nanother space\nanother apostrophe.\n\nI would encode that value as follows: %27Noise%20%2D%20Commercial%27.\n\nCreate an API call to return the unique keys, created_dates and complaint_types for the rows where the agency is listed as ‘FDNY’. In other words, translate the following dplyr call into a SoSQL query:\nnyc_311 %>%\n  select(unique_key, created_date, complaint_type) %>%\n  filter(agency == 'FDNY')\n  \nPlot the data as a point point to match my plot below.\n\n\n# Code below!\n# nyc_311_fdny <- read_csv(<URL HERE!>)\n# Plot here!\n\n\n\n\n\n\n\n\nWe can also string together multiple filter conditions with operators like AND, OR, NOT, IS NULL, etc. $where=<field>=<value> AND <field>=<value>. Note that there are spaces between the first and second condition above, and spaces are reserved characters. We need to replace those spaces with the percent encodings for spaces: $where=<field>=<value>%20AND%20<field>=<value>.\n\n\nQuestion\n\nCreate an API call to return the unique keys, created dates, incident addresses, and BBLs (this is a unique ID for the tax lot of a building in NYC) for rows with complaint type “Food Poisoning” in Manhattan’s community district 5 (which hosts Times Square and other major NYC tourist attractions). Limit the results to 3000 entries in NYC’s 311 open dataset. In other words, translate the following dplyr call into a SoSQL query:\n    nyc_311 %>% \n      select(unique_key, created_date, incident_address, bbl) %>% \n      filter(complaint_type == \"Food Poisoning\" & \n              community_board == \"05 MANHATTAN\") %>% \n      head(3000)\n      \nWrangle the resulting data frame to determine the 5 addresses with the most food poisoning complaints in this community district. It will look something like my data frame below.\n\n\n# Code below!\n# nyc_311_food_poisoning <- read_csv(<URL HERE!>)\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n$order\nThe $order parameter will re-arrange the rows, according to the values in a column that we supply, such that the smallest value will appear at the top and the largest value will appear at the bottom (e.g. $order=<field>). We can tack DESC to end of this parameter in order to reverse the sorting order (e.g. $order=<field>%20DESC).\n\nQuestion\n\nOrder the previous result in descending order by created date, and the subset to the 500 most recent complaints. In other words, translate the following dplyr call into a SoSQL query:\n    nyc_311 %>% \n      select(unique_key, created_date, incident_address, bbl) %>% \n      filter(complaint_type == \"Food Poisoning\" & \n              community_board == \"05 MANHATTAN\") %>% \n      arrange(desc(created_date)) %>%\n      head(500)\nWrangle the resulting data frame to determine the count of complaints at each BBL, and order the results by the count. Store the data frame in nyc_311_food_poisoning_ordered_counts.\nI’ve written an API query to return most recent restaurant grades for restaurants in Manhattan’s community district 5. Join the two data frames by the bbl.\n\nNote that there can be more than one restaurant on a particular tax lot. This means that, if there were x number of Food Poisoning complaints at a particular lot, those complaints may have been associated with any number of restaurants at that lot. BBL is the most specific field we have available for identifying the location associated with a complaint in 311. This means that we are most likely going to have a one-to-many join, and we need to be careful when making assumptions about associations across the datasets.\n\nYour resulting data frame should look something like mine below.\n\n\n# Code below!\n# nyc_311_food_poisoning_ordered <- read_csv(<URL HERE!>)\n# nyc_311_food_poisoning_ordered_counts <- <WRANGLE HERE!>\n\nrestaurant_inspections <- read_csv(\"https://data.cityofnewyork.us/resource/43nn-pn8j.csv?$where=community_board=%27105%27%20AND%20grade%20IS%20NOT%20NULL&$select=camis,dba,bbl,grade,max(grade_date)&$group=camis,dba,bbl,grade&$limit=3000\")\n\n# Join datasets here!\n\n\n\n\n\n  \n\n\n\n\n\nDates can be tricky to work within APIs because we need to know exactly how they are formatted in order to do meaningful things with them. Typically we want to look up how dates are formatted in the API documentation. In this dataset, the dates are formatted as follows: yyyy-mm-ddThh:mm:ss.0000 (e.g. 2022-11-01T00:00:00.000).\nWhen we know how dates are formatted, there’s often many ways we can subset the data to certain dates. For instance, I can access all of the rows where a date is between('date 1' AND 'date 2'). I can extract the year as an integer from a column using date_extract_y(<date-field>) and then filter to the rows in that year. A full list of data transformations that can be applied is Socrata is available here.\n\n\n\nQuestion\n\nCreate an API call to return the unique keys, incident addresses, and created dates for rows with complaint type “Construction Lead Dust” that were created in October 2022. In other words, translate the following dplyr call into a SoSQL query:\n    nyc_311 %>% \n      select(unique_key, created_date, incident_address)\n      filter(complaint_type == 'Construction Lead Dust' & \n              created_date > '2022-10-01' &     \n              created_date < '2022-10-30')\nOnce you get this working, write a function called get_construction_lead_dust_complaints. This function will take a borough as an argument, and should construct a string URL that will further filter the data to a given borough (hint: you will need the function paste0 to construct this string). In your function, read the data frame using read_csv and return the data frame.\nI’ve created a vector of three NYC boroughs for you. Iterate your function over this vector, returning the results as a data frame with rows bound. Your resulting data frame should look something like mine below.\n\n\n# Code below!\n# nyc_311_construction_lead_dust <- read_csv(<URL HERE!>) \n# get_construction_lead_dust_complaints <- <FUNCTION HERE!>\n\nboroughs <- c(\"MANHATTAN\", \"BROOKLYN\", \"BRONX\")\n\n# Write code to iterate function here!\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n$group\nAggregating in SoQL calls looks and feels a bit different than it does in dplyr. The $group= parameter indicates that we wish to return a result grouped by a particular field. However, instead of using a verb like summarize to indicate that we want to perform a calculation in each group, we are going to specify what aggregated columns we want returned to us via the $select parameter.\nSo let’s say I want to return counts of the number of rows assigned to each agency in this dataset. In other words, I want a dataframe with a column listing the agency and a column listing the counts of rows with that agency listed.\nFirst, I would set $select=agency,count(*). Here * refers to everything (i.e. all rows), and count() is a function indicating that we should count all rows. After indicating what columns I want back, then I would set $group=agency.\n\nnyc_311_agency <-\n  read_csv(\"https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$select=agency,count(*)&$group=agency\")\n\nhead(nyc_311_agency)\n\n\n\n  \n\n\n\n\nQuestion\n\nCreate an API call to return the counts per descriptor and borough for the complaint type “Consumer Complaint.” In other words, translate the following dplyr call into a SoSQL query:\n    nyc_311 %>% \n      filter(complaint_type == \"Consumer Complaint\") %>% \n      group_by(descriptor,borough) %>% \n      summarize(count = n())\n\nPivot the data so that there are separate columns indicating counts for each borough in the dataset. Your data frame will look something like mine below.\n\n\n# Code below!\n# nyc_311_consumer_complaint <- read_csv(<URL HERE!>)\n# Pivot here!\n\n\n\n\n\n  \n\n\n\n\n\ncount(*) is function SoQL function. A full list of similar summary functions in SoQL is available here. Another useful SoQL function is distinct, which can be used to return the distinct values in a column. When we use distinct in conjunction with count, we can count the distinct values in a column (e.g. count(distinct%20<field>)).\n\n\n\nQuestion\n\nCreate an API call to return the counts of complaints with the descriptor “Gender Pricing” in each borough, along with the count of distinct incident addresses for these complaints in each borough. In other words, translate the following dplyr call into a SoSQL query:\n    nyc_311 %>% \n      filter(complaint_type == \"Gender Pricing\") %>% \n      group_by(borough) %>% \n      summarize(count = n(),\n                count_distinct_incident_address = length(unique(incident_address)) )\n\nPivot and plot the data to match my plot below.\n\n\n# Code below!\n# nyc_311_gender_pricing <- read_csv(<URL HERE!>)\n# Plot here!\n\n\n\n\n\n\n\n\n\n\n\n\n$having\nOnce we’ve aggregated data in a SoQL query, we no longer use $where= to filter it. Instead, we use $having= to filter aggregate data. For example, in the above call, I could tack on ...&having=count>10 to return the rows from my aggregated data where the count was greater than 10.\n\nQuestion\n\nCreate an API call to return the latitude, longitude, and counts per BBL for the complaint type “HEAT/HOT WATER.” Filter out rows where the BBL is ‘0000000000’, and sort the counts in descending order. Filter the results to the rows where the count of heat and hot water complaints is greater than 1000. In other words, translate the following dplyr call into a SoSQL query:\n    nyc_311 %>%\n      filter(complaint_type == \"HEAT/HOT WATER\" & \n              bbl != \"0000000000\") %>% \n      group_by(bbl, latitude, longitude) %>% \n      summarize(count = n()) %>% \n      arrange(desc(count)) %>%\n      ungroup() %>%\n      filter(count > 1000)\nConvert the result into a geom object, create a palette using the counts variable, and map the points via leaflet to match my map below.\n\n\n# Code below!\n# nyc_311_heat_counts <- read_csv(<URL HERE!>) |> st_as_sf()\n# Palette here!\n# Map here!\n\n\n\n\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nAs noted in the introductory paragraphs, 311 can both over-count and under-count certain quality of life concerns in New York City. It can be a valuable tool for activists trying to draw attention to certain issues, and it can also be weaponized against certain communities when complaints are issued against an establishment in a discriminatory way. Despite this, the data is often used by city decision-makers to determine where to prioritize resources and services. What is the value of 311 data? When and wow should it be used?"
  },
  {
    "objectID": "labs/lab10.html#how-do-apis-work",
    "href": "labs/lab10.html#how-do-apis-work",
    "title": "Lab 10: APIs",
    "section": "How do APIs work?",
    "text": "How do APIs work?\nAPIs enable us to access data from an online service via an API query. Here are the steps that take place behind the scenes when we issue a Web-based API query:\n\nWe provide a line of code that indicates the location of a database online (or the data’s endpoint) and what subsets and aggregates of data want. This line of code is called an API query. API queries look very much like URLs that you enter into a Web browser to access a Web page.\nWe issue that request by entering the API query into a Web browser, or by referencing it via an import function in R.\nA request to GET the data resource is issued via the Hypertext Transfer Protocol (HTTP) - a protocol that manages the transfer of data between clients and servers on the Web. HTTP sends a request to the location indicated by the endpoint to retrieve the data.\nA response code gets issued from the server to the client. There are many codes associated with different issues, but here are the most common:\n\n\n200 indicates success (or that the data can be accessed).\n\n\nexample <- GET(\"http://smith.edu/\")\nexample[[\"status_code\"]]\n\n[1] 200\n\n\n\n403 indicates that the client does not have permission to access the data.\n404 indicates that the resource could not be found at the endpoint the client specified. For example, below the directory this-url-does-not-exist does not exist at http://smith.edu/.\n\n\nexample <- GET(\"http://smith.edu/this-url-does-not-exist\")\nexample[[\"status_code\"]]\n\n[1] 404\n\n\nIf successful, the server also sends the requested resource to the client.\nThis all happens very quickly, and most of it we don’t see!"
  },
  {
    "objectID": "templates/lab10.html",
    "href": "templates/lab10.html",
    "title": "Lab 10: APIs",
    "section": "",
    "text": "Question\n\nWrite an API query to access data at the endpoint https://data.cityofnewyork.us/resource/erm2-nwe9 in a CSV format. Let’s not worry about adding parameters yet. The resulting data frame should look something like my data frame below.\n\n\n# Code below!\n\n#nyc_311 <- read_csv(<URL HERE!>)\n#head(nyc_311)\n\n\n\nQuestion\n\nLet’s say I wanted to access data at the endpoint https://data.cityofnewyork.us/resource/erm2-nwe9 in a CSV format. Let’s not worry about adding parameters yet. Adjust your call above to limit the results to 20 rows of 311 data. The resulting data frame should look something like my data frame below.\n\n\n# Code below!\n\n#nyc_311 <- read_csv(<URL HERE!>)\n#head(nyc_311)\n\n\n\nQuestion\n\nCreate an API call to return the unique keys and complaint types for the latest 30 entries in NYC’s 311 open dataset. In other words, translate the following dplyr call into a SoSQL query:\nnyc_311 %>% \n  select(unique_key, complaint_type) %>% \n  head(30)\nPlot the counts of complaint types as a barplot. It should look something like my plot below.\n\n\n# Code below!\n# nyc_311_unique_keys <- read_csv(<URL HERE!>)\n# Plot here!\n\n\n\nQuestion\n\nCreate an API call to return the unique keys, created dates, incident addresses, and BBLs (this is a unique ID for the tax lot of a building in NYC) for rows with complaint type “Food Poisoning” in Manhattan’s community district 5 (which hosts Times Square and other major NYC tourist attractions). Limit the results to 3000 entries in NYC’s 311 open dataset. In other words, translate the following dplyr call into a SoSQL query:\n    nyc_311 %>% \n      select(unique_key, created_date, incident_address, bbl) %>% \n      filter(complaint_type == \"Food Poisoning\" & \n              community_board == \"05 MANHATTAN\") %>% \n      head(3000)\n      \nWrangle the resulting data frame to determine the 5 addresses with the most food poisoning complaints in this community district. It will look something like my data frame below.\n\n\n# Code below!\n# nyc_311_food_poisoning <- read_csv(<URL HERE!>)\n\n\n\nQuestion\n\nOrder the previous result in descending order by created date, and the subset to the 500 most recent complaints. In other words, translate the following dplyr call into a SoSQL query:\n    nyc_311 %>% \n      select(unique_key, created_date, incident_address, bbl) %>% \n      filter(complaint_type == \"Food Poisoning\" & \n              community_board == \"05 MANHATTAN\") %>% \n      arrange(desc(created_date)) %>%\n      head(500)\nWrangle the resulting data frame to determine the count of complaints at each BBL, and order the results by the count. Store the data frame in nyc_311_food_poisoning_ordered_counts.\nI’ve written an API query to return most recent restaurant grades for restaurants in Manhattan’s community district 5. Join the two data frames by the bbl.\n\nNote that there can be more than one restaurant on a particular tax lot. This means that, if there were x number of Food Poisoning complaints at a particular lot, those complaints may have been associated with any number of restaurants at that lot. BBL is the most specific field we have available for identifying the location associated with a complaint in 311. This means that we are most likely going to have a one-to-many join, and we need to be careful when making assumptions about associations across the datasets.\n\nYour resulting data frame should look something like mine below.\n\n\n# Code below!\n# nyc_311_food_poisoning_ordered <- read_csv(<URL HERE!>)\n# nyc_311_food_poisoning_ordered_counts <- <WRANGLE HERE!>\n\nrestaurant_inspections <- read_csv(\"https://data.cityofnewyork.us/resource/43nn-pn8j.csv?$where=community_board=%27105%27%20AND%20grade%20IS%20NOT%20NULL&$select=camis,dba,bbl,grade,max(grade_date)&$group=camis,dba,bbl,grade&$limit=3000\")\n\n# Join datasets here!\n\n\n\nQuestion\n\nCreate an API call to return the unique keys, incident addresses, and created dates for rows with complaint type “Construction Lead Dust” that were created in October 2022. In other words, translate the following dplyr call into a SoSQL query:\n    nyc_311 %>% \n      select(unique_key, created_date, incident_address)\n      filter(complaint_type == 'Construction Lead Dust' & \n              created_date > '2022-10-01' &     \n              created_date < '2022-10-30')\nOnce you get this working, write a function called get_construction_lead_dust_complaints. This function will take a borough as an argument, and should construct a string URL that will further filter the data to a given borough (hint: you will need the function paste0 to construct this string). In your function, read the data frame using read_csv and return the data frame.\nI’ve created a vector of three NYC boroughs for you. Iterate your function over this vector, returning the results as a data frame with rows bound. Your resulting data frame should look something like mine below.\n\n\n# Code below!\n# nyc_311_construction_lead_dust <- read_csv(<URL HERE!>) \n# get_construction_lead_dust_complaints <- <FUNCTION HERE!>\n\nboroughs <- c(\"MANHATTAN\", \"BROOKLYN\", \"BRONX\")\n\n# Write code to iterate function here!\n\n\n\nQuestion\n\nCreate an API call to return the counts per descriptor and borough for the complaint type “Consumer Complaint.” In other words, translate the following dplyr call into a SoSQL query:\n    nyc_311 %>% \n      filter(complaint_type == \"Consumer Complaint\") %>% \n      group_by(descriptor,borough) %>% \n      summarize(count = n())\n\nPivot the data so that there are separate columns indicating counts for each borough in the dataset. Your data frame will look something like mine below.\n\n\n# Code below!\n# nyc_311_consumer_complaint <- read_csv(<URL HERE!>)\n# Pivot here!\n\n\n\nQuestion\n\nCreate an API call to return the counts of complaints with the descriptor “Gender Pricing” in each borough, along with the count of distinct incident addresses for these complaints in each borough. In other words, translate the following dplyr call into a SoSQL query:\n    nyc_311 %>% \n      filter(complaint_type == \"Gender Pricing\") %>% \n      group_by(borough) %>% \n      summarize(count = n(),\n                count_distinct_incident_address = length(unique(incident_address)) )\n\nPivot and plot the data to match my plot below.\n\n\n# Code below!\n# nyc_311_gender_pricing <- read_csv(<URL HERE!>)\n# Plot here!\n\n\n\nQuestion\n\nCreate an API call to return the latitude, longitude, and counts per BBL for the complaint type “HEAT/HOT WATER.” Filter out rows where the BBL is ‘0000000000’, and sort the counts in descending order. Filter the results to the rows where the count of heat and hot water complaints is greater than 1000. In other words, translate the following dplyr call into a SoSQL query:\n    nyc_311 %>%\n      filter(complaint_type == \"HEAT/HOT WATER\" & \n              bbl != \"0000000000\") %>% \n      group_by(bbl, latitude, longitude) %>% \n      summarize(count = n()) %>% \n      arrange(desc(count)) %>%\n      ungroup() %>%\n      filter(count > 1000)\nConvert the result into a geom object, create a palette using the counts variable, and map the points via leaflet to match my map below.\n\n\n# Code below!\n# nyc_311_heat_counts <- read_csv(<URL HERE!>) |> st_as_sf()\n# Palette here!\n# Map here!"
  },
  {
    "objectID": "labs.html",
    "href": "labs.html",
    "title": "Labs",
    "section": "",
    "text": "Problem Solving\n\nProblem Solving Lab Instructions\nTemplate\n\n\n\nLab 1: Understanding Datasets\n\nLab 1 Instructions\nTemplate\n\n\n\nLab 2: Visualization Aesthetics\n\nLab 2 Instructions\nTemplate\n\n\n\nLab 3: Plotting Freqencies\n\nLab 3 Instructions\nTemplate\n\n\n\nLab 4: GitHub\n\nLab 4 Instructions\n\n\n\nLab 5: Data Wrangling\n\nLab 5 Instructions\nTemplate\n\n\n\nLab 6: Joining Datasets\n\nLab 6 Instructions\nTemplate\n\n\n\nLab 7: Tidying Datasets\n\nLab 7 Instructions\nTemplate\n\n\n\nLab 8: Programming with Data\n\nLab 8 Instructions\nTemplate\n\n\n\nLab 9: Mapping with Leaflet\n\nLab 9 Instructions\nTemplate\n\n\n\nLab 10: Working with APIs\n\nLab 10 Instructions\nTemplate"
  },
  {
    "objectID": "labs/lab9.html#introduction",
    "href": "labs/lab9.html#introduction",
    "title": "Lab 9: Mapping with Leaflet",
    "section": "",
    "text": "In this lab, we will build a map that visualizes unaddressed housing violations in NYC in order to identify some of the city’s worst landlords. In doing so, we will gain practice in producing point maps in Leaflet. This analysis is based off of a similar analysis conducted by the NYC Public Advocate’s Office.\n\n\n\nTransform point data to an appropriate CRS\nMap point data in Leaflet\nCreate palettes for points on maps\nAdd legends and labels to a map"
  }
]