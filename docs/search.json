[
  {
    "objectID": "schedule.html#september-07-2022",
    "href": "schedule.html#september-07-2022",
    "title": "Schedule",
    "section": "September 07, 2022",
    "text": "September 07, 2022\n\nWhat is Data Science?\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n\n\n No Readings\n\n\n If you wish you may fill out the Trigger Warnings Questionnaire in Moodle.\n Be sure to configure Slack notifications for our course. I also encourage you to download and install a Desktop version of Slack.\n Class slides are here"
  },
  {
    "objectID": "schedule.html#september-09-2022",
    "href": "schedule.html#september-09-2022",
    "title": "Schedule",
    "section": "September 09, 2022",
    "text": "September 09, 2022\n\nInfrastructure Set-up\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n Fill out First Day of Class Questionnaire in Moodle\n Complete Syllabus Quiz in Moodle\n Contact me if you will be using a Chromebook\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#september-12-2022",
    "href": "schedule.html#september-12-2022",
    "title": "Schedule",
    "section": "September 12, 2022",
    "text": "September 12, 2022\n\nData Fundamentals\nUNDERSTANDING DATASETS\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#september-14-2022",
    "href": "schedule.html#september-14-2022",
    "title": "Schedule",
    "section": "September 14, 2022",
    "text": "September 14, 2022\n\nIntroduction to R\nUNDERSTANDING DATASETS\n\nDue TodayFurther ResourcesAnnouncements\n\n\n 2. R Basics , Irizarry, Rafael A. (2022). Introduction to Data Science. Data Analysis and Prediction Algorithms with R. URL: https://rafalab.github.io/dsbook/ (visited on Jan. 14, 2022).\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#september-16-2022",
    "href": "schedule.html#september-16-2022",
    "title": "Schedule",
    "section": "September 16, 2022",
    "text": "September 16, 2022\n\nLab: Understanding Datasets\nUNDERSTANDING DATASETS\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n Problem-Solving Lab Due\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#september-19-2022",
    "href": "schedule.html#september-19-2022",
    "title": "Schedule",
    "section": "September 19, 2022",
    "text": "September 19, 2022\n\nGrammar of Graphics\nVISUALIZATION AESTHETICS\n\nDue TodayFurther ResourcesAnnouncements\n\n\n 2. Data Visualization , Baumer, Benjamin S., Daniel T. Kaplan, and Nicholas J. Horton (2021). Modern Data Science with R. 2nd. CRC Press. URL: https://mdsr-book.github.io/mdsr2e/ (visited on Jan. 14, 2022).\n\n\n list()"
  },
  {
    "objectID": "schedule.html#september-21-2022",
    "href": "schedule.html#september-21-2022",
    "title": "Schedule",
    "section": "September 21, 2022",
    "text": "September 21, 2022\n\nVisualization Conventions\nVISUALIZATION AESTHETICS\n\nDue TodayFurther ResourcesAnnouncements\n\n\n 11. Data Visualization Principles , Irizarry, Rafael A. (2022). Introduction to Data Science. Data Analysis and Prediction Algorithms with R. URL: https://rafalab.github.io/dsbook/ (visited on Jan. 14, 2022).\n Lab 1 Due\n\n\n Tufte, Edward R. (2001). The Visual Display of Quantitative Information. 2nd edition. Cheshire, Conn: Graphics Press. ISBN: 978-1-930824-13-3.\n\n\n Class slides are here.\n Quiz 1 has been posted and is due next Wednesday.\n Monday’s solutions are here."
  },
  {
    "objectID": "schedule.html#september-23-2022",
    "href": "schedule.html#september-23-2022",
    "title": "Schedule",
    "section": "September 23, 2022",
    "text": "September 23, 2022\n\nLab: Designing Effective Data Visualizations\nVISUALIZATION AESTHETICS\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n Study Plan Due\n\n\n D’Ignazio, Catherine and Lauren Klein (2020). “3. On Rational, Scientific, Objective Viewpoints from Mythical, Imaginary, Impossible Standpoints”. En. In: Data Feminism. Publisher: PubPub. MIT Press. URL: https://data-feminism.mitpress.mit.edu/pub/5evfe9yd/release/1 (visited on Aug. 24, 2021)."
  },
  {
    "objectID": "schedule.html#september-26-2022",
    "href": "schedule.html#september-26-2022",
    "title": "Schedule",
    "section": "September 26, 2022",
    "text": "September 26, 2022\n\nFrequency Plots\nPLOTTING FREQUENCIES AND DISTRIBUTIONS\n\nDue TodayFurther ResourcesAnnouncements\n\n\n 2. Data Visualization , Ismay, Chester and Albert Y. Kim (2021). Modern Dive: Statistical Inference via Data Science. CRC Press. URL: https://moderndive.com/ (visited on Jan. 14, 2022).\n Complete CATME Survey\n\n\n No Readings\n\n\n Class slides are here\n Friday’s solutions are here"
  },
  {
    "objectID": "schedule.html#september-28-2022",
    "href": "schedule.html#september-28-2022",
    "title": "Schedule",
    "section": "September 28, 2022",
    "text": "September 28, 2022\n\nBoxplots\nPLOTTING FREQUENCIES AND DISTRIBUTIONS\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n Lab 2 Due\n\n\n No Readings\n\n\n Class slides are here\n Quiz 2 posted today.\n Mini-project 1 will be posted on Friday."
  },
  {
    "objectID": "schedule.html#september-30-2022",
    "href": "schedule.html#september-30-2022",
    "title": "Schedule",
    "section": "September 30, 2022",
    "text": "September 30, 2022\n\nLab: Visualizing Data\nPLOTTING FREQUENCIES AND DISTRIBUTIONS\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n Project 1 Assigned\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#october-03-2022",
    "href": "schedule.html#october-03-2022",
    "title": "Schedule",
    "section": "October 03, 2022",
    "text": "October 03, 2022\n\nGitHub Essentials\nGITHUB\n\nDue TodayFurther ResourcesAnnouncements\n\n\n Bryan, Jennifer (2018). “Excuse Me, Do You Have a Moment to Talk About Version Control?” In: The American Statistician 72.1. Publisher: Taylor & Francis _ eprint: https://doi.org/10.1080/00031305.2017.1399928, pp. 20-27. DOI: 10.1080/00031305.2017.1399928. URL: https://doi.org/10.1080/00031305.2017.1399928 (visited on Jan. 14, 2022).\n Group Contract Due\n\n\n Brennan, Stephen (2022). GitHub for Non-Coders - Stephen Brennan. URL: https://brennan.io/2015/08/07/github-noncoders/ (visited on Jan. 14, 2022).\n\n\n Class slides are here"
  },
  {
    "objectID": "schedule.html#october-05-2022",
    "href": "schedule.html#october-05-2022",
    "title": "Schedule",
    "section": "October 05, 2022",
    "text": "October 05, 2022\n\nLab: Collaborating via GitHub\nGITHUB\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n Lab 3 Due\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#october-07-2022",
    "href": "schedule.html#october-07-2022",
    "title": "Schedule",
    "section": "October 07, 2022",
    "text": "October 07, 2022\n\nWork on Group Projects in Class\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#october-10-2022",
    "href": "schedule.html#october-10-2022",
    "title": "Schedule",
    "section": "October 10, 2022",
    "text": "October 10, 2022\n\nNo Class\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#october-12-2022",
    "href": "schedule.html#october-12-2022",
    "title": "Schedule",
    "section": "October 12, 2022",
    "text": "October 12, 2022\n\nSubsetting, Aggregating, and Summarizing Data\nTRANSFORMING DATA\n\nDue TodayFurther ResourcesAnnouncements\n\n\n 3. Data Wrangling , Ismay, Chester and Albert Y. Kim (2021). Modern Dive: Statistical Inference via Data Science. CRC Press. URL: https://moderndive.com/ (visited on Jan. 14, 2022).\n Lab 4 Due\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#october-14-2022",
    "href": "schedule.html#october-14-2022",
    "title": "Schedule",
    "section": "October 14, 2022",
    "text": "October 14, 2022\n\nLab: Aggregating and Summarizing Data\nTRANSFORMING DATA\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n Project 1 Due\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#october-17-2022",
    "href": "schedule.html#october-17-2022",
    "title": "Schedule",
    "section": "October 17, 2022",
    "text": "October 17, 2022\n\nExploratory Data Analysis\nTRANSFORMING DATA\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#october-19-2022",
    "href": "schedule.html#october-19-2022",
    "title": "Schedule",
    "section": "October 19, 2022",
    "text": "October 19, 2022\n\nJoining Datasets\nJOINING DATASETS\n\nDue TodayFurther ResourcesAnnouncements\n\n\n 5. Data wrangling on multiple tables , Baumer, Benjamin S., Daniel T. Kaplan, and Nicholas J. Horton (2021). Modern Data Science with R. 2nd. CRC Press. URL: https://mdsr-book.github.io/mdsr2e/ (visited on Jan. 14, 2022).\n Quiz 1 Recommended Deadline\n Lab 5 Due\n\n\n No Readings\n\n\n Class slides are here"
  },
  {
    "objectID": "schedule.html#october-21-2022",
    "href": "schedule.html#october-21-2022",
    "title": "Schedule",
    "section": "October 21, 2022",
    "text": "October 21, 2022\n\nLab: Joining Datasets\nJOINING DATASETS\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n Project 2 Assigned\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#october-24-2022",
    "href": "schedule.html#october-24-2022",
    "title": "Schedule",
    "section": "October 24, 2022",
    "text": "October 24, 2022\n\nTidying Datasets\nTIDYING DATA\n\nDue TodayFurther ResourcesAnnouncements\n\n\n 26. Parsing dates and times , Irizarry, Rafael A. (2022). Introduction to Data Science. Data Analysis and Prediction Algorithms with R. URL: https://rafalab.github.io/dsbook/ (visited on Jan. 14, 2022).\n Group Contract Due\n\n\n 25. String processing , Irizarry, Rafael A. (2022). Introduction to Data Science. Data Analysis and Prediction Algorithms with R. URL: https://rafalab.github.io/dsbook/ (visited on Jan. 14, 2022)."
  },
  {
    "objectID": "schedule.html#october-26-2022",
    "href": "schedule.html#october-26-2022",
    "title": "Schedule",
    "section": "October 26, 2022",
    "text": "October 26, 2022\n\nPivoting Datasets\nTIDYING DATA\n\nDue TodayFurther ResourcesAnnouncements\n\n\n 6. Tidy Data , Baumer, Benjamin S., Daniel T. Kaplan, and Nicholas J. Horton (2021). Modern Data Science with R. 2nd. CRC Press. URL: https://mdsr-book.github.io/mdsr2e/ (visited on Jan. 14, 2022).\n Lab 6 Due\n\n\n Wickham, Hadley (2014). “Tidy Data”. En. In: Journal of Statistical Software 59, pp. 1-23. DOI: 10.18637/jss.v059.i10. URL: https://doi.org/10.18637/jss.v059.i10 (visited on Jan. 14, 2022).\n\n\n Class slides are here"
  },
  {
    "objectID": "schedule.html#october-28-2022",
    "href": "schedule.html#october-28-2022",
    "title": "Schedule",
    "section": "October 28, 2022",
    "text": "October 28, 2022\n\nLab: Pivoting Datasets\nTIDYING DATA\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n Quiz 1 Due\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#october-31-2022",
    "href": "schedule.html#october-31-2022",
    "title": "Schedule",
    "section": "October 31, 2022",
    "text": "October 31, 2022\n\nProgramming in R\nPROGRAMMING IN R\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#november-02-2022",
    "href": "schedule.html#november-02-2022",
    "title": "Schedule",
    "section": "November 02, 2022",
    "text": "November 02, 2022\n\nWriting Functions\nPROGRAMMING IN R\n\nDue TodayFurther ResourcesAnnouncements\n\n\n 7. Iteration , Baumer, Benjamin S., Daniel T. Kaplan, and Nicholas J. Horton (2021). Modern Data Science with R. 2nd. CRC Press. URL: https://mdsr-book.github.io/mdsr2e/ (visited on Jan. 14, 2022).\n Lab 7 Due\n\n\n No Readings\n\n\n Class slides are here"
  },
  {
    "objectID": "schedule.html#november-04-2022",
    "href": "schedule.html#november-04-2022",
    "title": "Schedule",
    "section": "November 04, 2022",
    "text": "November 04, 2022\n\nWork on Group Projects in Class\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#november-07-2022",
    "href": "schedule.html#november-07-2022",
    "title": "Schedule",
    "section": "November 07, 2022",
    "text": "November 07, 2022\n\nIteration\nPROGRAMMING IN R\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#november-09-2022",
    "href": "schedule.html#november-09-2022",
    "title": "Schedule",
    "section": "November 09, 2022",
    "text": "November 09, 2022\n\nLab: Programming in R\nPROGRAMMING IN R\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n Study Plan Assessment Due\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#november-11-2022",
    "href": "schedule.html#november-11-2022",
    "title": "Schedule",
    "section": "November 11, 2022",
    "text": "November 11, 2022\n\nNo Class\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n Project 2 Due\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#november-14-2022",
    "href": "schedule.html#november-14-2022",
    "title": "Schedule",
    "section": "November 14, 2022",
    "text": "November 14, 2022\n\nMap Projections and Spatial Thinking\nMAPPING\n\nDue TodayFurther ResourcesAnnouncements\n\n\n 17. Working with geospatial data (17.1-17.3) , Baumer, Benjamin S., Daniel T. Kaplan, and Nicholas J. Horton (2021). Modern Data Science with R. 2nd. CRC Press. URL: https://mdsr-book.github.io/mdsr2e/ (visited on Jan. 14, 2022).\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#november-16-2022",
    "href": "schedule.html#november-16-2022",
    "title": "Schedule",
    "section": "November 16, 2022",
    "text": "November 16, 2022\n\nMapping Point Data in Leaflet\nMAPPING\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n Lab 8 Due\n\n\n No Readings\n\n\n Class slides are here"
  },
  {
    "objectID": "schedule.html#november-18-2022",
    "href": "schedule.html#november-18-2022",
    "title": "Schedule",
    "section": "November 18, 2022",
    "text": "November 18, 2022\n\nLab: Mapping in Leaflet\nMAPPING\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n Project 3 Assigned\n Quiz 2 Recommended Deadline\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#november-21-2022",
    "href": "schedule.html#november-21-2022",
    "title": "Schedule",
    "section": "November 21, 2022",
    "text": "November 21, 2022\n\nChloropleth Maps\nMAPPING\n\nDue TodayFurther ResourcesAnnouncements\n\n\n 17. Working with geospatial data (17.4-17.8) , Baumer, Benjamin S., Daniel T. Kaplan, and Nicholas J. Horton (2021). Modern Data Science with R. 2nd. CRC Press. URL: https://mdsr-book.github.io/mdsr2e/ (visited on Jan. 14, 2022).\n Group Contract Due\n\n\n No Readings\n\n\n Class slides are here"
  },
  {
    "objectID": "schedule.html#november-23-2022",
    "href": "schedule.html#november-23-2022",
    "title": "Schedule",
    "section": "November 23, 2022",
    "text": "November 23, 2022\n\nNo Class\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n Project 2 Due\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#november-25-2022",
    "href": "schedule.html#november-25-2022",
    "title": "Schedule",
    "section": "November 25, 2022",
    "text": "November 25, 2022\n\nNo Class\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#november-28-2022",
    "href": "schedule.html#november-28-2022",
    "title": "Schedule",
    "section": "November 28, 2022",
    "text": "November 28, 2022\n\nWorking with APIs\nMAPPING\n\nDue TodayFurther ResourcesAnnouncements\n\n\n Deluca, Eric and Sara Nelson (2017). “7. Lying With Maps”. In: Mapping, Society, and Technology. Ed. by Steven Manson. Minneapolis, Minnesota: University of Minnesota Libraries Publishing. URL: https://open.lib.umn.edu/mapping/chapter/7-lying-with-maps/ (visited on Apr. 14, 2022).\n Quiz 2 Due\n Lab 9 Due\n\n\n No Readings\n\n\n Class slides are here\n MP2 due Wednesday; last day to request an extension (by 5PM)\n Quiz 4 due Wednesday (5PM)"
  },
  {
    "objectID": "schedule.html#november-30-2022",
    "href": "schedule.html#november-30-2022",
    "title": "Schedule",
    "section": "November 30, 2022",
    "text": "November 30, 2022\n\nAdvanced APIs\nDATA RETRIEVAL\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#december-02-2022",
    "href": "schedule.html#december-02-2022",
    "title": "Schedule",
    "section": "December 02, 2022",
    "text": "December 02, 2022\n\nWork on Group Projects in Class\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n Quiz 3 Recommended Deadline\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#december-05-2022",
    "href": "schedule.html#december-05-2022",
    "title": "Schedule",
    "section": "December 05, 2022",
    "text": "December 05, 2022\n\nLab: How to Lie with Maps\nDATA RETRIEVAL\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#december-07-2022",
    "href": "schedule.html#december-07-2022",
    "title": "Schedule",
    "section": "December 07, 2022",
    "text": "December 07, 2022\n\nSQL\nDATA RETRIEVAL\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n Project 3 Due\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#december-09-2022",
    "href": "schedule.html#december-09-2022",
    "title": "Schedule",
    "section": "December 09, 2022",
    "text": "December 09, 2022\n\nreview\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n\n\n No Readings"
  },
  {
    "objectID": "schedule.html#december-12-2022",
    "href": "schedule.html#december-12-2022",
    "title": "Schedule",
    "section": "December 12, 2022",
    "text": "December 12, 2022\n\nWrap-up\n\nDue TodayFurther ResourcesAnnouncements\n\n\n No Readings\n Quiz 3 Due\n Lab 10 Due\n\n\n No Readings"
  },
  {
    "objectID": "lab1.html",
    "href": "lab1.html",
    "title": "Lab 1: Understanding Datasets",
    "section": "",
    "text": "This lab is all about learning to understand the context and parts of a dataset by referencing and interpreting data dictionaries and technical data documentation.\n\n\n\nRead a data dictionary\nReference data documentation\nIdentify unique observations in a dataset\nUnderstand different variable types\nLook up value codes and recode a variable\nDetermine the number of missing values in a variable and why they are missing"
  },
  {
    "objectID": "lab1.html#review-of-key-terms",
    "href": "lab1.html#review-of-key-terms",
    "title": "Lab 1: Understanding Datasets",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\nRectangular Datasets\n\ndatasets in which all rows are the same length, and all columns are the same length\n\nObservations\n\nrows in a dataset; represent discrete entities we observe in the world\n\nVariables\n\ncolumns in a dataset; describe something about an observation\n\nVector\n\none-dimensional set of values that are all of the same type\n\nData Frame\n\na list of vectors of equal lengths; typically organizes data into a two-dimensional table composed of columns (the vectors) and rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nI use the letters df as a placeholder to refer to an arbitrary data frame in R. Any time that I refer to df in a lab, know that you can swap out df with any data frame in your environment.\n\n\n\nUnique Key\n\nvariable (column) in the dataset that can be used to uniquely identify each row\n\nNominal categorical variables\n\nvariables that identify something else; sometimes, numbers are considered nominal categorical variables (e.g. zip code)\n\nOrdinal categorical variables\n\ncategorical variables that can be ranked or placed in a particular order (e.g. High, Medium, Low)\n\nDiscrete numeric variables\n\nnumeric variables that represent something that is countable (e.g. the number of students in a classroom, the number pages in a book)\n\nContinuous numeric variables are variables\n\nvariables in which it is always possible to measure the value more precisely (e.g. time can be measured with infinite amount of specificity - hours > minutes > seconds > milliseconds > microseconds > nanoseconds …)"
  },
  {
    "objectID": "lab1.html#scorecard-dataset",
    "href": "lab1.html#scorecard-dataset",
    "title": "Lab 1: Understanding Datasets",
    "section": "Scorecard Dataset",
    "text": "Scorecard Dataset\nIn his 2013 State of the Union Address, President Barack Obama announced his plans to create a “college scorecard” that would allow prospective students and parents to compare schools in terms of cost, offerings, diversity, completion rates, and post-graduate earnings. This data was first published in 2015 and since has undergone several improvements and revisions.\nThe College Scorecard dataset is massive. In fact, I thought long and hard about whether this was really the first dataset I wanted to introduce to you in a lab. It includes information about over 6500 institutions in the U.S., and has more than 3000 columns documenting information about those institutions. I chose this dataset for this lab because, if you can learn to read this data dictionary, you will be leaps and bounds ahead of the game in learning to read other data dictionaries. (It’s also just a super cool dataset, and hint, hint: you will get a chance to dive into it in much more detail in a few weeks). While the full data is available online, we are only going to work with a small subset of the data today."
  },
  {
    "objectID": "lab1.html#setting-up-your-environment",
    "href": "lab1.html#setting-up-your-environment",
    "title": "Lab 1: Understanding Datasets",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nInstall the RScorecard package by entering the following into your Console: install.packages(\"rscorecard\")\nCreate a Scorecard API Key at this link. Shortly after you fill out the form, you will be emailed a key. Copy that key into code chunk below, replacing all of the following text in sc_key(): Sys.getenv(“SCORECARD_KEY”). Be sure to wrap the key in quotation marks.\nDownload the Scorecard Data Dictionary and Technical Documentation for Institution-Level Data Files here.\nRun the code below to the import 2018 Scorecard data for Massachusetts into R. Call me or one of the data assistants over if you get an error.\n\n\nlibrary(tidyverse)\nlibrary(rscorecard)\nsc_key(Sys.getenv(\"SCORECARD_KEY\")) # Replace Sys.getenv(\"SCORECARD_KEY\") here with your API Key in quotation marks\n\nscorecard <- sc_init() %>%\n  sc_year(2018) %>%                 #Note how we are looking at only 2021 data here!\n  sc_filter(stabbr == \"MA\") %>%     #Note how we are looking at only Massachusetts data here!\n  sc_select(unitid, instnm, city, highdeg, control, ugds, adm_rate, costt4_a, costt4_p, pcip27, pctfloan, admcon7, wdraw_orig_yr2_rt, cdr3) %>%\n  sc_get()"
  },
  {
    "objectID": "lab1.html#glimpsing-the-data",
    "href": "lab1.html#glimpsing-the-data",
    "title": "Lab 1: Understanding Datasets",
    "section": "Glimpsing the Data",
    "text": "Glimpsing the Data\nWhen working with very large datasets, we need tools to help us get a sense of the dataset without having to load the entire data frame. For instance, we can view the first 6 rows of the dataset by calling head().\n\nscorecard %>% head()\n\n# A tibble: 6 × 15\n  unitid instnm       city  highdeg control  ugds adm_r…¹ costt…² costt…³ pcip27\n   <int> <chr>        <chr>   <int>   <int> <int>   <dbl>   <int>   <int>  <dbl>\n1 164368 Hult Intern… Camb…       4       2   574   0.571   60090      NA 0     \n2 164447 American In… Spri…       4       2  1307   0.684   47742      NA 0     \n3 164465 Amherst Col… Amhe…       3       2  1855   0.128   71300      NA 0.110 \n4 164492 Anna Maria … Paxt…       4       2  1131   0.736   51109      NA 0     \n5 164535 Assabet Val… Marl…       1       1    50   0.452      NA   19703 0     \n6 164562 Assumption … Worc…       4       2  2014   0.811   53303      NA 0.0162\n# … with 5 more variables: pctfloan <dbl>, admcon7 <int>,\n#   wdraw_orig_yr2_rt <dbl>, cdr3 <dbl>, year <dbl>, and abbreviated variable\n#   names ¹​adm_rate, ²​costt4_a, ³​costt4_p\n\n\nstr() provides a great deal of information about the observations in the data frame, including the number of variables, the number of observations, the column names, their data types, and a list of observations.\n\nscorecard %>% str()\n\ntibble [153 × 15] (S3: tbl_df/tbl/data.frame)\n $ unitid           : int [1:153] 164368 164447 164465 164492 164535 164562 164580 164599 164614 164632 ...\n $ instnm           : chr [1:153] \"Hult International Business School\" \"American International College\" \"Amherst College\" \"Anna Maria College\" ...\n $ city             : chr [1:153] \"Cambridge\" \"Springfield\" \"Amherst\" \"Paxton\" ...\n $ highdeg          : int [1:153] 4 4 3 4 1 4 4 1 3 4 ...\n $ control          : int [1:153] 2 2 2 2 1 2 2 3 2 2 ...\n $ ugds             : int [1:153] 574 1307 1855 1131 50 2014 2361 45 56 1921 ...\n $ adm_rate         : num [1:153] 0.571 0.684 0.128 0.736 0.452 ...\n $ costt4_a         : int [1:153] 60090 47742 71300 51109 NA 53303 68482 NA 26691 46315 ...\n $ costt4_p         : int [1:153] NA NA NA NA 19703 NA NA NA NA NA ...\n $ pcip27           : num [1:153] 0 0 0.11 0 0 ...\n $ pctfloan         : num [1:153] 0.0566 0.8333 0.1661 0.7464 0.4286 ...\n $ admcon7          : int [1:153] 3 5 1 3 3 3 1 NA 1 5 ...\n $ wdraw_orig_yr2_rt: num [1:153] NA 0.181 0.124 0.129 NA ...\n $ cdr3             : num [1:153] 0.043 0.073 0.014 0.073 0.065 0.044 0.019 0.06 0.024 0.048 ...\n $ year             : num [1:153] 2018 2018 2018 2018 2018 ...\n\n\nYou can also click on the name of your data frame in your Environment panel in RStudio, and it will open a new tab in RStudio that displays the data in a tabular format. Try clicking on scorecard in your Environment panel.\n\n\n\n\n\n\nTip\n\n\n\nThis is the same as calling df %>% View() in your Console."
  },
  {
    "objectID": "lab1.html#getting-to-know-this-dataset",
    "href": "lab1.html#getting-to-know-this-dataset",
    "title": "Lab 1: Understanding Datasets",
    "section": "Getting to Know this Dataset",
    "text": "Getting to Know this Dataset\n\nObservations (Rows)\nIn starting our data analysis, we need to have a good sense of what each observation in our dataset refers to - or its observational unit. Think of it this way. If you were to count the number rows in your dataset, what would that number refer to? A unique key is a variable (or set of variables) that uniquely identifies an observation in the dataset. Think of a unique key as a unique way to identify a row and all of the values in it. There should never be more than one row in the dataset with the same unique key. A unique key tells us what each row in the dataset refers to.\n\n\n\n\n\n\nExercise 1\n\n\n\nSee if you can identify a unique key for this dataset. Write some lines of code to determine whether the column you’ve identified can act as unique key for the data. Hint: You need to check whether the values in the column ever repeat.\n\n\n\n# Write code to check if values in column are all unique here.\n\n\n\n\n\n\n\nTip\n\n\n\nNote that NAME is typically not an appropriate variable to use as a unique key. Let me provide an example to demonstrate this. When I worked for BetaNYC, I was trying to build a map of vacant storefronts in NYC by mapping all commercially zoned properties in the city, and then filtering out those properties where a business was licensed or permitted. This way the map would only include properties where there wasn’t a business operating. One set of businesses I was filtering out was restaurants. The only dataset that the city had made publicly available for restaurant permits was broken. It was operating on an automated process to update whenever there was a change in the permit; however, whenever a permit was updated, rather than updating the appropriate fields in the existing dataset, it was creating a new row in the dataset that only included the permit holder (the restaurant name), the permit type, and the updated fields. Notably the unique permit ID was not being included in this new row. We pointed this issue out to city officials, but fixing something like this can be slow and time-consuming, so in the meantime, we looked into whether we could clean the data ourselves by aggregating the rows that referred to the same restaurant. However, without the permit ID it was impossible to uniquely identify the restaurants in the dataset. Sure, we had the restaurant name, but do you know how many Wendy’s there are in NYC?\n\n\nAnytime we count something in the world, we are not only engaging in a process of tabulation; we are also engaging in a process of defining. If I count the number of students in a class, I first have to define what counts as a student. If someone is auditing the class, do they count? If I, as the instructor, am learning from my students, do I count myself as a student? As I make decisions about how I’m going to define “student,” those decisions impact the numbers that I produce. When I change my definition of “student,” how I go about tabulating students also changes. Thus, as we prepare to count observations in a dataset, it is important to know how those observations are defined.\n\n\n\n\n\n\nExercise 2\n\n\n\nAt this point, you’ve probably figured out that each row in this dataset is a higher education institution. …but there are many different ways that we can define higher education institutions, and that will impact what gets included and excluded in our data. Referencing the Technical Documentation, locate a definition for the unit of observation in this dataset. What institutions are included, and what institutions are excluded? Summarize a definition below.\n\n\n\n\n\n\n\n\nYour Response\n\n\n\nFill response here.\n\n\n\n\nVariables (Columns)\nNote the column names for this dataframe, and the kinds of values that appear in those columns. Some of them (like city and year) might make sense to you immediately. Others (like pcip27 and highdeg) might be much more confusing. To figure out what we are looking out, we are going to need to refer to the dataset’s data dictionary.\nOpen the data dictionary you downloaded in an earlier step. It will open as an Excel file. Click on the tab labeled “Institution_Data_Dictionary”. There are thousands of variables in this dataset, falling into the broader categories of school, completion, admissions, cost, etc. Note how the file is organized, and specifically draw your attention to:\n\nColumn 1 (NAME OF DATA ELEMENT): This is a long description of the variable and gives you clues as to what is represented in it.\nColumn 6 (VARIABLE NAME): This is the column name for the variable. This is how you will reference the variable in R.\nColumn 7 (VALUE): These are the possible values for that variable. Note that for many categorical variables, the values are numbers. We are going to have to associate the numbers with their corresponding labels.\nColumn 8 (LABEL): These are the labels associated with the values recorded for the variable.\nColumn 11 (NOTES): This provides notes about the variable, including whether it is currently in use and what missing values indicate.\n\n\n\n\n\n\n\nExercise 3\n\n\n\nFor each of the variable names in the scorecard data frame, look up the associated name in the data dictionary. You will need to search for the variable name in the sixth column of the data dictionary (I recommend using Ctrl-F to quickly locate the variable name in the spreadsheet.) Once you’ve found the variable name, reference column 1 to determine what this variable means, and reference columns 7 and 8 to see what possible values will appear in that column.\nIdentify one nominal variable, one ordinal variable, one discrete variable, and one continuous variable in scorecard and list their variable names below. Then uncomment the lines below and use the typeof() function to see how R determined their data types. Did any surprise you?\n\n\n\n\n\n\n\n\nYour Response\n\n\n\n\nNominal variable: _____\nOrdinal variable: _____\nDiscrete variable: _____\nContinuous variable: _____\n\n\n\n\n#typeof(scorecard$_____)\n#typeof(scorecard$_____)\n#typeof(scorecard$_____)\n#typeof(scorecard$_____)\n\n\n\nValues (Cells)\nYou may have noticed that several categorical variables are coded as numbers in the imported dataset. For instance, look at the column control which designates the institution’s ownership. Running the code below, we see that the distinct values in that column are 1, 2, and 3.\n\nscorecard %>% distinct(control)\n\n# A tibble: 3 × 1\n  control\n    <int>\n1       2\n2       1\n3       3\n\n\nWhen we reference that column in the data dictionary (row 27), we see that a 1 in that column designates that the institution is Public, a 2 that the institution is Private nonprofit, and a 3 that the institution is Private for-profit. While I can always look that up, sometimes it is helpful to have that information in our dataset. For instance, let’s say I create a bar plot that’s supposed to show how many higher education institutions have each type of ownership in MA (which you will learn how to do soon!). The plot can be confusing when control is a series of numbers.\n\nggplot(scorecard, aes(x = control)) +\n  geom_bar()\n\n\n\n\nWith this in mind, sometimes it can be helpful to recode the values in a column. Recoding data involves replacing the values in a vector according to criteria that we provide. Remember how all columns in a data frame are technically vectors? We can use the recode() function to recode all of the values in the control vector. We are going to store the recoded values in a new column in our dataset called control_text. Check out the code below to see how we do this. Reference the help pages for recode (i.e. ?recode) to help you interpret the code.\n\nscorecard$control_text <-\n  recode(\n    scorecard$control, \n    \"1\" = \"Public\", \n    \"2\" = \"Private nonprofit\", \n    \"3\" = \"Private for-profit\",\n    .default = NA_character_\n  )\n\nCheck out our barplot now!\n\nggplot(scorecard, aes(x = control_text)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\nWrite code below to recode the admcon7 variable and store the results in a new variable in scorecard called admcon7_text. You’ll need to look up the values in the data dictionary. If you’ve done this correctly, running this code should produce a barplot that displays multiple bars.\n\n\n\nscorecard$admcon7_text <-\n  recode(\n    scorecard$admcon7, \n    #Fill replacements here\n    .default = NA_character_\n  )\n\nggplot(scorecard, aes(x = admcon7_text)) +\n  geom_bar()\n\n\n\n\n\n\nMissing Values\nWhen we have missing values in a rectangular dataset, we have to provide a placeholder for the missing value in order for the dataset to remain rectangular. If we just skipped the value, then our dataset wouldn’t necessarily have rows of all equal lengths and columns of all equal lengths. In R, NA serves as that placeholder. Before we start analyzing data, it can be important to note how many NA values we have in a column so that we can determine if the data is representative.\nThe function is.na() checks whether a value is an NA value and returns TRUE if it is and FALSE if it isn’t. Providing a vector to is.na() will check this for every value in the vector and return a logical vector indicating TRUE/FALSE for every original value in the vector.\n\nis.na(scorecard$wdraw_orig_yr2_rt)\n\n  [1]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE\n [25] FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE\n [37] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n [49] FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE\n [73]  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE\n [85] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE\n [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE\n[109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE\n[121]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE\n[133]  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n[145] FALSE  TRUE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE\n\n\nWhen we sum() across a logical vector, R will calculate the number of TRUE values in the vector.\n\nsum(is.na(scorecard$wdraw_orig_yr2_rt))\n\n[1] 48\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\nCalculate the number of missing values in both the costt4_a and the costt4_p columns. Reference the NOTES column in the data dictionary to determine why there are so many NA values in these columns. Add a comment to the code chunk below, explaining the reason for missing values in these columns.\n\n\n\n# Calculate the number of NA values here\n\n# Add comment to explain missing values here\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nReferencing the College Scorecard data documentation, see if you can determine which students are included in calculations of earnings and debt. How might the data’s coverage bias the values that get reported? What might be the social consequences of these biases? Share your ideas on our sds-192-discussions Slack channel."
  },
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "Lab 6: Joining Data",
    "section": "",
    "text": "In this lab, you will …\n\n\n\n…"
  },
  {
    "objectID": "lab6.html#review-of-key-terms",
    "href": "lab6.html#review-of-key-terms",
    "title": "Lab 6: Joining Data",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\n\n\n\n\n\nTip\n\n\n\nYou may wish to reference this Data Wrangling Cheatsheet when completing this lab.\n\n\nleft\nright\ninner\nouter\nanti"
  },
  {
    "objectID": "lab6.html#twitter-dataset",
    "href": "lab6.html#twitter-dataset",
    "title": "Lab 6: Joining Data",
    "section": "Twitter Dataset",
    "text": "Twitter Dataset"
  },
  {
    "objectID": "lab6.html#setting-up-your-environment",
    "href": "lab6.html#setting-up-your-environment",
    "title": "Lab 6: Joining Data",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nInstall the TwitteR package by entering the following into your Console: install.packages(\"twitteR\")\nAPI Key and Secret\nConfigure App\nhttp://127.0.0.1:1410\nhttps://github.com/sds-192-intro-fall22/sds-192-public-website-quarto\n\n\nlibrary(rtweet)\n\nWarning: package 'rtweet' was built under R version 4.1.2\n\nauth_setup_default()\n\nUsing default authentication available.\nReading auth from '/Users/lpoirier_1/Library/Preferences/org.R-project.R/R/rtweet/default.rds'\n\nrt <- search_tweets('\"Smith College\"', n = 1000, include_rts = FALSE)\nusers <- users_data(rt)\nin_reply <- lookup_tweets(rt$in_reply_to_status_id)\nusers_reply <- users_data(in_reply)\n\nuseRs <- search_users(\"#smithies\", n = 200)\n\nuseRs_twt <- tweets_data(useRs)\n\nR_foundation_flw <- get_followers(\"lindsaypoirier\", n = 30000, \n                                  retryonratelimit = TRUE)\nR_foundation_flw_data <- lookup_users(head(R_foundation_flw$from_id, 100), verbose = FALSE)\nR_foundation_tweets <- get_timelines(head(R_foundation_flw$from_id, 10))\n\nWarning: `get_timelines()` was deprecated in rtweet 1.0.0.\nPlease use `get_timeline()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\n…\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\n… If you finish early, I encourage you to attempt to plot some of this data below using ggplot()!"
  },
  {
    "objectID": "lab5.html",
    "href": "lab5.html",
    "title": "Lab 5: Exploratory Data Analysis",
    "section": "",
    "text": "In this lab, you will apply the 5 data wrangling verbs we learned this week in order to analyze data regarding NYPD stop, question, and frisk. Specifically, we will replicate data analysis performed by the NYCLU in 2011 to demonstrate how the practice was being carried out unconstitutionally in New York.\n\n\n\nAnswer questions with data using data wrangling verbs\nConsider the implications of racial categorization"
  },
  {
    "objectID": "lab5.html#review-of-key-terms",
    "href": "lab5.html#review-of-key-terms",
    "title": "Lab 5: Exploratory Data Analysis",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\n\n\n\n\n\nTip\n\n\n\nYou may wish to reference this Data Wrangling Cheatsheet when completing this lab.\n\n\n\nfilter\n\nfilters to rows (observations) that meet a certain criteria\n\nselect\n\nkeeps only selected variables (columns)\n\narrange\n\nsorts values in a variable (column)\n\nsummarize\n\ncalculates a single value by performing an operation across a variable (column); summarizing by n() calculates the number of observations in the column\n\ngroup_by\n\ngroups rows (observations) by shared values in a variable (column); when paired with summarize(), performs an operation in each group\n\nmutate\n\ncreates a new variable (column) and assigns values according to criteria we provide"
  },
  {
    "objectID": "lab5.html#stop-question-and-frisk-dataset",
    "href": "lab5.html#stop-question-and-frisk-dataset",
    "title": "Lab 5: Exploratory Data Analysis",
    "section": "Stop, Question, and Frisk Dataset",
    "text": "Stop, Question, and Frisk Dataset\nIn 1968, the Supreme Court case Terry v. Ohio ruled that a police officer could stop an individual without “probable cause” but with “reasonable suspicion” the suspect had committed a crime. Further, an officer could frisk an individual without “probable cause” but with “reasonable suspicion” the suspect was carrying a weapon.\nIn the early 1990s, stop, question, and frisk became a widely prevalent tactic in policing in NYC. Every time an officer stops a civilian in NYC, the officer is supposed to complete a UF-250 form (see image below) outlining the details of the stop.\n\n\n\nUF250 Side 2, Dan Nguyen’s Blog\n\n\n\n\n\nUF250 Side 2, Dan Nguyen’s Blog\n\n\nAs a result of some high profile shootings in the late 1990s and mid-2000s, both the New York State Attorney General’s Office and the New York Civil Liberties Union began to examine NYPD stop and frisk activity for racial profiling. With pressure from these organizations, in the mid-2000s, information recorded on UF-250 forms began getting reported in public databases. In the early 2010s, when the NYPD’s use of stop and frisk went before the US District Court, this data was integral in proving that stop and frisk was being carried out in NYC in an unconstitutional way. The ruling mandated that the NYPD create a policy outlining when stops were authorized, and since the practice has declined considerably. See\n(Smith 2018) and (Southall and Gold 2019) for more information.\n\nData Dictionary\nWe will analyze data for all stops in 2011. This data can be found here. However, we’re only going to be considering a subset of the columns. Here is a data dictionary for the columns we’re using in this analysis.\n\n\n\n\n\n\n\n\nVARIABLE\nDEFINITION\nPOSSIBLE VALUES\n\n\n\n\npct\nPrecinct of the stop\n1:123\n\n\narrestsum\nWas an arrest made or summons issued?\n1 = Yes\n0 = No\n\n\nfrisked\nWas suspect frisked?\n1 = Yes\n0 = No\n\n\nwpnfound\nWas a weapon found on suspect?\n1 = Yes\n0 = No\n\n\nrace_cat\nSuspect’s race\nAMERICAN INDIAN/ALASKAN NATIVE\nASIAN/PACIFIC ISLANDER\nBLACK\nBLACK-HISPANIC\nOTHER\nWHITE\nWHITE-HISPANIC\n\n\nage\nSuspect’s age\n999 indicates a missing value"
  },
  {
    "objectID": "lab5.html#setting-up-your-environment",
    "href": "lab5.html#setting-up-your-environment",
    "title": "Lab 5: Exploratory Data Analysis",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nlibrary(tidyverse)\n\nsqf_url <- \"https://www1.nyc.gov/assets/nypd/downloads/zip/analysis_and_planning/stop-question-frisk/sqf-2011-csv.zip\"\ntemp <- tempfile()\ndownload.file(sqf_url, temp)\nsqf_zip <- unzip(temp, \"2011.csv\")\nsqf_2011 <- read.csv(sqf_zip, stringsAsFactors = FALSE) \nsqf_2011_race_cat <- read.csv(\"https://raw.githubusercontent.com/lindsaypoirier/STS-101/master/Data/SQF/sqf_race_categories.csv\", stringsAsFactors = FALSE) \nrm(sqf_url)\nrm(temp)\nrm(sqf_zip)\n\n\nPrepare Dataset for Analysis\nThe original dataset codes each race with a single letter. This adds a column called race_cat that writes out each racial category in accordance with the data documentation. It also replaces every instance of “Y” in the dataset with 1 and every instance of “N” with 0. This will allow us to sum the Yes’s in the dataset. You’ll learn more about how this code works in coming weeks.\n\nsqf_2011 <- \n  sqf_2011 %>% \n  select(pct, race, age, frisked, pistol, riflshot, asltweap, knifcuti, machgun, othrweap, sumissue, arstmade) %>%\n  left_join(sqf_2011_race_cat, by = \"race\") %>%\n  mutate(across(frisked:arstmade, \n         ~ case_when(. == \"Y\" ~ 1, . == \"N\" ~ 0)))\nrm(sqf_2011_race_cat)\n\nThe original dataset had separate variables for indicating whether a pistol, rifle, assault weapon, knife, machine gun, or other weapon was found on a suspect. We will create a variable equal to 1 if any of these weapons were found on the suspect. Further, the original dataset had separate variables for indicating whether a stop resulted in an arrest made or summons issued. We will create a variable equal to 1 if either occurred.\n\n\n\n\n\n\nExercise 1\n\n\n\nAdd new columns indicating 1) whether a weapon was found or 2) an arrest/summons was made.\n\n\n\nsqf_2011 <- \n  sqf_2011 %>%\n  #Add a variable for weapon found\n  _____(wpnfound = case_when(pistol == 1 |\n                               riflshot == 1 | \n                               asltweap == 1 |\n                               knifcuti == 1 | \n                               machgun == 1 | \n                               othrweap == 1 ~ 1,\n                             TRUE ~ 0))\nsqf_2011 <- \n  sqf_2011 %>%\n  #Add a variable for arrest made or summons issued\n  _____(arrestsumm = case_when(sumissue == 1 | \n                                arstmade == 1 ~ 1,\n                               TRUE ~ 0))\n\n\n\n\n\n\n\nExercise 2\n\n\n\nSubset the dataset to the six variables listed in the data dictionary above.\n\n\n\nsqf_2011 <-\n  sqf_2011 %>%\n  _____(pct, arrestsumm, _____, wpnfound, race_cat, _____)"
  },
  {
    "objectID": "lab5.html#analysis",
    "href": "lab5.html#analysis",
    "title": "Lab 5: Exploratory Data Analysis",
    "section": "Analysis",
    "text": "Analysis\n\n\n\n\n\n\nExercise 3\n\n\n\nCalculate the number of stops in 2011. Hint: Keep in mind that every row in the dataset represents one stop.\n\n\n\n\n\n\n\n\nTip\n\n\n\nsummarize() will return a data frame with the summarized value. Remember a data frame is a two-dimensional table with rows and columns (even if it’s a 1x1 table). If we want just the value, we can call pull() to extract the value from that two–dimensional table.\n\n\n\ntotal_stops <-\n  sqf_2011 %>%\n  summarize(Count = _____) %>%\n  pull()\n\ntotal_stops\n\n\n\n\n\n\n\nExercise 4\n\n\n\nHow many stops did not result in an arrest or summons in 2011? What percentage of stops did not result in an arrest or summons?\n\n\n\nsqf_2011 %>%\n  #Subset to rows where suspect innocent\n  _____(arrestsumm _____ 0) %>% \n  #Calculate number of observations\n  _____(total_innocent = n(), \n            percent_innocent = _____ / total_stops * 100)\n\n\n\n\n\n\n\nExercise 5\n\n\n\nIn how many stops were the individuals aged 14-24? In what percentage of stops were the individuals aged 14-24?\n\n\n\nsqf_2011 %>%\n  #Subset to rows where suspect age 14-24\n  _____(age _____ 14 & age _____ 24) %>% \n  #Calculate number of observations and percentage of observations\n  _____(total_14_24 = _____, \n            percent_14_24 = n() / total_stops * 100)\n\nWhy doesn’t this match the values we see on the NYCLU website?\nNote the following from the NYCLU’s 2011 report on Stop, Question, and Frisk data:\n\n“In a negligible number of cases, race and age information is not recorded in the database. Throughout this report, percentages of race and age are percentages of those cases where race and age are recorded, not of all stops.”\n\n\ntotal_stops_age_recorded <-\n  sqf_2011 %>%\n  #Subset to rows where age is not 999\n  _____(age _____ 999) %>% \n  summarize(Count = n()) %>%\n  pull()\n\nsqf_2011_sub %>%\n  filter(age >= 14 & age <= 24) %>%\n  summarize(total_14_24 = n(), \n            percent_14_24 = n() / total_stops_age_recorded * 100)\n\nThis still doesn’t match the values we see on the website, but it does match the values we see in the NYCLU’s 2011 report on Stop, Question, and Frisk data. This is typically when I would reach out to a representative at the NYCLU to inquire about the discrepancy.\n\n\n\n\n\n\nExercise 6\n\n\n\nHow many stops were there per race in 2011? What percentage of stops per race in 2011? Arrange by number of stops in descending order.\n\n\n\ntotal_stops_race_recorded <-\n  sqf_2011 %>%\n  #Subset to rows where race_cat is not NA or \"OTHER\"\n  filter(_____(race_cat) & race_cat _____ \"OTHER\") %>% \n  summarize(Count = n()) %>%\n  pull()\n\nsqf_2011 %>%\n  #Subset to rows where race_cat is not NA or \"OTHER\"\n  _____(_____(race_cat) & race_cat _____ \"OTHER\") %>% \n  #Group by race\n  _____(race_cat) %>% \n  #Calculate number of observations\n  _____(stops = n(), \n            percent_stops = n() / total_stops_race_recorded * 100) %>%\n  #Sort by stops in descending order\n  _____(_____(stops)) \n\nNote how this dataset categorizes race. Many different government datasets categorize race in many different ways. This, for instance, is not how the US Census categorizes race. How we categorize race matters for how we can talk about discrimination and racial profiling. Imagine if WHITE was not categorized separately from WHITE-HISPANIC. The values in this dataset would appear very differently! The NYCLU chose to aggregate two racial categories in this dataset into the one category - Latino - in order to advance certain claims regarding discrimination. What we should remember is that these racial categories are not reported by those stopped; they are recorded by officers stopping individuals. They may not reflect how individuals identify themselves.\n\nsqf_2011 %>%\n  #Subset to rows where race_cat is \"WHITE-HISPANIC\" or \"BLACK-HISPANIC\"\n  _____(race_cat _____ c(\"WHITE-HISPANIC\", \"BLACK-HISPANIC\")) %>% \n  #Calculate number of observations\n  _____(stops_Latinx = _____, \n            percent_Latinx = n() / total_stops_race_recorded * 100)\n\n\n\n\n\n\n\nExercise 7\n\n\n\nWhat percentage of stops in 2011 resulted in a frisk per race? What percentage of stops in 2011 resulted in a weapon found per race? What percentage of stops in 2011 resulted in an arrest or summons per race? In your resulting data table, each row should be a race, and there should be columns for stops, percent_stops, percent_frisked, percent_wpnfound , and percent_arrestsum.\n\n\n\n# Write code here. \n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nIn her ruling in the 2011 District Court case Floyd vs. the City of New York, Honorable Shira Sheidlen remarked the following regarding the database of stops we analyzed today:\n\nBecause it is impossible to individually analyze each of those stops, plaintiffs’ case was based on the imperfect information contained in the NYPD’s database of forms (“UF-250s”) that officers are required to prepare after each stop. The central flaws in this database all skew toward underestimating the number of unconstitutional stops that occur: the database is incomplete, in that officers do not prepare a UF-250 for every stop they make; it is one-sided, in that the UF250 only records the officer’s version of the story; the UF-250 permits the officer to merely check a series of boxes, rather than requiring the officer to explain the basis for her suspicion; and many of the boxes on the form are inherently subjective and vague (such as “furtive movements”). Nonetheless, the analysis of the UF-250 database reveals that at least 200,000 stops were made without reasonable suspicion.\n\nWhat are some of the consequences of the incompleteness and/or one-sided nature of this dataset? How should we think about and communicate its flaws vs its value? How might this data collection program be redesigned so as to represent more diverse perspectives? Share your ideas on our sds-192-discussions Slack channel.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you finish early, I encourage you to attempt to plot some of this data below using ggplot()!"
  },
  {
    "objectID": "problem-solving.html",
    "href": "problem-solving.html",
    "title": "Problem Solving",
    "section": "",
    "text": "This lab will introduce you to resources and techniques for problem solving in R. You should reference this lab often throughout the semester for reminders on best practices for addressing errors and getting help.\n\n\n\nInterpret error messages in R\nRead R cheatsheets\nAccess R help pages\nReference Stack Overflow and other online resources for help"
  },
  {
    "objectID": "problem-solving.html#interpreting-error-messages",
    "href": "problem-solving.html#interpreting-error-messages",
    "title": "Problem Solving",
    "section": "Interpreting Error Messages",
    "text": "Interpreting Error Messages\nThroughout this week, we have taken a look at different error messages that R presents when it can’t evaluate our code. Today, we will consider these in more detail. First, it’s important to make some distinctions between the kinds of messages that R presents to us when attempting to run code:\n\nErrors\n\nTerminate a process that we are trying to run in R. They arise when it is not possible for R to continue evaluating a function.\n\nWarnings\n\nDon’t terminate a process but are meant to warn us that there may be an issue with our code and its output. They arise when R recognizes potential problems with the code we’ve supplied.\n\nMessages\n\nAlso don’t terminate a process and don’t necessarily indicate a problem but simply provide us with more potentially helpful information about the code we’ve supplied.\n\n\nCheck out the differences between an error and a warning in R by reviewing the output in the Console when you run the following code chunks.\n\nError in R\n\nsum(\"3\", \"4\")\n\nError in sum(\"3\", \"4\"): invalid 'type' (character) of argument\n\n\n\n\nWarning in R\n\nvector1 <- 1:5\nvector2 <- 3:6\nvector1 + vector2\n\nWarning in vector1 + vector2: longer object length is not a multiple of shorter\nobject length\n\n\n[1]  4  6  8 10  8\n\n\nSo what should you do when you get an error message? How should you interpret it? Luckily, there are some clues and standardized components of the message the indicate why R can’t execute the code. Consider the following error message that you received when running the code above:\nError in sum(“3”, “4”) : invalid ‘type’ (character) of argument\nThere are three things we should pay attention to in this message:\n\nThe word “Error” indicates that this code did not run.\nThe text immediately after the word “in” tells us which specific function did not run.\nThe text after the colon gives us clues as why the code did not run.\n\nReviewing the error above, I can guess that there was a problem with the argument that I supplied to the sum() function, and specifically that I supplied a function of the wrong type.\n\n\n\n\n\n\nExercise 1\n\n\n\nRun the codes below and check out the error messages. Review the code to fix each of the errors. Note that each subsequent code chunk relies on the previous code chunk, so you will need to fix the errors in order and run the chunks in order.\n\n\n\n# Create three vectors\na <- 1, 2, 3, 4, 5\nb <- \"a\", \"b\", \"c\", \"d\", \"e\"\nc <- TRUE, FALSE, TRUE, TRUE, FALSE\n\nError: <text>:2:7: unexpected ','\n1: # Create three vectors\n2: a <- 1,\n         ^\n\n\n\n# Add the values in the vector a\na_added <- add(a)\n\nError in add(a): could not find function \"add\"\n\n\n\n# Multiply the previous output by 3\nthree_times_a_added <- added_a * 3\n\nError in eval(expr, envir, enclos): object 'added_a' not found\n\n\n\n# Create a dataframe with col1 and col2\ndf <- data.frame(\n  col1 = c(1, 2, 3)\n  col2 = c(\"a\", \"b\", \"c\")\n\nError: <text>:4:3: unexpected symbol\n3:   col1 = c(1, 2, 3)\n4:   col2\n     ^\n\n\n\n# Add a new column to df\ndf$col3 <- c(TRUE, FALSE)\n\nError in df$col3 <- c(TRUE, FALSE): object of type 'closure' is not subsettable"
  },
  {
    "objectID": "problem-solving.html#preparing-to-get-help",
    "href": "problem-solving.html#preparing-to-get-help",
    "title": "Problem Solving",
    "section": "Preparing to Get Help",
    "text": "Preparing to Get Help\nWhen we do get errors in our code and need to ask for help in interpreting them, it’s important to provide collaborators with the information they need to help us. Sometimes when teaching R I will hear things like: “My code doesn’t work!” or “I’m stuck and don’t know what to do,” and it can be challenging to suss out the root of the issue without more information. Here are some strategies for describing issues you are having with your code:\n\nReference line numbers. Notice the left side of this document has a series of numbers listed vertically next to each line? These are known as line numbers. Oftentimes, if you are having an issue with your code and ask me to review it, I will say something like: “Check out line 53.” By this I mean that you should scroll the document to the 53rd line. You can similarly tell me or your peers which line of your code you are struggling with.\nCompose good reproducible examples. A good reproducible example includes all of the lines of code that we need to reproduce an output on our own machines. This means that if you create a vector in a previous code snippet and then supply it as an argument in another code snippet, you are going to want to make sure that both of these lines of code appear in your reproducible example. Further, if the functions that you are using are from certain packages, you will want to make sure the library() call to load that package is in your reproducible example.\nUse the code and code block buttons in Slack to share example code. First, when we copy and paste code from RStudio into programs like Slack and email, we can’t see the output. Second, certain characters like quotation marks and apostrophes are treated differently across these programs. For example, run the code chunk below and check out the output in your Console. The first line of code I typed directly into RStudio. The second I copied over from Slack.\n\n\n# typed directly into RStudio\ntoupper(\"apple\")\n# copied from Slack\ntoupper(“apple”)\n\nError: <text>:4:9: unexpected input\n3: # copied from Slack\n4: toupper(“\n           ^\n\n\nNotice the slight differences in the quotation marks? R recognizes the first but doesn’t recognize the second, even though I used the same keyboard key to create both. This is due to the fact that these two systems use different character encodings.\nThe Code button (for a single line of code) and Code Block button (for multiple lines of code) in Slack are useful tools for composing code and avoiding character encoding issues. If you click these buttons when typing a Slack message, you can enter code in the red outlined box that appears, and this will easily copy to RStudio. I will ask you to always use these features when copying code this semester.\n\n\n\n\n\n\nTip\n\n\n\nIn Slack, you can also wrap text backticks (` `) to have it output in a single-line code block, and three backticks (``` ```) to have it output as a multi-line code block.\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nCopy and paste one line of code from this lab as a threaded comment on the Slack message I posted on September 7, 2022. Be sure the text is formatted as code."
  },
  {
    "objectID": "problem-solving.html#help-pages",
    "href": "problem-solving.html#help-pages",
    "title": "Problem Solving",
    "section": "Help pages",
    "text": "Help pages\nOne resource we’ve already discussed are the R help pages. I tend to use the help pages when I know the function I need to use, but can’t remember how to apply it or what its parameters are. Help pages typically include a description of the function, its arguments, details about the function, the values it produces, a list of related functions, and examples of its use. We can access the help pages for a function by typing the name of the function with a question mark in front of it into our Console (e.g. ?log or ?sum). Some help pages are well-written and include helpful examples, while others are spotty and don’t include many examples.\n\n\n\n\n\n\nExercise 3\n\n\n\nAccess the help pages for the function sort(). Write code below to sort the vector a (which you created in an earlier step) in decreasing order.\n\n\n\n# Write code here."
  },
  {
    "objectID": "problem-solving.html#cheatsheets",
    "href": "problem-solving.html#cheatsheets",
    "title": "Problem Solving",
    "section": "Cheatsheets",
    "text": "Cheatsheets\nThe R community has developed a series of cheatsheets that list the functions made available through a particular package and their arguments. I tend to use cheatsheets when I know what I need to do to a dataset in R, but I can’t recall the function that enables me to do it.\n\n\n\n\n\n\nExercise 4\n\n\n\nNavigate to this cheatsheet for base R. Imagine we collected the temperature of our home each day for the past ten days (see code below). Let’s say we wanted to find how each day ranked from coolest to hottest. For instance, I wanted to know the ranking for day 1 vs. day 2 vs. day 3, and so on. Using the cheatsheet, find the function that will allow you to generate a ranking of each day’s temperature. Search the help pages for this function to determine how to randomly rank two days with the same temperature. Write a comment to yourself describing how this function is different than sorting the data.\n\n\n\n#Create a vector of temperatures\ntemps_to_factor <- c(68, 70, 78, 75, 69, 80, 66, 66, 79)\n\n# Write code below to rank the days with random ties\n\n\n# Replace this line with a comment to yourself describing how this function is different than sorting the data."
  },
  {
    "objectID": "grading_contract.html",
    "href": "grading_contract.html",
    "title": "Assessment",
    "section": "",
    "text": "This course will be graded using a standards-based assessment system. In a more traditional grading system, your scores on a series of assignments are averaged over the course of the semester. In this course, rather than assessing and averaging your achievement on particular assignments, I will instead be assessing the development of your fluency in a set of pre-defined standards. You will have multiple opportunities over the course of the semester to showcase the depth of your understanding regarding these standards. A standards-based grading system carries the following benefits:\nThis is my second iteration of teaching a standards-based course, and I’ve made several revisions from the first iteration based on what I’ve learned from experience and student feedback. I’m excited to refine this system this semester as I believe it aligns with my overarching goals for the course. In this course, I find it far more important that you come away with an understanding of the concepts behind core data science strategies (along with an ability to find and interpret reference materials) than it is to demonstrate memorization of R syntax. Developing this understanding will empower you to learn and apply new data science languages on your own."
  },
  {
    "objectID": "grading_contract.html#what-are-the-standards-i-will-be-assessed-on-in-this-course",
    "href": "grading_contract.html#what-are-the-standards-i-will-be-assessed-on-in-this-course",
    "title": "Assessment",
    "section": "What are the standards I will be assessed on in this course?",
    "text": "What are the standards I will be assessed on in this course?\n\n\n\n\n\n\nData Visualization\n\n\n\n\nThis dimension refers to the development of your ability to produce multiple types of compelling and well-designed visualizations from data.\n\nVISUALIZATION AESTHETICS PLOTTING FREQUENCIES AND DISTRIBUTIONS MAPPING\n\n\n\n\n\n\n\n\nData Wrangling\n\n\n\n\nThis dimension refers to the development of your ability to transform datasets into new formats in order to prepare them for further analysis or visualization.\n\nTRANSFORMING DATA JOINING DATASETS TIDYING DATA\n\n\n\n\n\n\n\n\nData Science Workflow\n\n\n\n\nThis dimension refers to the development of your ability to apply data science best practices in your work.\n\nUNDERSTANDING DATASETS DATA RETRIEVAL GITHUB PROGRAMMING IN R\n\n\n\n\n\n\n\n\nData Ethics\n\n\n\n\nThis dimension refers to your ability to recognize and navigate ethical dilemmas that emerge in data science work. There is not a formal standard for data ethics because issues related to data ethics will be discussed in every unit of the course, and you will be assessed on data ethics issues related to other standards in quizzes and projects."
  },
  {
    "objectID": "grading_contract.html#how-will-i-be-assessed-on-the-course-standards",
    "href": "grading_contract.html#how-will-i-be-assessed-on-the-course-standards",
    "title": "Assessment",
    "section": "How will I be assessed on the course standards?",
    "text": "How will I be assessed on the course standards?\n\nInformal Assessments\n\nReadingsLab Recaps\n\n\nEach week, you will be assigned a section of the course texts to read prior to class. I expect that you will come having read this section in order to prepare for in-class exercise and labs. You do not need to complete the exercises in the course texts but may choose to do so if you wish. Please note however that we won’t have time to go over the solutions in class, and I don’t have a solutions manual for these texts (though I’m happy to go over them in office hours). All course readings will be available in Perusall, and you can post questions and comments in the reading for myself or your classmates to answer.\n\n\nLab solutions will be posted as video lectures in Perusall. I expect that you will check your lab answers by reviewing these videos. You may leave comments in Perusall at certain timestamps as questions come up.\n\n\n\n\n\nFormal Assessments\n\n\n\n\n\n\nTip\n\n\n\n5PM (close of business day) will be the cut-off time for all assignments in this course. The reason I’ve set the deadlines to 5PM is that I’d like to discourage students from staying up late into the night to complete assignments. Note that there is a 24-hour grace period for submitting labs and project assignments. This means that you will still get full credit for these assignments as long as they are submitted by 5PM the day following the assignment due date. However, quizzes and course advancement assignments must be submitted by 5PM on the due date for credit.\n\n\n\nLabsProjectsQuizzesCourse Advancement\n\n\nIn most weeks, you will be assigned a lab, which you will start in class and complete at home. There will be one lab per standard. Labs will be designed to help you practice applying the course standards towards the analysis of a dataset. You may work on labs in groups, but all group members should submit their own lab. Labs will be graded for completion, and you can earn 3 points per standard by completing the lab associated with that standard. All sections of the lab must be completed in good faith to earn these points.\n\n\nThere will be 3 projects, to be completed in groups of 3-4, assigned over the course of the semester. In each you will have an opportunity to demonstrate fluency in standards we have covered up to that point in the semester. I will provide prompts for each project, but you will have a lot of flexibility to demonstrate your own creativity and explore your own interests in designing a project around the prompt. You can earn up to 3 points towards 9 of the ten standards based on your project submission. If you don’t earn full credit on a standard for a project submission, you may improve your score on that standard in the next project. The only standard that won’t be covered in projects is Data Retrieval, which we will cover too late in the semester to work into projects. Projects will be graded for fluency.\n\n\nThere will be 3 quizzes administered throughout the semester - each assessing 3-4 course standards. There will be 3 questions per standard on each quiz, and you can earn up to 3 points towards each standard based on your quiz attempt: 1 point per question. In this sense, quizzes will be graded for fluency.\nQuizzes will be taken at home, administered in Moodle, and are open book/open Internet. You may start a quiz at any time before its due date, but it must be completed by its due date in order to earn credit. Please note that extensions will not be granted for quizzes.\n\n\nThere are a series of very short assignments on the syllabus that are designed to ensure that you are prepared for individual and collaborative work. This includes things like reviewing the syllabus, developing a course study plan, developing group collaboration plans, and completing peer evaluations. In total, there are 13 course advancement assignments, and you can earn 1 point towards your final grade per assignment.\n\n\n\nAssignment\nPoints\n\n\n\n\nSyllabus Quiz\n1\n\n\nCATME Survey\n1\n\n\nProblem Solving Lab\n2\n\n\nStudy Plan\n2\n\n\nStudy Plan Evaluation\n2\n\n\nGroup Contract (x3)\n3\n\n\n\nThese assignments will be graded for completion. For the most part, you will get out of them, what you put into them. Because these assignments are designed to keep our course running smoothly, please note that extensions will not be granted for course advancement assignments.\n\n\n\n\n\n\n\n\n\nReassessment\n\n\n\nThis course will have an optional final exam. The final exam is an opportunity to reassess standards that you have not received full quiz credit on. If you choose to take the final exam, you only need to complete the sections of the exam associated with standards you wish to reassess, and the score you ultimately receive for a standard will be based on whichever is higher of your quiz score or your final exam score for that standard."
  },
  {
    "objectID": "grading_contract.html#how-will-this-system-work",
    "href": "grading_contract.html#how-will-this-system-work",
    "title": "Assessment",
    "section": "How will this system work?",
    "text": "How will this system work?\nIf you’ve been adding, you may have figured out by this point that we have:\n\\((3 * 10) + (3 * 10) + (3 * 9) = 87\\)\nYou can earn the remaining 13 points by completing the course advancement assignments, for a grand total of 100 points. At the end of the semester, I will sum your scores on all standards and other assignments and assign final grades accordingly:\n\n\n\nLetter Grade\nNumeric Grade\n\n\n\n\nA\n≥ 92.5\n\n\nA-\n≥ 90.0\n\n\nB+\n≥ 87.5\n\n\nB\n≥ 82.5\n\n\nB-\n≥ 80.0\n\n\nC+\n≥ 77.5\n\n\nC\n≥ 72.5\n\n\nC-\n≥ 70.0\n\n\nD+\n≥ 67.5\n\n\nD\n≥ 62.5\n\n\nD-\n≥ 60.0\n\n\nE\n< 60.0"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SDS 192: Introduction to Data Science",
    "section": "",
    "text": "Data science involves applying a set of strategies to transform a recorded set of values into something from which we can glean knowledge and insight. This course will introduce you to concepts and methods from the field of data science, along with how to apply them in R. You will learn how to acquire, clean, wrangle, and visualize data. You will also learn best practices in data science workflows, such as code documentation and version control. Issues in data ethics will be addressed throughout the course.\nClasses will be held on Mondays, Wednesdays, and Fridays from 9:25 AM to 10:40 AM.\nThere are no prerequisites, but a willingness to write code is necessary. Coding for the first time can be intimidating, but I intend do everything in my power to support you through the learning curve and to make things both fun and relevant in the process. I personally picked up most of my data science skills through a lot of trial-and-error, practice, and curiosity. My hope is that, in this course, you will learn through experimentation, along with independent and collaborative problem-solving. Honing these competencies will serve you as you move on to other courses in the SDS program and/or at Smith.\n\n\n\n\n\n\n\n\nLindsay Poirier, she/her/hers.\n\n\n\n\n\nWhile you’re welcome to refer to me as Professor Poirier, I would prefer it if you called me Lindsay. I am a cultural anthropologist that studies how public interest datasets get produced, how communities think about and interface with data, and how data infrastructure can be designed more equitably. My Ph.D. is in an interdisciplinary discipline called Science and Technology Studies - a field that studies the intricate ways science, technology, culture, and politics all co-constitute each other. I work on a number of collaborative research projects that leverage public data to deepen understanding of social and environmental inequities in the US, while also qualitatively studying the politics behind data gaps and inconsistencies. As an instructor, I prioritize active learning and often structure courses as flipped classrooms. You can expect in-class time to predominantly involve a mix of lectures and live problem-solving exercises.\n\n\n\nSlackOffice Hours\n\n\nI can best support students in this course when I can readily keep tabs on our course-related communication. Because of this, I ask that you please don’t email me regarding course-related questions or issues. The best way to get in touch with me is via our course Slack. If you have course-related questions, I encourage you to ask them in the #sds-192-questions channel. When discretion is needed, feel free to DM. Please reserve more formal concerns like grades or accommodation requests for an in-person (or in-person virtual) conversation.\nDuring the week, I will try my best to answer all Slack messages within 24 hours of receiving them. Please note that to maintain my own work-life balance, I often don’t answer Slack messages late in the evenings or on the weekends. It’s important that you plan when you start your assignments accordingly.\n\n\nOffice hours are a great opportunity for us to chat about what you’re learning in the course, clarify expectations on assignments, and review work in progress. I also love when students drop in to office hours to request book recommendations, discuss career or research paths, or just to say hi! I encourage each student in the course to join office hours at least once this semester. If you’re unable to attend my office hours at the regularly scheduled time, there is link on Moodle to book a meeting with me.\n\nMonday, 3-4, McConnell 212-213\nWednesday, 3-4, McConnell 212-213\n\n\n\n\n\n\n\n\nA number of excellent textbooks introducing data science concepts and methods have been written in the past few years, including a few from faculty in the Smith SDS department. To accompany the topics we will cover each week, I will be selecting my favorite chapters from these books and posting them to Perusall. However, all three books we will engage in this course cover almost every topic we will address, so feel free to supplement your reading with corresponding chapters in the other books: especially if you find yourself drawn to the teaching and writing style in a certain book. All books are available for free online.\n\nBaumer, Benjamin S., Daniel T. Kaplan, and Nicholas J. Horton. 2021. Modern Data Science with R. 2nd ed. CRC Press. https://mdsr-book.github.io/mdsr2e/.\nIrizarry, Rafael A. 2022. Introduction to Data Science. Data Analysis and Prediction Algorithms with R. https://rafalab.github.io/dsbook/.\nIsmay, Chester, and Albert Y. Kim. 2021. Modern Dive: Statistical Inference via Data Science. CRC Press. https://moderndive.com/.\n\nEach week I will also list optional reading and resources in our course schedule that you may reference if you are struggling with a topic or if you wish to explore that topic further. I will update this list often throughout the semester.\n\n\n\nThis course will be graded via a standards-based assessment system.\n\n\n\n\n\n\nSpinelli Center\n\n\n\nSmith’s Spinelli Center offers a number of resources to support SDS students. Spinelli Center Data Assistants will visit our classroom regularly to support you through lab work. The Center also offers drop-in tutoring hours Sunday through Thursday 7-9 PM. Finally, you can drop-in or schedule an appointment with the Data Research and Statistics Counselor (Osman Keshawarz). To schedule an appointment, email qlctutor@smith.edu.\n\n\n\n\n\n\nPreparationAttendanceExtensionsAcademic Honesty\n\n\nThis is a 4-credit course with 3 hours per week of in-classroom instructions. Smith expects students to devote 9 out-of-class hours per week to 4-credit classes. I have designed the course assignments and selected the course readings with this target in mind.\n\n\nAttending class is not only important for your learning but also an act of community. Attendance is expected in this course. Many course assignments will be completed in-class. That said, you do not need to inform me when you will be absent. If you are sick, please stay home. If you must miss a class entirely, you should contact a peer to discuss what was missed.\n\n\nThere is an automatic 24-hour grace period on all lab and project assignments. There will be no penalties for submitting the project within this 24-hour period, and you do not need to inform me that you intend to take the extra time. You can also request up to a 72-hour extension on any project or lab assignment, as long as you make that request at least 48 hours before the original assignment due date. You can request an extension by filling out the Extension Request form on Moodle, and I will confirm your extension on Slack. Beyond this, late assignments will not be accepted.\n\n\n\nSmith College expects all students to be honest and committed to the principles of academic and intellectual integrity in their preparation and submission of course work and examinations. Students and faculty at Smith are part of an academic community defined by its commitment to scholarship, which depends on scrupulous and attentive acknowledgement of all sources of information, and honest and respectful use of college resources.\n\nAny cases of dishonesty or plagiarism will be reported to the Academic Honor Board. Examples of dishonesty or plagiarism include:\n\nSubmitting work completed by another student as your own.\nCopying and pasting words from sources without quoting and citing the author.\nParaphrasing material from another source without citing the author.\nFailing to cite your sources correctly.\nFalsifying or misrepresenting information in submitted work.\nPaying another student or service to complete assignments for you.\n\n\n\n\n\n\n\n\n\n\nDeadlines for Quizzess and Course Advancement Assignments\n\n\n\nThe standards you will be practicing in this course all build off of each other, and it’s important that I know how students are doing on each standard in order to direct my teaching going forward. Because of this, the deadlines for quizzes are a bit more firm than for other assignments. On the course schedule and in Moodle you will see a suggested deadline for quizzes and a final deadline. I won’t be able to accept quiz submissions after the final deadline, so you’ll want to be sure to stay on top of these dates. Similarly, course advancements assignments are assignments that need to be completed to keep our course moving. Because of this, they need to be completed by the due date. I will give you an opportunity to work on many of these assignments in class.\n\n\n\n\n\n\nCode of ConductPrinciples of CommunityPronouns\n\n\nAs the instructor for this course, I am committed to making participation in this course a harassment-free experience for everyone, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, or religion. Examples of unacceptable behavior by participants in this course include the use of sexual language or imagery, derogatory comments or personal attacks, trolling, public or private harassment, insults, or other unprofessional conduct.\nAs the instructor I have the right and responsibility to point out and stop behavior that is not aligned to this Code of Conduct. Participants who do not follow the Code of Conduct may be reprimanded for such behavior. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the instructor.\nAll students and the instructor are expected to adhere to this Code of Conduct in all settings for this course: seminars, office hours, and over Slack.\nThis Code of Conduct is adapted from the Contributor Covenant, version 1.0.0, available here.\n\n\nI hope that we can foster a collaborative and caring environment in this classroom: one that celebrates successes, respects individual strengths and weaknesses, demonstrates compassion for each other’s struggles, and affirms diverse identities. Here are some ideas that I have for creating this environment in our course:\n\nCheck-in with colleagues before starting collaborative work. “What three words describe how you’re feeling?” “Name one challenge and one success from this week.” “What are you doing for self-care right now?” Thank each other for sharing where they’re at.\nConsider when to step up and when to step back in class discussions, creating space for others to contribute. Listening is just as important to community-building as speaking.\nAcknowledge that there is much we don’t know about how our colleagues experience the world. …but don’t ask colleagues to speak on behalf of a social group you perceive them to be a part of.\nCheer on colleagues as they give presentations or try something out for the first time.\nAsk questions often in our #sds-192-questions channel. Help each other out by answering questions when you can.\nMistakes happen. I will certainly make mistakes in class. Admit mistakes, and then move on.\n\n\n\nUsing the proper pronouns for our students is foundational to a safe, respectful classroom environment that creates a culture of trust. For information on pronouns and usage, please see the Office of Equity and Inclusion link here: Pronouns\n\n\n\n\n\n\n\nAccommodationsStudent Well-beingTrigger Warnings\n\n\nIt is my goal for everyone to succeed in this course. If you have personal circumstances that may impact your experience of our classroom, I encourage you to contact Office of Disability Services in College Hall 104 or at ods@smith.edu. The Office will generate a letter that indicates to me what kind of support you need and how I can make your classroom experience more accommodating. Once you have this letter, you are welcome to visit my office hours or email me to discuss ideas about how we can tailor the course accordingly. While you can request accommodations at any time, the sooner we start this conversation, the better. If you have concerns about the course that are not addressed through ODS, please contact me. At no point will I ask you to divulge details about your personal circumstances to me.\n\n\nCollege life is stressful, and life outside of college can be overwhelming. It is my position that attending to your physical and mental health and well-being should be a top priority. I will remind you of this often throughout the semester. I encourage you to schedule a time to talk with me if you are struggling with this course. If you, or anyone you know, is experiencing distress, there are numerous campus resources that can provide support via the Schacht Center. I can point you to these resources at any time throughout the semester.\n\n\nA trigger is a topic or image that can precipitate an intense emotional response. When common triggering topics are to be covered in this course, I will do my best to provide a trigger warning in advance of the discussion. However, I can’t always anticipate triggers. With this in mind I’ve set up an anonymous form, available on Moodle, where you can indicate topics for which you would like me to provide a warning.\n\n\n\n\n\n\n\nMoodlePerusallSlackGitHub\n\n\nGrades, forms, handouts, and quizzes will be available on the course Moodle.\n\n\nAll course readings and recorded lectures will be available on Perusall. You can access Perusall via our course Moodle page.\n\n\n\n#general: Course announcements (only I can post)\n#sds-192-discussions: Share news articles and relevant opportunities\n#sds-192-questions: Ask and answer questions about our course\nYou can also create private Slack channels with your project group members.\n\n\n\nI will be using GitHub Classroom to distribute several course assignments, and you will submit assignments by pushing changes to template documents to a private GitHub repository. I will provide guidance on how to do this early in the semester.\n\n\n\n\n\n\n\n\n\nRStudio/RStudio Server\n\n\n\nThis class will use the R statistical software package. In the first week of the course, I will help you install and configure R and RStudio. If you are using a laptop, you will install both on that computer. If you are using a Chromebook or Tablet, an account will be created for you on the Smith College RStudio Server so that you can access a cloud-based version of RStudio. You should let me know in the first week of the course if you are using a Chromebook or tablet."
  },
  {
    "objectID": "lab3.html",
    "href": "lab3.html",
    "title": "Lab 3: Plotting Frequencies and Distributions",
    "section": "",
    "text": "The goal of this lab is to provide you with practice in producing data visualizations that help to answer a research question.\n\n\n\nProduce and interpret univariate plots\nProduce and interpret multivariate plots\nContextualize plots with descriptive labels and titles"
  },
  {
    "objectID": "lab3.html#review-of-key-terms",
    "href": "lab3.html#review-of-key-terms",
    "title": "Lab 3: Plotting Frequencies and Distributions",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\nMultivariate Plots\n\nPlots that summarize and visualize the distribution and relationship between multiple variables\n\nUnivariate Plots\n\nPlots that summarize and visualize the distribution of a single variable"
  },
  {
    "objectID": "lab3.html#spotify-dataset",
    "href": "lab3.html#spotify-dataset",
    "title": "Lab 3: Plotting Frequencies and Distributions",
    "section": "Spotify Dataset",
    "text": "Spotify Dataset\nToday, we are prioritizing joy! Our research question will be: How joyful are popular Spotify playlists in my favorite music genre?\nThe music feature from Spotify’s data that serves as a measure of joy is called valence. This is the description from their API documentation for valence:\n\nA measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n\n(Pretty vague if you ask me, but today we’ll go with it.)"
  },
  {
    "objectID": "lab3.html#setting-up-your-environment",
    "href": "lab3.html#setting-up-your-environment",
    "title": "Lab 3: Plotting Frequencies and Distributions",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\n\n\n\n\n\nTip\n\n\n\nToday’s set-up is a little more complicated than usual, so be sure to take it slow and ask questions as they come up!\n\n\n\nInstall the Spotify R package by entering the following into your Console: install.packages(\"spotifyr\")\nLog-in to Spotify’s Developer Dashboard here. If you have a Spotify account, you can log-in with that account. Otherwise, you should create one.\nClick the ‘Create an App’ button to create an app named “SDS 192 Class Project”. You can indicate that this is a “Project for SDS 192 class”\nClick Edit Settings. Under the heading Redirect URIs copy and paste this URL: http://localhost:1410/, and click Add. Scroll to the bottom of the window and click Save. This is going to allow us to authenticate our Spotify accounts through our local computers.\nClick the Users and Access button. Scroll down to Add New User, and add your name and the email address associated with your Spotify account. Click Add.\nClick “Show Client Secret”. Copy client id and secret below, and then run the code chunk.\n\n\nlibrary(spotifyr)\n# id <- 'FILL CLIENT ID HERE'\n# secret <- 'FILL CLIENT SECRET HERE'\n# Sys.setenv(SPOTIFY_CLIENT_ID = id)\n# Sys.setenv(SPOTIFY_CLIENT_SECRET = secret)\n\n\nBelow replace poiril with your Spotify username. This is the ID that appears in the upper right hand corner when you log into your Spotify account (not your developer account.)\nSearch Spotify for your favorite music genre and select three playlists from the search. Note that this code will only work for playlists, and playlists may be a ways down in the search results.\nWhen you click on a playlist, notice the URL in the navigation bar of your web browser. It should look something like spotify.com/playlist/LONG_STRING_OF_CHARACTERS. Copy the long string of characters at the end of the URL, and paste it into the function below. Your characters should replace the example playlist I’ve added: 7ryj1GwAWYUY36VQd4uXoq.\nRepeat this for the other two playlists, replacing my other examples. Then run the code.\n\n\nlibrary(tidyverse)\nspotify_playlists <- get_playlist_audio_features(\n  username = \"poiril\",\n  playlist_uris = c(\"7ryj1GwAWYUY36VQd4uXoq\",\n                    \"3DYUw0nHB9o8tLZKQup4zp\",\n                    \"37i9dQZF1DXdOEFt9ZX0dh\"),\n  authorization = get_spotify_access_token()\n) %>%\n  select(-c(track.artists, \n            track.available_markets, \n            track.album.artists, \n            track.album.available_markets, \n            track.album.images))"
  },
  {
    "objectID": "lab3.html#data-analysis",
    "href": "lab3.html#data-analysis",
    "title": "Lab 3: Plotting Frequencies and Distributions",
    "section": "Data Analysis",
    "text": "Data Analysis\nIn the exercises below, you will create a series of plots that will enable us to compare the joyfulness of the three playlists you’ve selected.\nI encourage you to take a look at the spotify_playlists data frame in your environment. You’ll note that each row in the dataset is a song/track from one of the the three playlists you selected (unique ID would be track.id), and columns provide information about that song (such as the track.name, the playlist it is a part of, its key, loudness, danceability, and acousticness). We can produce some pretty cool visualizations from this data. For instance check out how we might compare the relationship between the energy and acousticness of songs across the three selected playlists.\n\nspotify_playlists %>%\n  ggplot(aes(x = acousticness, y = energy)) +\n  geom_point(alpha = 0.5, size = 0.5) +\n  coord_flip() +\n  facet_wrap(vars(playlist_name)) +\n  labs(title = \"Acousticness and Energy of Songs in Classic Rock Spotify Playlists, 2022\",\n       x = \"Acousticness\", \n       y = \"Energy\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou might use this plot as a starting template for the plots you are going to create below!\n\n\nWe’re going to start our analysis with univariate plotting. Specifically, we are going to produce data visualizations that count the number of observations in a dataset that fall into specific groupings. When grouping observations by a categorical variable, we will produce a bar plot. When grouping observations into intervals of a numeric variable, we will produce a histogram. Remember when labeling that these plots visualize frequency.\n\n\n\n\n\n\nWarning\n\n\n\nKeep in mind that all plots should have 5 contextual details represented in titles or labels:\n\nThe data’s unit of observation\nThe variables represented\nAny filters applied\nThe geographic scope of the data\nThe temporal scope of the data\n\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\nHow many songs are in each playlist? Create a plot to visualize this, and order the results by the number of songs. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nExercise 2\n\n\n\nWhat is the distribution of valence across all of the songs (in intervals of 0.1 valence)? Create a histogram to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n\n# Create plot here\n\nLet’s move on to some multivariate plotting. Remember that we can add further variables to a plot via a number of different aesthetics (e.g. color: fill= or col=; size: size=; position: x= or y=, small multiples: + facet_wrap(vars(...)) ). Whenever we add further data to a plot, we should be on the lookout for overplotting.\n\n\n\n\n\n\nExercise 3\n\n\n\nWhat is the distribution of valence across all of the songs (in intervals of 0.1 valence) in each playlist? Create a histogram to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nExercise 4\n\n\n\nWhat are differences in the summary statistics (max, min, median, etc.) of the valence of songs in each playlist? Create grouped boxplot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nExercise 5\n\n\n\nDo happier songs tend to be more danceable in each playlist? Create a plot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context. Also be sure to adjust your plot to address overplotting.\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nExercise 6\n\n\n\nDo songs composed in the minor or major mode tend to be happier in each playlist? Create a plot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nExercise 7\n\n\n\nDo happier songs tend to have a higher tempo across all playlists? What role might the song’s mode play? Create a plot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context. Also be sure to adjust your plot to address overplotting.\n\n\n\n#Create boxplot here\n\n\n\n\n\n\n\nExercise 8\n\n\n\nBased on all of your plots, summarize what you learned about the joyfulness of these three playlists below.\n\n\n\n\n\n\n\n\nYour Response\n\n\n\nFill response here.\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nCheck out this article documenting ethical concerns regarding Spotify’s data collection practices. Should we be concerned about the assumptions that Spotify makes about us based on our music streaming habits? What about the way they curate music for us? What are some of the social consequences to this form of user surveillance? Share your ideas on our `sds-192-discussions` Slack channel."
  },
  {
    "objectID": "lab2.html",
    "href": "lab2.html",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "",
    "text": "Designing effective data visualizations involves reviewing available data and then determining how best to map variables in the data onto a variety of visual cues. When we refer to visual cues, we are referring to those visual components of the plot that help us discern differences across data points. For instance, a plot might use different shapes or colors to represent different categories of data. A plot might also place points at different positions or bars at different heights to represent different numeric values. This week we will practice mapping variables in a dataset onto different plot aesthetics in order to tell different stories with the data. We will only be creating one type of plot today - a scatterplot. However, we are going to show how we can use different visual cues to plot a number of different variables onto a scatterplot.\n\n\n\nRead the ggplot cheatsheets\nMap variables onto plot aeshetics\nAdjust the attributes of a plot\nAdjust the scales of aeshetics on plots\nDeal with overplotting\nFacet plots into small multiples"
  },
  {
    "objectID": "lab2.html#review-of-key-terms",
    "href": "lab2.html#review-of-key-terms",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\nAesthetics\n\nVisual cues that we map variables in a dataset onto\n\nCartesian grid\n\nA 2-dimensional grid with an intersecting x and y axis\n\nSequential color\n\nA uni-directional ordering of shades\n\nQualitative color\n\nA discrete set of colors\n\nDivergent color\n\nA diverging ordering of color shades\n\nOverplotting\n\nInstances when visual representations of individual data points overlap on a plot making aspects of the plot illegible"
  },
  {
    "objectID": "lab2.html#county-health-ranking-dataset",
    "href": "lab2.html#county-health-ranking-dataset",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "County Health Ranking Dataset",
    "text": "County Health Ranking Dataset\nToday we will be working with an incredible data resource from the University of Wisconsin Population Health Institute. It aggregates data from a number of government sources to produce county health indicators for every county in the US. Something to keep in mind when reviewing this data:\nNote that, while this is a powerful data source for visualizing health disparities, particularly in smaller counties, there can be large degrees of uncertainty in the reporting of certain health measures. We won’t be working with this dataset’s confidence intervals today, so we do want to consider that the visualizations that we see are not a perfect reflection of county health. Run the code below to load in the datasets we will be working with today."
  },
  {
    "objectID": "lab2.html#setting-up-your-environment",
    "href": "lab2.html#setting-up-your-environment",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nInstall the RColorBrewer package by entering the following into your R Console: install.packages(\"RColorBrewer\")\nRun the code below to the import the bridge inventory for Massachusetts and for Hampshire County into R. Call me or one of the data assistants over if you get an error.\n\n\nlibrary(tidyverse)\nlibrary(RColorBrewer)\ncounties <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_counties.csv\")\nroute_prefixes <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_route_pre.csv\")\nmaintenance <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_maintenance.csv\")\nkinds <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_kind.csv\")\n\nnbi_ma <- read.delim(\"https://www.fhwa.dot.gov/bridge/nbi/2022/delimited/MA22.txt\", sep = \",\") %>%\n  left_join(counties) %>%\n  left_join(route_prefixes) %>%\n  left_join(maintenance) %>%\n  left_join(kinds) %>%\n  filter(SERVICE_ON_042A == 1) %>%\n  select(STRUCTURE_NUMBER_008, COUNTY_CODE_003_L, ROUTE_PREFIX_005B_L, MAINTENANCE_021_L, YEAR_BUILT_027, ADT_029, STRUCTURE_KIND_043A_L, STRUCTURAL_EVAL_067, BRIDGE_IMP_COST_094) %>%\n  mutate(STRUCTURE_KIND_043A_L = \n           case_when(\n             STRUCTURE_KIND_043A_L == \"Concrete continuous\" ~ \"Concrete\",\n             STRUCTURE_KIND_043A_L == \"Steel continuous\" ~ \"Steel\",\n             STRUCTURE_KIND_043A_L == \"Prestressed concrete continuous\" ~ \"Prestressed concrete\",\n             TRUE ~ STRUCTURE_KIND_043A_L)) %>%\n  mutate(BRIDGE_IMP_COST_094 = BRIDGE_IMP_COST_094 * 1000)\n\nnbi_hampshire <- nbi_ma %>% filter(COUNTY_CODE_003_L == \"Hampshire\")"
  },
  {
    "objectID": "lab2.html#visualization-principles",
    "href": "lab2.html#visualization-principles",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Visualization Principles",
    "text": "Visualization Principles\n\n\n\n\n\n\nTip\n\n\n\nYou may wish to reference this ggplot cheatsheet when completing this lab. Note how this cheatsheet is organized. There are headings for things like:\n\nBasics\nGeom functions\nScales\nCoordinate systems\n\nThe first tip to find what you’re looking for is to consider what heading the graphic element will likely fall under. Also note that, for many functions on this cheatsheet:\n\nan image is provided for how the plot will transform,\nthe function and its arguments are referenced in bold, and\na text description of what will happen when you apply the function is provided\n\nFinally note that most of these functions listed on this cheatsheet are appended to the ggplot() function call with a + sign."
  },
  {
    "objectID": "lab2.html#exercise-1-reference-the-ggplot-cheatsheet-to-add-a-subtitle-to-this-plot-and-to-set-the-theme-to-minimal-to-balance-the-data-to-ink-ratio.",
    "href": "lab2.html#exercise-1-reference-the-ggplot-cheatsheet-to-add-a-subtitle-to-this-plot-and-to-set-the-theme-to-minimal-to-balance-the-data-to-ink-ratio.",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Exercise 1: Reference the ggplot() cheatsheet to add a subtitle to this plot and to set the theme to minimal to balance the data-to-ink ratio.",
    "text": "Exercise 1: Reference the ggplot() cheatsheet to add a subtitle to this plot and to set the theme to minimal to balance the data-to-ink ratio.\nYour subtitle should be the data’s source: County Health Rankings, University of Wisconsin Population Health Institute\n\nggplot(ma_county_health_2021, \n       aes(x = Name, y = HIV_prevalence_raw_value)) +\n  geom_col() +\n  coord_flip() +\n  labs(title = \"Health Measures by County, Massachusetts, 2021\",\n       x = \"County Name\", \n       y = \"HIV Prevalence Measure\",\n       subtitle = \"County Health Rankings, University of Wisconsin Population Health Institute\") +\n  theme_minimal()\n\n\n\n\n\nWhat kinds of disparities in HIV prevalence are indicated here? What social and historical conditions might explain these disparities?"
  },
  {
    "objectID": "lab2.html#exercise-2-reference-the-ggplot-cheatsheet-to-add-a-subtitle-to-this-plot-same-as-above-to-set-the-theme-to-minimal-and-add-a-trend-line.",
    "href": "lab2.html#exercise-2-reference-the-ggplot-cheatsheet-to-add-a-subtitle-to-this-plot-same-as-above-to-set-the-theme-to-minimal-and-add-a-trend-line.",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Exercise 2: Reference the ggplot() cheatsheet to add a subtitle to this plot (same as above), to set the theme to minimal, and add a trend line.",
    "text": "Exercise 2: Reference the ggplot() cheatsheet to add a subtitle to this plot (same as above), to set the theme to minimal, and add a trend line.\n\n\n\n\n\n\nHint\n\n\n\nThis line adds smoothed conditional means, or a regression line, to the plot.\n\n\n\nggplot(ma_county_health_2021, \n       aes(x = Food_insecurity_raw_value, y = Child_mortality_raw_value)) +\n  geom_point() +\n  labs(title = \"Health Measures by County, Massachusetts, 2021\",\n       x = \"Food Insecurity Measure\", \n       y = \"Child Mortality Measure\",\n       subtitle = \"County Health Rankings, University of Wisconsin Population Health Institute\") +\n  theme_minimal() +\n  geom_smooth()\n\nWarning: Removed 2 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 2 rows containing missing values (geom_point).\n\n\n\n\n\n\nReflection: What kind of relationship between food security and child mortality do we see here?"
  },
  {
    "objectID": "lab2.html#exercise-3-divide-this-plot-by-state-using-facet_wrap.",
    "href": "lab2.html#exercise-3-divide-this-plot-by-state-using-facet_wrap.",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Exercise 3: Divide this plot by state using facet_wrap().",
    "text": "Exercise 3: Divide this plot by state using facet_wrap().\n\nggplot(ne_county_health_2021, \n       aes(x = Unemployment_raw_value, y = Uninsured_raw_value)) +\n  geom_point() +\n  geom_smooth() +\n  labs(title = \"Health Measures by County, New England, 2021\",\n       x = \"Unemployment Measure\", \n       y = \"Uninsured Measure\",\n       subtitle = \"County Health Rankings, University of Wisconsin Population Health Institute\") +\n  theme_minimal() +\n  facet_wrap(vars(State_Abbreviation))\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\n\n\nReflection: In which states do counties have the highest uninsured measures? In which states do counties have the lowest?"
  },
  {
    "objectID": "lab2.html#exercise-4-adding-a-size-aesthetic-to-plot",
    "href": "lab2.html#exercise-4-adding-a-size-aesthetic-to-plot",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Exercise 4: Adding a Size Aesthetic to Plot",
    "text": "Exercise 4: Adding a Size Aesthetic to Plot\nReference the ggplot() cheatsheet to adjust the following plot in these ways:\n\nSet the size of the point to the median income value\nChange the label for the legend (hint: you will supply the name of the aesthetic argument in the labs() function)\nSet the legend’s position to the bottom of the plot\n\n\nggplot(ne_county_health_2021, \n       aes(x = Poor_physical_health_days_raw_value, \n           y = Life_expectancy_raw_value, \n           size = Median_household_income_raw_value)) +\n  geom_point(alpha = 0.5) +\n  labs(title = \"Health Measures by County, New England, 2021\",\n       x = \"Average number of physically unhealthy days reported in past 30 days (age-adjusted)\", \n       y = \"Life Expectancy\", \n       size = \"Median Household Income\",\n       subtitle = \"County Health Rankings, University of Wisconsin Population Health Institute\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_size_continuous(labels = scales::comma)\n\n\n\n\n\nBonus!: See if you can figure out how to convert the income values in the legend from scientific to comma notation. (Hint: You might reference how we converted decimals to percentages along our axes in the last lab.) Reflection: What does this plot tell us that we couldn’t see if we only considered the variables on the x and y axis? # Adding Color to Plots\n\nWe’re going to use the RBrewer package to add color to these plots. To start, enter the following in your Console to check out this package’s color palettes:\ndisplay.brewer.all(colorblindFriendly = TRUE)\nThe first set is sequential, the second is qualitative, and the third is divergent."
  },
  {
    "objectID": "lab2.html#exercise-5-add-a-sequential-color-aesthetic-to-plot",
    "href": "lab2.html#exercise-5-add-a-sequential-color-aesthetic-to-plot",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Exercise 5: Add a Sequential Color Aesthetic to Plot",
    "text": "Exercise 5: Add a Sequential Color Aesthetic to Plot\nCopy and paste the plot you created in Exercise 4 into the code snippet below. Change the size argument in your aes() function to col (short for color). Be sure to also change this argument in your labs() function to update your legend label. Finally, append the following argument to the end of your plot:\nscale_color_distiller(palette = \"Greens\", labels = scales::comma)\n\nggplot(ne_county_health_2021, \n       aes(x = Poor_physical_health_days_raw_value, \n           y = Life_expectancy_raw_value, \n           col = Median_household_income_raw_value)) +\n  geom_point() +\n  labs(title = \"Health Measures by County, New England, 2021\",\n       x = \"Average number of physically unhealthy days reported in past 30 days (age-adjusted)\", \n       y = \"Life Expectancy\", \n       col = \"Median Household Income\",\n       subtitle = \"County Health Rankings, University of Wisconsin Population Health Institute\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_color_distiller(palette = \"Greens\", labels = scales::comma)\n\n\n\n\n\nWhy do we use a sequential pattern here instead of a divergent pattern? Because Median Household Income is a continuous variable that starts at 0 and increases in one direction. There’s not a middle or neutral value that we are comparing incomes around. We should only use divergent color palettes in cases when we are comparing distances in two directions from some middle point. For instance, maybe I would use a divergent palette to visualize temperatures above or below freezing. ## Exercise 6: Add a Qualitative Color Aesthetic to Plot\n\nCopy and paste the plot we created above into the code snippet below. Instead of coloring the points by Median Household Income, color them by state. Be sure to update your legend title. Finally, convert the sequential palette into a categorical palette. To determine how to do this, direct your attention to the Color and Fill Scales section of the ggplot() cheatsheet. How should the word “distiller” be edited in this function call in order create a palette with a discrete set of colors? Once you determine this, update the function call and the palette to “Set2.”\nYou don’t need to change the word “color” to “fill” when adjusting the scale function name. We apply the color argument to points and lines, and the fill argument to bars and shapes. This is a point plot, so we will stick with color!\n\nggplot(ne_county_health_2021, \n       aes(x = Poor_physical_health_days_raw_value, \n           y = Life_expectancy_raw_value, \n           col = State_Abbreviation)) +\n  geom_point() +\n  labs(title = \"Health Measures by County, New England, 2021\",\n       x = \"Average number of physically unhealthy days reported in past 30 days (age-adjusted)\", \n       y = \"Life Expectancy\", \n       col = \"State\",\n       subtitle = \"County Health Rankings, University of Wisconsin Population Health Institute\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\nReflection: To what extent can you identify disparities in life expectancies and unhealthy days across states? Which states tend to be higher and which lower? ## Exercise 7: Facet the Plot by State\n\nCopy and paste the plot you created above into the code snippet below. Remove the col aesthetic and all labels/legends and then facet the plot by state.\n\nggplot(ne_county_health_2021, \n       aes(x = Poor_physical_health_days_raw_value, \n           y = Life_expectancy_raw_value)) +\n  geom_point() +\n  labs(title = \"Health Measures by County, New England, 2021\",\n       x = \"Average number of physically unhealthy days reported in past 30 days (age-adjusted)\", \n       y = \"Life Expectancy\", \n       subtitle = \"County Health Rankings, University of Wisconsin Population Health Institute\") +\n  theme_minimal() +\n  facet_wrap(vars(State_Abbreviation))\n\n\n\n\n\nReflection: Which of the plots you created in the last two exercises better enables you to compare health disparities across states?\n# Overplotting\n\nThe final topic for today’s lab is overplotting - when we have so much overlapping data represented on a plot that it becomes difficult to draw any conclusions from it. Run the code below to see an example of this\n\nggplot(county_health_2021, \n       aes(x = Children_in_poverty_raw_value, \n           y = Math_scores_raw_value, \n           col = Broadband_access_raw_value)) +\n  geom_point() +\n  labs(title = \"Health Measures by County, US, 2021\",\n       x = \"Children in Poverty Measure\", \n       y = \"Math Scores Measure\",\n       subtitle = \"County Health Rankings, University of Wisconsin Population Health Institute\", \n       col = \"Broadband Access Measure\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_color_distiller(palette = \"Greens\", labels = scales::comma)\n\nWarning: Removed 448 rows containing missing values (geom_point).\n\n\n\n\n\nFortunately, there are some adjustments we can make to deal with overplotting. First, we can adjust the transparency of the shapes on our plot by setting the alpha argument in our geom function to a number between 1 and 0. We can also reduce the size of the shapes on our plot by setting the size argument in our geom function to a number between 1 and 0."
  },
  {
    "objectID": "lab2.html#exercise-8-adjust-the-alpha-and-size-of-points-on-plot",
    "href": "lab2.html#exercise-8-adjust-the-alpha-and-size-of-points-on-plot",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Exercise 8: Adjust the Alpha and Size of Points on Plot",
    "text": "Exercise 8: Adjust the Alpha and Size of Points on Plot\nCopy and paste the plot I created above into the code snippet below. In the geom_point() function, set the alpha to 0.75 and the size to 0.5. Notice the adjustments to the plot as you update these attributes.\n\nggplot(county_health_2021, \n       aes(x = Children_in_poverty_raw_value, \n           y = Math_scores_raw_value, \n           col = Broadband_access_raw_value)) +\n  geom_point(alpha = 0.75, size = 0.5) +\n  labs(title = \"Health Measures by County, US, 2021\",\n       x = \"Children in Poverty Measure\", \n       y = \"Math Scores Measure\",\n       subtitle = \"County Health Rankings, University of Wisconsin Population Health Institute\", \n       col = \"Broadband Access Measure\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_color_distiller(palette = \"Greens\", labels = scales::comma)\n\nWarning: Removed 448 rows containing missing values (geom_point).\n\n\n\n\n\n\nReflection: What conclusions would you draw from this plot? Sometimes, overplotting can also be an issue when we are plotting points discretely. Run the code below and review the plot.\n\n\nggplot(ne_county_health_2021, \n       aes(x = State_Abbreviation, \n           y = Flu_vaccinations_raw_value)) +\n  geom_point() +\n  labs(title = \"Health Measures by County, New England, 2021\",\n       x = \"State\", \n       y = \"Flu Vaccination Measure\",\n       subtitle = \"County Health Rankings, University of Wisconsin Population Health Institute\") +\n  theme_minimal()\n\n\n\n\nWhile it may look fine at first glance, it turns out that some counties have the exact same flu vaccination measure and are overlapping each other on that plot. We could add an alpha argument to see where there are overlaps.\n\nggplot(ne_county_health_2021, \n       aes(x = State_Abbreviation, \n           y = Flu_vaccinations_raw_value)) +\n  geom_point(alpha = 0.35) +\n  labs(title = \"Health Measures by County, New England, 2021\",\n       x = \"State\", \n       y = \"Flu Vaccination Measure\",\n       subtitle = \"County Health Rankings, University of Wisconsin Population Health Institute\") +\n  theme_minimal()\n\n\n\n\nHowever, in this case, to be sure that every county is represented on the plot, a better option, would be to add jitter to the plot. Jitter offsets the points from their original position slightly so that we can make out points that overlap. ## Exercise 9: Add jitter to the Plot\nCopy the plot that I created above into the code snippet below. Using the cheatsheet, find the geom function for creating a jittered plot. Set the width of the jitter to 0.15\n\nggplot(ne_county_health_2021, \n       aes(x = State_Abbreviation, \n           y = Flu_vaccinations_raw_value)) +\n  geom_jitter(width = 0.15) +\n  labs(title = \"Health Measures by County, New England, 2021\",\n       x = \"State\", \n       y = \"Flu Vaccination Measure\",\n       subtitle = \"County Health Rankings, University of Wisconsin Population Health Institute\") +\n  theme_minimal()\n\n\n\n\n\nReflection: Which New England states have counties with the highest flu vaccination rates? In which states are there greater disparities in vaccination rates across counties?\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\n…"
  },
  {
    "objectID": "lab2.html#national-bridge-inventory-dataset",
    "href": "lab2.html#national-bridge-inventory-dataset",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "National Bridge Inventory Dataset",
    "text": "National Bridge Inventory Dataset\nEvery year the U.S. Federal Highway Administration publishes a dataset listing every federally-regulated bridge and tunnel in the U.S., along with its location, design features, operational conditions, and inspection ratings. The data is used to review the safety of these transportation infrastructures for the traveling public. As you can imagine, for politicians promising to the improve the state of transportation infrastructure, this dataset is integral to determining where to allocate improvement funds:\n\n\nToday, we are going to look at a subset of 2022 NBI data for Massachusetts and for Hampshire County, MA, and we are going to focus solely on highway bridges (excluding pedestrian and railroad bridges). We’re going to look at what kinds of variables might contribute to poor bridge conditions, where there are poor bridge conditions, and which entities are responsible for maintaining them. The data documentation for this dataset is quite thick, so I will provide you with a data dictionary for today.\n\n\n\n\n\n\n\nVARIABLE NAME\nDESCRIPTION\n\n\n\n\nSTRUCTURE_NUMBER_008\nUnique ID for the bridge\n\n\nCOUNTY_CODE_003_L\nName of the county where the bridge is located\n\n\nROUTE_PREFIX_005B_L\nRoute signing prefix for the inventory route\n\n\nMAINTENANCE_021_L\nThe actual name(s) of the agency(s) responsible for the maintenance of the structure\n\n\nYEAR_BUILT_027\nThe year of construction of the structure\n\n\nADT_029\nThe average daily traffic volume for the inventory route based on most recent available data\n\n\nSTRUCTURE_KIND_043A_L\nThe kind of material and/or design of the structure\n\n\nSTRUCTURAL_EVAL_067\nA rating of the structural evaluation of the bridge based on inspections of its main structures, substructures, and/or its load ratings\n\n\nBRIDGE_IMP_COST_094\nEstimated costs for bridge improvements"
  },
  {
    "objectID": "lab2.html#visualization-aesthetics",
    "href": "lab2.html#visualization-aesthetics",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Visualization Aesthetics",
    "text": "Visualization Aesthetics\nThe visual cues that we select to display on a plot largely depend on the kind of data that we have available. Last week we discussed the differences between categorical variables and numeric variables. Certain types of visual cues are more suited for representing categorical variables, while other types of visual cues are more suited for representing numeric variables. For instance, we wouldn’t use different shapes to represent different numeric values because there is no obvious ordering to a group of shapes (e.g. a triangle isn’t necessarily larger or greater than a circle; they’re just different). Because of this, shapes are much more appropriate for representing nominal categorical variables. Size is a much more effective visual cue for numeric variables because size can increase as the values in the data increase.\n\n\n\n\n\n\n\n\nCue\nEffective for what kinds of variables?\nExample where applied?\n\n\n\n\nShape\nCategorical\nPoints on scatterplots\n\n\nSize\nNumeric\nPoints on scatterplots\n\n\nArea\nNumeric\nBars in bar plots\n\n\nColor\nCategorical (Qualitative palette)\nNumeric (Sequential/Divergent)\nPoints on scatterplots;\nBars in bar plots;\nLines in a line plot\n\n\nPosition\nCategorical; Numeric\nPoints on scatterplots\n\n\nAngle\nNumeric\nSlices in pie chart\n\n\n\nYou’ll remember from lecture that we map variables onto these visual cues via the aesthetic function (aes()) in ggplot().\n\n\n\n\n\n\nTip\n\n\n\nYou may wish to reference this ggplot cheatsheet when completing this lab. Note how this cheatsheet is organized. There are headings for things like:\n\nBasics\nGeom functions\nScales\nCoordinate systems\n\nThe first tip to find what you’re looking for is to consider what heading the graphic element will likely fall under. Also note that, for many functions on this cheatsheet:\n\nan image is provided for how the plot will transform,\nthe function and its arguments are referenced in bold, and\na text description of what will happen when you apply the function is provided\n\nFinally note that most of these functions listed on this cheatsheet are appended to the ggplot() object with a + sign.\n\n\n\nPosition\nSince today we will be working with scatterplots, let’s start by talking about position. When we refer to position, we refer to the location of a point on a Cartesian plane (i.e. its position on an x-axis and its position on a y-axis). We can create scatterplots by mapping a variable in our dataset onto the x-axis and another variable onto the y-axis (aes(x = VARIABLE_NAME, y = VARIABLE_NAME)). Let’s take a look at what happens when we map the year a bridge was built onto the x aesthetic and the bridge’s structural evaluation onto the y aesthetic for all Hampshire County Massachusetts highway bridges.\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point() +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal()\n\n\n\n\nEach point on the plot corresponds to one observation (row) in the dataset. Since each row in this dataset is one Hampshire County, MA highway bridge, each point on this plot also represents one Hampshire County, MA highway bridge. The position of the point indicates to us the year that bridge was built and its structural evaluation. Zooming out to look at all of this data we can see that newer bridges tend to have higher structural evaluations.\n\nOverplotting\nYou’ll notice that parts of this plot can be challenging to read because some points overlap each other making it hard to distinguish one from the next. This is an example of overplotting, and there are a number of strategies we can take to addressing it. Most of these strategies involve revising attributes of the points on the plot (e.g. the size, transparency, and position of the points).\nNote that attributes are different than aesthetics. Recall that when we assign aesthetics in a plot, we are adjusting the visual cues on a plot according to the values in a variable. The visual cues will be different based on the values in that variable (e.g. size will be greater with greater values). On the other hand, when we adjust attributes, we are adjusting the visual cues on a plot in a fixed way (i.e. every point will be styled in the same way regardless of its value). Because of this attributes are assigned outside of the aes() (aesthetic) function. Let’s adjust the plot we created above by reducing the size of every point (size =), reducing the transparency of every point (alpha =), and adding \"jitter\" to the plot (position=). Recall that alpha is assigned between 0 and 1: 0 being transparent and 1 being opaque. Jitter means that we add a bit of random noise to the points on the plot in order to prevent overlapping points.\n\n\n\n\n\n\nExercise 1\n\n\n\nBelow, adjust the size to 2 and the alpha to 0.5, and the position to \"jitter\". Note how this changes the plot.\n\n\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(size = 1, alpha = 1, position = \"identity\") +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal()\n\n\n\n\n\n\n\nColor\nColor can be used on plots to either distinguish between discrete values in a categorical variable or to represent the range of values in numeric variable. We use different kinds of color palettes for each of these scenarios. Palettes refer to a range of colors. We can have palettes with discrete colors (e.g. red, orange, and blue) or palettes with a gradient of colors (e.g. lightest red to darkest red).\n\n\n\n\n\n\n\n\nPalette\nEffective for what kinds of variables?\nExample\n\n\n\n\nQualitative\nCategorical\nRed, yellow, blue\n\n\nSequential\nNumeric\nLight blue to dark blue\n\n\nDivergent\nNumeric, extending in two directions (e.g. >1 and <1)\nBlue to purple to red\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nIn the plot below, color the points on the plot with a qualitative palette by setting col= to the categorical variable ROUTE_PREFIX_005B_L. Remember that, in this case, color is an aesthetic, so this must be added inside of the aes() function. Add a label for the legend by setting col= in the labs() function to a phrase that describes the color variable.\n\n\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       col = \"ADD LEGED LABEL HERE\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\nCopy the plot from above below but swap out the variable your mapped to the color aesthetic with a numeric variable from the dataset. Be sure to also adjust the legend label. What kind of palette gets created? What new information do you gain about Hampshire County bridges from the plot?\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nYour Response\n\n\n\nFill response here.\n\n\n\n\nShape\nA different way to differentiate data on a plot is to map the shape aesthetic onto the plot. In this case, rather than all observations in the dataset appearing as points on a plot, observations will appear as different shapes based on their associated values in a categorical variable.\n\n\n\n\n\n\nExercise 4\n\n\n\nIn the plot below, assign the shape of the data on the plot by setting shape= to the categorical variable STRUCTURE_KIND_043A_L. Remember that, in this case, shape is an aesthetic, so this must be added inside of the aes() function. Add a label for the legend by setting shape= in the labs() function to a phrase that describes the shape variable.\n\n\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       shape = \"ADD LEGED LABEL HERE\") +\n  theme_minimal()\n\n\n\n\n\n\nSize\nWhile we might map a categorical variable onto the the shape aesthetic, we can alternatively map a numeric variable onto the size aesthetic. For instance, note what we learn when we map the variable for average daily traffic onto the size aesthetic below.\n\nnbi_hampshire %>%\n  ggplot(aes(x = ROUTE_PREFIX_005B_L, \n             y = STRUCTURAL_EVAL_067, \n             size = ADT_029)) +\n  geom_point(alpha = 0.5, position = \"jitter\") +\n  coord_flip() +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Route Prefix\", \n       y = \"Structural Evaluation\",\n       size = \"Average Daily Traffic\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote how I moved my legend position to the bottom using + theme(legend.position = \"bottom\") in the code above.\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\nCopy one of your plots from above, and inside the aes() function, set size= to a numeric variable not already represented on the plot. Keep in mind that you’ll need to remove the size= attribute from geom_point() for the points to size correctly. Be sure to also adjust the legend labels. What new information do you gain about Hampshire County bridges from the plot?\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nYour Response\n\n\n\nFill response here."
  },
  {
    "objectID": "lab2.html#scales",
    "href": "lab2.html#scales",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Scales",
    "text": "Scales\nWhen we map a variable onto an aesthetic, we are only indicating that the variable should be mapped. We are not indicating how the variable should be mapped. In order to indicate how we want a variable mapped to an aesthetic, we can adjust its scales. Scales are adjusted by tacking the following onto a ggplot() object: + scale_<aesthetic>_<type>(). For instance, let’s say that I wanted to adjust the scale of my x-axis to a log scale. I would attached + scale_x_log10() to my ggplot() object.\n\n\n\n\n\n\n\n\nScale\nDescription\nExample\n\n\n\n\nContinuous\nNumeric values are mapped along a continuum\n+ scale_y_continuous()\n\n\nDiscrete\nCategorical values are mapped into discrete buckets\n+ scale_color_discrete()\n\n\nBinned\nNumeric values are mapped into discrete bins\n+ scale_size_binned()\n\n\nLog\nNumeric values are mapped logarthmically\n+ scale_y_log10()\n\n\nDate-Time\nNumeric values are mapped along a timeline\n+ scale_x_datetime()\n\n\n\nLet’s talk about how we would adjust the scales for each of the aesthetics we’ve covered so far.\n\nPosition\nWe can adjust the scale of our x and y-axes by adding + scale_x_<type>() or + scale_y_<type>() to our plots. Note what happens when we attempt to create a scatterplot that shows the relationship between the year a bridge was built and the bridge improvement costs for all MA highway bridges.\n\nnbi_ma %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = BRIDGE_IMP_COST_094)) +\n  geom_point(size = 1, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Bridge Improvement Costs\") +\n  theme_minimal() + \n  scale_y_continuous(labels = scales::comma)\n\nWarning: Removed 1 rows containing missing values (geom_point).\n\n\n\n\n\nDue to huge disparities in costs for bridge improvements, this plot is difficult to interpret. Most bridge improvement costs are under $10,000,000, but with at least one bridge with costs just under a $1,000,000,000, the vast majority of the points on the plot appear at the very bottom of the y-axis scale and are largely indiscernible from one another. This is a case when it makes sense to apply a log scale to the y-axis.\n\n\n\n\n\n\nExercise 6\n\n\n\nCopy the plot that I created above, and change the y-axis scale from continuous to log10. You might reference the formula specified above or reference the ggplot cheatsheet for help. Note that your y-axis will appear in scientific notation. Convert it to comma notation by adding: labels = scales::comma as an argument in the scale function call.\n\n\n\n# Create plot here\n\nSometimes I might wish to group certain numeric values into bins on a plot. For instance, let’s say I just want to see how many bridges are structurally deficient in comparison to bridges that are operationally sound. Typically a bridge is considered structurally deficient when it scores 4 or lower. So I want to group the numeric values in STRUCTURAL_EVAL_067 into two bins: 0-4 and 4-9. To do this, I will set the scale to: + scale_y_binned() and add an argument to establish the bin breaks: breaks = c(4, 9) as well as an argument to label the bin breaks: labels = c(\"Structurally Deficent\", \"No Deficiencies\"). Check out what happens to the y-axis scale when I do this below.\n\nnbi_ma %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(size = 1, alpha = 0.2, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal() + \n  scale_y_binned(breaks = c(4, 9), labels = c(\"Structurally Deficent\", \"No Deficiencies\"))\n\n\n\n\n\n\n\n\n\n\nExercise 7\n\n\n\nThe Interstate Highway Act of 1956 (signed under Dwight D. Eisenhower) established the U.S.’s interstate highway system and is considered the country’s largest public works project. Adjust the x-axis scale of the plot below to help tell this story. Specifically, bin years into before 1954 (when Eisenhower was elected) and 1954-2021. Set the label at 1954 to “Eisenhower Elected” and the label at 2021 to “Today.”\n\n\n\nnbi_ma %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = ROUTE_PREFIX_005B_L)) +\n  geom_point(size = 1, alpha = 0.2, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Route Prefix\") +\n  theme_minimal()\n\n\n\n\n\n\nColor\nWe’ve already talked about how both categorical and numeric variables can be mapped to the color aesthetic. However, sometimes, we want to be able to further customize which colors should appear on the plot and how they appear on the plot. The RColorBrewer package, which you installed earlier in the lab includes a number of palettes for coloring points on a plot. Check them out below:\n\nRColorBrewer::display.brewer.all() \n\n\n\n\nNote how the first set of palettes is sequential, the second set is categorical, and the third set is divergent. We can assign these palettes to our plots using one of two functions: + scale_color_brewer() for categorical data and + scale_color_distiller() for numeric data. Within this function, we can set the argument palette equal to one of the palettes specified in the image above.\n\n\n\n\n\n\nExercise 8\n\n\n\nAdd `+ scale_color_brewer()` to the ggplot() object below and assign a categorical palette. Refer to the ggplot cheatsheet or the help pages for help with formatting the function call.\n\n\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067,\n             col = ROUTE_PREFIX_005B_L)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, Massachusetts, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       col = \"Route Prefix\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\n\n\nExercise 9\n\n\n\nAdd `+ scale_color_distiller()` to the ggplot() object below and assign a sequential palette. By default the colors will be ordered from darkest to lightest. Add an argument to the function call to reverse the direction of the colors. Refer to the ggplot cheatsheet or the help pages for help with formatting the function call.\n\n\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067,\n             col = ADT_029)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       col = \"Average Daily Traffic\") +\n  theme_minimal()\n\n\n\n\n\n\nSize and Shape\nThe following represents the values associated with ggplot() point shapes.\n\nWe can manually assign shapes using + scale_shape_manual(values = c(<shape_values>)).\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067, \n             shape = MAINTENANCE_021_L)) +\n  geom_point(alpha = 0.5, size = 2, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       shape = \"Maintainer\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",\n        legend.box=\"vertical\") +\n  guides(shape = guide_legend(nrow = 3, byrow = TRUE)) +\n  scale_shape_manual(values = c(15:17))\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that the guides() function above allows us to wrap the legend into three rows!\n\n\nAlso note how we bin point sizes just like we binned values on the x and y axis.\n\nnbi_hampshire %>%\n  ggplot(aes(x = ROUTE_PREFIX_005B_L, \n             y = STRUCTURAL_EVAL_067, \n             size = ADT_029)) +\n  geom_point(alpha = 0.5, position = \"jitter\") +\n  coord_flip() +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Route Prefix\", \n       y = \"Structural Evaluation\",\n       size = \"Average Daily Traffic\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_size_binned(breaks = c(0, 500, 5000, 100000))"
  },
  {
    "objectID": "lab2.html#faceting",
    "href": "lab2.html#faceting",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Faceting",
    "text": "Faceting\nFaceting involves breaking out a single plot into multiple smaller plots based on the value in a variable. Faceting is very helpful when we have a categorical variable with many distinct values. If we tried to color by that variable, the colors would likely be indistinguishable from one another. For instance, check out what happens when we try to color by the COUNTY_CODE_003_L variable below.\n\nnbi_ma %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067, \n             col = COUNTY_CODE_003_L)) +\n  geom_point(alpha = 0.5, size = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       col = \"County\") +\n  theme_minimal()\n\n\n\n\nThere are so many counties that it’s extremely challenging to distinguish between colors on this plot. In this case, instead of using a color aesthetic, we facet the plot by adding + facet_wrap(vars(COUNTY_CODE_003_L)) to the ggplot() object. Check out what happens when we do that below.\n\nnbi_ma %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(alpha = 0.1, size = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal() +\n  facet_wrap(vars(COUNTY_CODE_003_L))\n\n\n\n\n\n\n\n\n\n\nExercise 10\n\n\n\nFacet the plot below by STRUCTURE_KIND_043A_L to see the degree of structural deficiencies for different kinds of highway bridges of different ages in Massachusetts.\n\n\n\nnbi_ma %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(alpha = 0.1, size = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nIn response to concerns regarding domestic security, some stakeholders have questioned whether the Federal Highway Administration should be publicly disclosing information about the location and deficiencies of U.S. bridges and other forms of transportation infrastructure. What do you see as the value of making this data available to the public? Does its value outweigh national security concerns? Are these concerns legitimate? Share your ideas on our sds-192-discussions Slack channel."
  },
  {
    "objectID": "learning-styles.html",
    "href": "learning-styles.html",
    "title": "Course Study Planning",
    "section": "",
    "text": "How do I study for this class?\n\n… and in the past I’ve sometimes struggled to answer that question because everyone learns differently. With this in mind, I’ve developed the following chart to help students figure out how best to study for this class given their personal learning style.\nThis chart is modeled based on research indexing different kinds of learning styles across four dimensions (active/reflective, sensing/intuitive, visual/verbal, and sequential/global). This model was formulated by Richard M. Felder and Linda K. Silverman. Later, Richard M. Felder and Barbara A. Soloman developed an assessment tool for learning about a respondents’ learning preferences.\nAs the first part of this assignment, you should take this assessment here, and review your results against the descriptions of the dimensions here.\nOnce you’ve reviewed your results, you can reference the following chart to see what I recommend in terms of your studying. Importantly, you should check out the study tips along all four dimensions (i.e. identify where you stand in each row of the table below). In the Study Plan assignment posted in Moodle, reflect on which study tips seem like they will be most supportive to you in 200-300 words.\n\n\n\n\n\n\nTip\n\n\n\nIt’s important to note that these are just suggestions for directing your study habits. As with all indexes, this one has its limits. In writing up your response, focus on those suggestions that speak to you! Later in the semester, you will have an opportunity to reflect on how studying is going and adjust as necessary. Remember that I’m here to help you figure out how best to navigate this course!\n\n\n\n\n\nDimension\nStudy Tips\nDimension\nStudy Tips\n\n\n\n\nMore Active (learn by doing)\n\nBe sure to complete all course lab assignments. Active learners will get a lot out of these!\nWork through practice problems in the course text.\nAsk tutors in the Spinelli Center if they can help you work through practice problems.\nFind online tutorials related to the course standards, and see if you can follow along to complete them in R.\nReach out to me for some good practice datasets and see if you can apply the coding skills we are learning to those datasets!\n\nMore Reflective (learn by thinking)\n\nWhile reviewing reading materials and lecture notes, see if you can stop and write short paragraphs summarizing a course concept or what is happening when we use certain functions in R.\nAsk tutors in the Spinelli Center if they can help you talk out a particular concept.\nFind a friend and see if you can explain a challenging course concept to them. …or see if you can explain the concept to me in office hours!\nForm a study group and build in time to talk through challenging course concepts with each other.\n\n\n\nSensing Preference (preference for learning facts)\n\nWhen reading the course texts, you may pay extra attention to the sections where the authors walk through real world examples where certain data science tools would be applied.\nAsk tutors in the Spinelli Center if they can walk you through a concept by showing a real world example of where it would be applied.\nIf course material is feeling too abstract, ask me to give an example where it may be applied in the real world in class or in office hours.\n\nIntuitive Preference (preference for learning theories and relationships)\n\nWhen reading the course texts, you may pay extra attention to the sections where the authors explain the meaning, rationales, and theories behind course concepts and tools and how they relate to one another.\nAsk tutors in the Spinelli Center if they can walk you through a concept by explaining its meaning and function and how it gets applied.\nIf course material is feeling too rigid or mechanical, ask me to explain the theories behind certain functions in class or in office hours.\n\n\n\nMore Visual (learn by seeing)\n\nBe sure to pay close attention to diagrams and images in course reading assignments and lecture slides.\nIf you’re not following along with a concept in lecture, ask me if we can pause and draw it out on the white board.\nAsk tutors in the Spinelli Center to help you draw a diagram or image to explain a challenging concept.\nAs you are reviewing lecture notes and readings, see if you can summarize a challenging concept by drawing a diagram or schematic of the concept. For instance, you might draw what happens to a data frame when you applying a certain wrangling verb.\nThere are often really helpful icon diagrams in R cheatsheets. I will do my best to always link relevant cheatsheets in course lab assignments.\n\nMore Verbal (learn by hearing and saying)\n\nWatch my lab recaps in Perusall and ask questions when you are struggling to understand a concept from the lab.\nIf you’re not following along with a concept in lecture, ask me if we can pause and explain the concept in a different way.\nAsk tutors in the Spinelli Center if they can explain a concept in a different way.\nForm a study group so that you can listen to other students explain course concepts.\nFind YouTube videos that explain course concepts that you are struggling with.\nDataCamp offers a number of courses on topics we will cover in the course, and students can create a free account! You may wish to work through a DataCamp course related to a concept you are struggling with.\n\n\n\nMore Sequential (learn step-by-step)\n\nAsk tutors in the Spinelli Center if they can walk you step-by-step through a practice problem.\nIdentify practice problems that you struggled with in labs or in class, and write out step-by-step how you would tackle that problem.\nReview quiz questions you got incorrect by working with a classmate to write out the steps to the correct answer.\nAsk me to explain the logic behind the steps we take to answer a certain problem in class or in office hours.\n\nMore Global (learn big picture)\n\nAsk tutors in the Spinelli Center if they can help you draw connections between different course concepts and ideas.\nCreate concept maps that show the relationships between different course concepts.\nReview quiz questions you got incorrect by working with a classmate to relate the question to another question you got correct or to another concept that you understand well.\nAsk me to explain how different course concepts and ideas connect to one another in class or in office hours."
  },
  {
    "objectID": "lab4.html",
    "href": "lab4.html",
    "title": "Lab 4: Git and GitHub",
    "section": "",
    "text": "There are many reasons that Git and GitHub are essential infrastructures for collaborative coding projects. For one Git saves snapshots of a code repository at different stages of a project so that we can track how it has changed over time and revert back to an older version if we discover a more recent error. We call this version control. Certain Git features also facilitate many people working on a coding project at once, by providing a number of tools to help prevent collaborators from over-writing each other’s work. These features also make it possible for developers to simultaneously modify, extend, and test components of the code without jeopardizing the project’s current functionality. Further, GitHub supports code publication; by publishing code on GitHub, you contribute to an open access/free software community, enabling others to learn from and build off of your work.\nDespite all of these awesome benefits, there can be a significant learning curve when getting started with Git and GitHub. There are new vocabularies, workflows, and error mitigation strategies to learn when getting started. This lab is designed to help you get acquainted with the concepts behind Git and GitHub, suggested workflows for collaborating on projects in this course, and error resolution strategies.\n\n\n\nCreate, update, and close issues\nBranch a repo\nIssue pull requests\nAddress common push/pull errors\nAddress merge conflicts"
  },
  {
    "objectID": "lab4.html#review-of-key-terms",
    "href": "lab4.html#review-of-key-terms",
    "title": "Lab 4: Git and GitHub",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\nRepo\n\nCollaborative storage space for folders, documents, data, and code\n\nBranch\n\nAn isolated version of a repo that can be modified without affecting the main branch\n\nClone\n\nCreates a copy of a repo stored in a remote space (e.g. GitHub) to your local machine (e.g. your computer)\n\nPull\n\nDownloads the latest version of a repo from remote space (e.g. GitHub) to your local machine (e.g. your computer)\n\nStage\n\nThe process of marking which changes of the code are ready to be saved\n\nCommit\n\nA stored snapshot of a repo at a particular moment in time\n\nPush\n\nUploads commits from your local machine (e.g. your computer) to a remote space (e.g. GitHub)\n\nPull Request\n\nA request for modified code to be integrated with a different branch\n\nMerge\n\nThe process of integrating code modifications from one branch into another branch"
  },
  {
    "objectID": "lab4.html#github-flow",
    "href": "lab4.html#github-flow",
    "title": "Lab 4: Git and GitHub",
    "section": "GitHub Flow",
    "text": "GitHub Flow\nIn my opinion, there are two kinds of workflows for GitHub. There’s the quick and dirty version, and there’s the long and elegant version. Below are the differences between these two workflows (don’t worry about if you don’t understand what the steps mean right now; we will learn all of them in the lab).\n\n\n\n\n\n\n\nQuick and Dirty Version\nLong and Elegant Version\n\n\n\n\n\nPull recent changes from GitHub to local machine.\nMake edits and save them\nStage and commit changes.\nPush changes from local machine to GitHub.\n\nIn the quick and dirty version, all of this occurs in the main branch.\n\nCreate an issue at GitHub.com\nBranch the repo at GitHub.com.\nPull recent changes from GitHub.com to local machine.\nMake edits in the new branch and save them.\nStage and commit changes.\nPush changes from local machine to GitHub.com.\nCreate a pull request at GitHub.com\nAssign a reviewer to review the proposed changes and wait for their approval.\nMerge changes, while also closing the issue and deleting the branch.\n\n\n\n\nTypically I recommend the long and elegant version as it is designed to avoid errors and ensure that collaborators are all on the same page regarding changes to files. However, occasionally when you have to make small, quick changes to a file, and it won’t impact your team mate’s work, it will make more sense to follow the quick and dirty workflow. The goal for today is to get practice in the long and elegant version.\n\nRepo\n\n\n\n\n\n\nExercise 1\n\n\n\nNavigate to _____ to accept the assignment. Enter your project group members.\n\n\nThis will create a GitHub repository called github-lab. You’ll notice that the repository has a few files - README.md, github-practice.Rmd, .gitingore. You’ll also notice in the repo’s right sidebar that your group members are listed as collaborators. This means that you all have access to read and write to this repository.\n\n\nClone\n\n\n\n\n\n\nExercise 2\n\n\n\nAll group members should clone the repo to their RStudio environment. To do so, copy the repo’s URL. Then in RStudio click on File > New Project > Version Control > Git, and enter then pasted the copied URL into the window that appears. Note what you see in the RStudio files pane after cloning the repo.\n\n\nRemember that cloning creates a copy of a remote repo on a local machine. In creating this project, you’ve copied all of the files that make up the github-lab repo at GitHub.com to your local computer. This means that you will find all of the files associated with this repo by navigating to the folder where you created the project on your computer.\nIt’s important to note that this is not just any old folder on your computer though. By cloning, you’ve created a git folder. This means that the folder has been set up in a way where git can track the changes that you make to it over time, and it knows that there is a remote version of the repository somewhere that you might want to keep it consistent with.\n\n\nIssues\n\n\n\n\n\n\nExercise 3\n\n\n\nNavigate back to GitHub, and click on the repo’s Issues tab. Each member of your team should create an issue by clicking the ‘New Issue’ button. Title the issue: “Adding <your name> to the assignment.” Submit the issue, and to the left of the screen, assign the issue to yourself.\n\n\nIssues support project planning by allowing you to track changes you hope to make to your project over time. By assigning issues to certain collaborators on your project team, you can have clear documentation of who is responsible for what.\n\n\n\n\n\n\nTip\n\n\n\nIn my own projects, I use Issues for a number of purposes. Sometimes I use Issues to bugs that I notice in my code that need to be fixed. Other times I use them to track features that I would like to add to my code down the road. Oftentimes, in my public repositories, I encourage others that are using my code to submit issues to ask questions about how something works, to report bugs, or to request features.\n\n\n\n\nBranch\nWhen you first create a repository all of the code will be stored in the main branch of the repository. If you were to think of a project like a tree growing up from the ground, then the main branch would be the like the trunk of the tree. One goal of a branching workflow in GitHub is to keep the most stable and polished versions of code in the main branch. So what do we do in the meantime - when we’re editing code and trying to sort out its bugs, and it’s not quite in that stable and polished state yet? That’s where branching comes in.\nWhen we create a branch of our repo, GitHub creates a separate copy of the repo where you can make changes without impacting what’s in the main branch. Later, once we’re done making changes and things are stable and polished, we will have the opportunity to merge those changes back into the main branch.\n\n\n\n\n\n\n\nTip\n\n\n\nBranching can get pretty wild in GitHub. You can have branches of branches of branches. I don’t recommend this. A good workflow is to create a branch for making specific changes, merge those changes back into main, delete the branch, and then create a new branch for the next batch of changes.\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\nClick on the Code tab on your repo’s page at GitHub.com. Directly below this tab, you will see a dropdown that is currently labeled “main.” This means that you are in the main branch. Each member of your team should click the down arrow, and create a branch by entering their first name into the textbox that appears, and then clicking “Create branch.”\n\n\n\n\n\nPull\nAs of right now, the branches that were created in the previous step only exist on GitHub.com, they don’t exist yet on your local machine. To get these changes to your local machine, you need to Pull the changes. Remember how we said that the super fancy git folder knows that there is a remote version of the repository somewhere that you might want to keep it consistent with? When we pull changes to our local machines, we are basically saying, check that remote version for changes, and then pull them into the repo on my computer.\n\n\n\n\n\n\nExercise 5\n\n\n\nHead back to RStudio. In the Environment pane, you will notice a tab labeled “Git.” It’s important to note that this tab will only appear in projects that are built from super fancy git folders. This is your RStudio command center for Git and GitHub. When you click on this tab, you will see a few buttons in the navigation bar. To pull changes, you should click the blue downward arrow. Click this button to pull the branches created remotely to your local machine.\n\n\n\n\n\nSwitch Branches\nEven though you pulled the new branches to your local machine, you are still currently working in the main branch. Remember that we always want to keep the main branch stable and polished. This is not where we are going to make edits. Instead, you will make edits in the branch that you just created. Later, we will merge those changes back into the main branch.\n\n\n\n\n\n\nExercise 6\n\n\n\nIn the top right hand corner of the Git tab, you will see a dropdown currently set to “main”. Click the downward arrow, and switch to your branch by selecting the appropriate branch.\n\n\n\n\n\nMake Changes\n\n\n\n\n\n\nExercise 7\n\n\n\nOnce in your branch, open github-practice.Rmd from the files pane. Decide within your group who will be Group member 1, 2, 3, and so on. Each group member should edit this file on their own machines by adding their name and only their group name to the appropriate location in the document (line 5, 7, or 9) based on their group member. It’s very important that this be the only section of the document you edit. Save the file by clicking File>Save.\n\n\n\n\nStage\nSometimes we make a changes to a few files, save them, and we’re ready to create a snapshot of our repo (i.e. create a commit) with some of those changes. Remember that creating this snapshot is almost like taking a photo of the repo at this particular moment, allowing us to later go back to that photo to see what the repo looked like in that moment. To let Git know which changes we want to include in that snapshot, we need to stage the files. Staging basically says, “these files are ready to be included in the snapshot.”\n\n\n\n\n\n\nExercise 8\n\n\n\nOnce you save the file you’ll notice in the RStudio Git pane that the file name appears after a blue square labeled “M” (which stands for Modified). This means that the file is ready for staging. Stage the file - indicating that it’s ready for committing - by clicking the checkbox in front of the file name.\n\n\n\n\n\nCommit\nAs we just noted, committing changes basically means taking a snapshot of a repo at a particular moment in time. Commits are given unique hashes - sort of like a unique identifier that enables us to access the snapshot of the repo at a later date. In collaborative projects, it is typically recommended to commit often - after any major changes are made to a file. This ensures that we can eventually go back to look at very specific changes. It’s also important to label commits with descriptive titles so that we can recall what changes within that commit.\nTo help put this into context, think back to our photograph metaphor. Let’s say that we are a photographer assigned to document how a baby develops in the first year of its life. If the photographer only took one photograph when the baby was 1 year old, we wouldn’t have a lot of documentation regarding how the baby developed! …so instead, let’s say that the photographer took a snapshot of the baby after every major milestone - their first laugh, their first solid food, their first crawl, their first word. We would have a lot more to go by when trying to understand how the baby developed. Same goes for committing code often.\nNow let’s say the photographer handed the batch of photos to the parents, and said - “look, here’s how your baby developed over time.” The parents might not remember which photograph was taken after which milestone. …but if the photographer were to label each photograph with things like “baby had first laugh,” the parents would be able to easily go back to specific moments in their baby’s development. This is why we want descriptive commit messages. We want to later be able to go back and scan through what changes were made after each commit.\n\n\n\n\n\n\nExercise 9\n\n\n\nCommit your changes by clicking the ‘Commit’ button in the Git pane. When you click this button, a new window will open showcasing the changes that have been made to the staged file. You should enter a commit message in the window that appears. Remember that commit messages should be descriptive. In this case, something like “added <your-name>’s name” would work. Click commit. Now a snapshot of this version of the code repo has been taken.\n\n\n\n\nPush\nOnly your local machine knows that a change has been made to the code. Remember again how we said that the super fancy git folder knows that there is a remote version of the repository somewhere that you might want to keep it consistent with? Now we want to do the opposite of pulling changes from GitHub to our local machines. Instead, we want to push the changes on our local machines to GitHub.\n\n\n\n\n\n\nExercise 10\n\n\n\nClick the Green upward arrow in the Git pane to push your changes to GitHub.\n\nOnce all group members have pushed their changes, head back over to GitHub. On the main code page, switch between branches and check out the contents of github-practice.Rmd in each branch. What differences do you notice?\n\n\n\n\nPull Request\nNote that now we have a few versions of our repo in separate branches on GitHub.com, and in each of those versions of the repo, the github-practice.Rmd file looks a little bit different. Now that we’ve made our changes and things are stable and polished, we want to move all of those changes into the main branch. To do this, we are going to issue a Pull Request. This is a request that signals to all of our collaborators that we are ready to move our changes back into the main branch.\n\n\n\n\n\n\nExercise 11\n\n\n\nOn your repo’s page in GitHub.com, click the “Pull Requests” tab, and then click the green “New Pull Request” button. You’re requesting to pull the changes from your personal branch into the main branch. This means that the base branch should be main, and the compare branch should be your personal branch.\n\nYou’ll see a screen where you can compare your branch to the main branch. Click the button to “Create Pull Request,” enter a descriptive title of the changes made, and then click “Create Pull Request” again.\n\n\n\n\nReview Pull Requests\nI recommend that you get in the habit of reviewing your collaborator’s work before merging their changes into the main branch. By creating pull requests, we scaffold an opportunity to review each other’s work before fully integrating the changes.\nNow there should be a pull request for all members of your team. Assign one team member to review one other team member’s code. All team members should have one reviewer.\n\n\n\n\n\n\nExercise 12\n\n\n\nOpen your own pull request in GitHub.com, and in the right sidebar, assign the team member responsible for reviewing your changes as a “Reviewer.”\n\nThen navigate to the pull request you are responsible for reviewing. Click on the “Files Changed” tab. Note that the left side of the screen shows the previous version of the file, and the right side of the screen shows the new version of the file. Lines in red have been deleted, and lines in green have been added.\n\nAfter looking through the changes, click the green button “Review changes.” Leave a note for your collaborator, indicating your evaluation of their changes. If everything looks good, check the radio button for “Approve.” If there are issues, check the radio button for “Request Changes.” Then click the button to “Submit Review.”\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf your reviewer requested changes, you should go back to RStudio, and make sure you are in your own branch. Then make the requested changes, save the file, stage the file, commit the changes, and push again. The changes to your file will be tracked in your pull request. After this, you may move on to the Merge step. See here for further options on dismissing or re-requesting reviews.\n\n\n\n\nMerge\nOnce all reviewers have approved changes, we are ready to merge those changes into the main branch. Open each pull request. If everything is good and ready to merge, you will see a green checkmark that says “This branch has no conflicts with the base branch.”\n\n(If you get a message that there are conflicts, call myself or one of the Data Assistants over.)\n\n\n\n\n\n\nExercise 13\n\n\n\nClick the button to “Merge Pull Request”. In the comment box that appears, enter the text “closes #”. When you enter this text, you will see a dropdown of issues and pull requests currently in the repo. Issues will have an icon that appears as a circle with a dot in the center.\n\nSelect the issue associated with this pull request, and then click “Confirm Merge.” This will both merge the changes into the main branch and simultaneously close the issue you opened earlier. Finally, click the button to the delete the branch. Once this has been completed for all pull requests, head back over to the “Code” tab at GitHub.com, and check out github-practice.Rmd. What has happened to the file since merging the code? Navigate to the “Issues” tab. What has happened to the issues since confirming the merge?\n\n\n\n\n\n\n\n\nBefore moving on to the next section...\n\n\n\nYou’ve now deleted branches at GitHub.com that your local machines don’t know have been deleted. Before moving on to the next step, you should navigate back to RStudio and pull these changes to your local machine by clicking the blue downward arrow. To streamline things in the next section of the lab, we are going to work entirely in the main branch (something that I would otherwise not recommend)."
  },
  {
    "objectID": "lab4.html#error-resolution",
    "href": "lab4.html#error-resolution",
    "title": "Lab 4: Git and GitHub",
    "section": "Error Resolution",
    "text": "Error Resolution\nThe workflow presented above seems to work all fine and dandy. …but there are a number of factors that can impede this seamless workflow. In this final section of the lab, we will go over three kinds of errors that you might come across in the workflow above, and talk about how you would resolve them. I can almost guarantee that you will deal with some of these issues when working on your group projects, so I would encourage you to keep this lab handy when engaging in project work.\n\nPush error\nA push error occurs when we make changes to files on our local machines, and go to commit and push them to GitHub.com, but other changes had already been made to the file at GitHub.com that were not yet pulled into our local environments. We get an error because our local repo is inconsistent with the remote repo. To fix this error, we need to pull changes to our local machine, and try committing and pushing again. Let’s replicate this error:\n\n\n\n\n\n\nExercise 14\n\n\n\nOne of your partners should navigate to the GitHub.com repository. Click on the file README.md. Click the pencil icon to edit the file. Replace the text: ADD NAME 1 HERE ADD NAME 2 HERE, and so on with your names. Scroll to the bottom of the page and commit changes noting in the message that your names were added.\nOther partner: Return to RStudio. Do not pull changes yet. Open github-practice.Rmd. On line 40 change the ncol() function to dim(). Save the file. Stage and commit your changes. Click the green upward arrow to push your changes. You should get an error that looks like this.\n\nFollow the steps above to resolve the error.\n\n\nAn easy way to avoid a push error is to always click the blue downward arrow to pull remote changes before starting to edit files on your local machine.\n\n\nPull error\nA pull error occurs when changes have already been made to the same location (the same line number) in both a remote file and a local file, and then we try to pull the changes from the remote repo to our local machines. As far as Git can tell, there are two options for what this line is supposed to look like, and it can’t tell which to prioritize. So Git recommends that, as a first step, we commit the changes that we made locally. It’s basically saying, let’s take a snapshot of your local repo as it looks right now, so that later we can figure out what to do about this conflict.\nIf this seems confusing imagine this: let’s say you write a paper, and you share it with one of your classmates to review. The classmate reads through it, makes suggested changes to the opening sentence, and sends it back you. …but, while your classmate was reviewing the paper, you were getting antsy about the paper deadline and started making your own edits to the paper, including edits to the opening sentence. Now you’re trying to incorporate the changes from your classmate’s review, and you’re not sure what to do about that opening sentence. As a first step, you have two options you can scrap your recent changes (maybe your classmate’s suggestions were better!) or you can save a separate copy of the file with your recent changes and figure out later how to resolve the differences. That’s exactly what we are going to do here:\nTo fix this error, you should stage and commit your local changes and then try pulling again. Let’s replicate this error:\n\n\n\n\n\n\nExercise 15\n\n\n\nOne of your partners should navigate to the GitHub.com repository. Click on the file github-practice.Rmd. Click the pencil icon to edit the file. Replace the code on line 47 with the following: colnames(pioneer_valley_2013).\nScroll to the bottom of the page and commit changes noting in the message how you updated the function.\nOther partner: Return to RStudio. Do not pull changes yet. Open github-practice.Rmd. Replace the code on line 47 with the following: ncol(pioneer_valley_2013)\nSave the file. Click the blue downward button to Pull changes. You should get an error that looks like this.\n\nFollow the steps above to resolve the error.\n\n\n\n\nMerge conflict\nSo now we have these two snapshots of github-practice.Rmd, and they are in conflict with one another. If we try to push our changes back to GitHub.com, Git is not going to know what to do. Should the file at GitHub.com look like the version currently at GitHub.com, should it look like the snapshot that we just commit to our local machines, or should it look like something else entirely?\nLet’s return to the example of trying to incorporate a peer’s edits to a paper that you have recently made changes to. We have to figure out what to do about that opening sentence. Do we want our version, their version, or some combination of the two? This is what it is like to fix a merge conflict.\nTo fix this error, open the file with conflicts and edit the lines with conflict.\n\n\n\n\n\n\nExercise 16\n\n\n\nOne of your partners should try to pull changes by clicking the blue downward arrow in RStudio. You will get an error that looks like this:\n{fig-alt=“This is the window that we see when we get a merge conflict. It says: CONFLICT (content): Merge conflict in README.md}\nTo fix this one your partners should open the file with the conflict. In this case it will be github-practice.Rmd. Scroll to the section of the file with the conflict. It will now look something like this:\n    <<<<<<< HEAD\n\n    ncol(pioneer_valley_2013)\n\n    =======\n\n    colnames(pioneer_valley_2013)\n\n    >>>>>>> ee175895783b64e0e1f696d9456be4c4c7c3f3bf\nThe code following HEAD represents the recent changes you made on your local machine, and the code right before the long string of characters represents the changes that were made in an earlier commit (the long string of characters is the commit hash). Decide what that line should look like and delete all other content. This means you must delete “<<<<<<< HEAD”, “=======”, and “>>>>>>> <long-hash>”, and you likely should delete at least one other line. Save the file, stage the file by clicking the checkbox next to the file in the Git page, and then commit your changes, and push them to GitHub.com.\n\n\n\n\n\n\n\n\nAvoiding Merge Conflicts\n\n\n\nYou may have noticed that the most frustrating merge conflicts tend to emerge when we have two people working on the same line of a repo’s file. The most effective way to avoid merge conflicts is to ensure that collaborators are working on different documents or different lines in a file. One way you might do this when starting to work on your group project is to open a file that you all plan to work on and having one of your team mates section off space of that file for different people to work. It might look something like this:\n\nOnce this change has been made, that group mate should stage, commit, and push the file to GitHub, and all other group mates should pull the change to their local machines.\n\n\n\n\nPath of Least Resistance\nI have been working with GitHub for years, and even to this day, I run into instances where things become so inconsistent between my local machine and the repo at GitHub.com that the fastest way to fix things is just to save local copies of the files that I’ve changed to somewhere else on my machine, delete the super fancy Git folder from its current location, and then re-clone the most up-to-date remote version to my local machine. Then I can figure out how I want to edit the most up-to-date version with my changes. This comic from XKCD captures this widely acknowledged solution beautifully:\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nIncreasingly, when data science researchers publish a paper in a journal, they are making the code they used to reach certain results freely available on GitHub.com for other researchers and the public to review. This is in part a response to the reproducibility crisis that you learned about in SDS 100. What do you see as the social benefits to making the code behind a data science finding publicly available online? What might be some of the social consequences of making this code freely available? How might we mitigate these consequences? Share your ideas on our sds-192-discussions Slack channel."
  },
  {
    "objectID": "lab1-template.html",
    "href": "lab1-template.html",
    "title": "Lab 1: Understanding Datasets",
    "section": "",
    "text": "Install the RScorecard package by entering the following into your Console: install.packages(\"rscorecard\")\nCreate a Scorecard API Key at this link. Shortly after you fill out the form, you will be emailed a key. Copy that key into code chunk below, replacing all of the following text in sc_key(): Sys.getenv(“SCORECARD_KEY”). Be sure to wrap the key in quotation marks.\nDownload the Scorecard Data Dictionary and Technical Documentation for Institution-Level Data Files here.\nRun the code below to the import 2018 Scorecard data for Massachusetts into R. Call me or one of the data assistants over if you get an error.\n\n\nlibrary(tidyverse)\nlibrary(rscorecard)\nsc_key(Sys.getenv(\"SCORECARD_KEY\")) # Replace Sys.getenv(\"SCORECARD_KEY\") here with your API Key in quotation marks\n\nscorecard <- sc_init() %>%\n  sc_year(2018) %>%                 #Note how we are looking at only 2021 data here!\n  sc_filter(stabbr == \"MA\") %>%     #Note how we are looking at only Massachusetts data here!\n  sc_select(unitid, instnm, city, highdeg, control, ugds, adm_rate, costt4_a, costt4_p, pcip27, pctfloan, admcon7, wdraw_orig_yr2_rt, cdr3) %>%\n  sc_get()\n\n\n\n\n\n\n\nExercise 1\n\n\n\nSee if you can identify a unique key for this dataset. Write some lines of code to determine whether the column you’ve identified can act as unique key for the data. Hint: You need to check whether the values in the column ever repeat.\n\n\n\n# Write code to check if values in column are all unique here.\n\n\n\n\n\n\n\nExercise 2\n\n\n\nAt this point, you’ve probably figured out that each row in this dataset is a higher education institution. …but there are many different ways that we can define higher education institutions, and that will impact what gets included and excluded in our data. Referencing the Technical Documentation, locate a definition for the unit of observation in this dataset. What institutions are included, and what institutions are excluded? Summarize a definition below.\n\n\n\n\n\n\n\n\nYour Response\n\n\n\nFill response here.\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\nFor each of the variable names in the scorecard data frame, look up the associated name in the data dictionary. You will need to search for the variable name in the sixth column of the data dictionary (I recommend using Ctrl-F to quickly locate the variable name in the spreadsheet.) Once you’ve found the variable name, reference column 1 to determine what this variable means, and reference columns 7 and 8 to see what possible values will appear in that column.\nIdentify one nominal variable, one ordinal variable, one discrete variable, and one continuous variable in scorecard and list their variable names below. Then uncomment the lines below and use the typeof() function to see how R determined their data types. Did any surprise you?\n\n\n\n\n\n\n\n\nYour Response\n\n\n\n\nNominal variable: _____\nOrdinal variable: _____\nDiscrete variable: _____\nContinuous variable: _____\n\n\n\n\n#typeof(scorecard$_____)\n#typeof(scorecard$_____)\n#typeof(scorecard$_____)\n#typeof(scorecard$_____)\n\n\n\n\n\n\n\nExercise 4\n\n\n\nWrite code below to recode the admcon7 variable and store the results in a new variable in scorecard called admcon7_text. You’ll need to look up the values in the data dictionary. If you’ve done this correctly, running this code should produce a barplot that displays multiple bars.\n\n\n\nscorecard$admcon7_text <-\n  recode(\n    scorecard$admcon7, \n    #Fill replacements here\n    .default = NA_character_\n  )\n\nggplot(scorecard, aes(x = admcon7_text)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\nCalculate the number of missing values in both the costt4_a and the costt4_p columns. Reference the NOTES column in the data dictionary to determine why there are so many NA values in these columns. Add a comment to the code chunk below, explaining the reason for missing values in these columns.\n\n\n\n# Calculate the number of NA values here\n\n# Add comment to explain missing values here"
  },
  {
    "objectID": "lab3-template.html",
    "href": "lab3-template.html",
    "title": "Lab 3: Plotting Frequencies and Distributions",
    "section": "",
    "text": "Tip\n\n\n\nToday’s set-up is a little more complicated than usual, so be sure to take it slow and ask questions as they come up!\n\n\n\nInstall the Spotify R package by entering the following into your Console: install.packages(\"spotifyr\")\nLog-in to Spotify’s Developer Dashboard here. If you have a Spotify account, you can log-in with that account. Otherwise, you should create one.\nClick the ‘Create an App’ button to create an app named “SDS 192 Class Project”. You can indicate that this is a “Project for SDS 192 class”\nClick Edit Settings. Under the heading Redirect URIs copy and paste this URL: http://localhost:1410/, and click Add. Scroll to the bottom of the window and click Save. This is going to allow us to authenticate our Spotify accounts through our local computers.\nClick the Users and Access button. Scroll down to Add New User, and add your name and the email address associated with your Spotify account. Click Add.\nClick “Show Client Secret”. Copy client id and secret below, and then run the code chunk.\n\n\nlibrary(spotifyr)\n# id <- 'FILL CLIENT ID HERE'\n# secret <- 'FILL CLIENT SECRET HERE'\n# Sys.setenv(SPOTIFY_CLIENT_ID = id)\n# Sys.setenv(SPOTIFY_CLIENT_SECRET = secret)\n\n\nBelow replace poiril with your Spotify username. This is the ID that appears in the upper right hand corner when you log into your Spotify account (not your developer account.)\nSearch Spotify for your favorite music genre and select three playlists from the search. Note that this code will only work for playlists, and playlists may be a ways down in the search results.\nWhen you click on a playlist, notice the URL in the navigation bar of your web browser. It should look something like spotify.com/playlist/LONG_STRING_OF_CHARACTERS. Copy the long string of characters at the end of the URL, and paste it into the function below. Your characters should replace the example playlist I’ve added: 7ryj1GwAWYUY36VQd4uXoq.\nRepeat this for the other two playlists, replacing my other examples. Then run the code.\n\n\nlibrary(tidyverse)\nspotify_playlists <- get_playlist_audio_features(\n  username = \"poiril\",\n  playlist_uris = c(\"7ryj1GwAWYUY36VQd4uXoq\",\n                    \"3DYUw0nHB9o8tLZKQup4zp\",\n                    \"37i9dQZF1DXdOEFt9ZX0dh\"),\n  authorization = get_spotify_access_token()\n) %>%\n  select(-c(track.artists, \n            track.available_markets, \n            track.album.artists, \n            track.album.available_markets, \n            track.album.images))\n\n\n\n\n\n\n\nExercise 1\n\n\n\nHow many songs are in each playlist? Create a plot to visualize this, and order the results by the number of songs. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nExercise 2\n\n\n\nWhat is the distribution of valence across all of the songs (in intervals of 0.1 valence)? Create a histogram to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nExercise 3\n\n\n\nWhat is the distribution of valence across all of the songs (in intervals of 0.1 valence) in each playlist? Create a histogram to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nExercise 4\n\n\n\nWhat are differences in the summary statistics (max, min, median, etc.) of the valence of songs in each playlist? Create grouped boxplot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nExercise 5\n\n\n\nDo happier songs tend to be more danceable in each playlist? Create a plot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context. Also be sure to adjust your plot to address overplotting.\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nExercise 6\n\n\n\nDo songs composed in the minor or major mode tend to be happier in each playlist? Create a plot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context.\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nExercise 7\n\n\n\nDo happier songs tend to have a higher tempo across all playlists? What role might the song’s mode play? Create a plot to visualize this. Be sure to give it a descriptive title and labels covering all 5 essential components of data context. Also be sure to adjust your plot to address overplotting.\n\n\n\n#Create boxplot here\n\n\n\n\n\n\n\nExercise 8\n\n\n\nBased on all of your plots, summarize what you learned about the joyfulness of these three playlists below.\n\n\n\n\n\n\n\n\nYour Response\n\n\n\nFill response here."
  },
  {
    "objectID": "problem-solving-template.html",
    "href": "problem-solving-template.html",
    "title": "Problem Solving",
    "section": "",
    "text": "# Create three vectors\na <- 1, 2, 3, 4, 5\nb <- \"a\", \"b\", \"c\", \"d\", \"e\"\nc <- TRUE, FALSE, TRUE, TRUE, FALSE\n\nError: <text>:2:7: unexpected ','\n1: # Create three vectors\n2: a <- 1,\n         ^\n\n\n\n# Add the values in the vector a\na_added <- add(a)\n\nError in add(a): could not find function \"add\"\n\n\n\n# Multiply the previous output by 3\nthree_times_a_added <- added_a * 3\n\nError in eval(expr, envir, enclos): object 'added_a' not found\n\n\n\n# Create a dataframe with col1 and col2\ndf <- data.frame(\n  col1 = c(1, 2, 3)\n  col2 = c(\"a\", \"b\", \"c\")\n\nError: <text>:4:3: unexpected symbol\n3:   col1 = c(1, 2, 3)\n4:   col2\n     ^\n\n\n\n# Add a new column to df\ndf$col3 <- c(TRUE, FALSE)\n\nError in df$col3 <- c(TRUE, FALSE): object of type 'closure' is not subsettable\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nCopy and paste one line of code from this lab as a threaded comment on the Slack message I posted on September 7, 2022. Be sure the text is formatted as code.\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\nAccess the help pages for the function sort(). Write code below to sort the vector a (which you created in an earlier step) in decreasing order.\n\n\n\n# Write code here. \n\n\n\n\n\n\n\nExercise 4\n\n\n\nNavigate to this cheatsheet for base R. Imagine we collected the temperature of our home each day for the past ten days (see code below). Let’s say we wanted to find how each day ranked from coolest to hottest. For instance, I wanted to know the ranking for day 1 vs. day 2 vs. day 3, and so on. Using the cheatsheet, find the function that will allow you to generate a ranking of each day’s temperature. Search the help pages for this function to determine how to randomly rank two days with the same temperature. Write a comment to yourself describing how this function is different than sorting the data.\n\n\n\n#Create a vector of temperatures\ntemps_to_factor <- c(68, 70, 78, 75, 69, 80, 66, 66, 79)\n\n# Write code below to rank the days with random ties\n\n\n# Replace this line with a comment to yourself describing how this function is different than sorting the data. \n\n\n\n\n\n\n\nExercise 5\n\n\n\nAdd a comment to the code chunk above, citing this Stack Overflow post. Be sure to cite the post properly. You can read through the post to double check your answers."
  },
  {
    "objectID": "lab2-template.html",
    "href": "lab2-template.html",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "",
    "text": "Install the RColorBrewer package by entering the following into your R Console: install.packages(\"RColorBrewer\")\nRun the code below to the import the bridge inventory for Massachusetts and for Hampshire County into R. Call me or one of the data assistants over if you get an error.\n\n\nlibrary(tidyverse)\nlibrary(RColorBrewer)\ncounties <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_counties.csv\")\nroute_prefixes <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_route_pre.csv\")\nmaintenance <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_maintenance.csv\")\nkinds <- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_kind.csv\")\n\nnbi_ma <- read.delim(\"https://www.fhwa.dot.gov/bridge/nbi/2022/delimited/MA22.txt\", sep = \",\") %>%\n  left_join(counties) %>%\n  left_join(route_prefixes) %>%\n  left_join(maintenance) %>%\n  left_join(kinds) %>%\n  filter(SERVICE_ON_042A == 1) %>%\n  select(STRUCTURE_NUMBER_008, COUNTY_CODE_003_L, ROUTE_PREFIX_005B_L, MAINTENANCE_021_L, YEAR_BUILT_027, ADT_029, STRUCTURE_KIND_043A_L, STRUCTURAL_EVAL_067, BRIDGE_IMP_COST_094) %>%\n  mutate(STRUCTURE_KIND_043A_L = \n           case_when(\n             STRUCTURE_KIND_043A_L == \"Concrete continuous\" ~ \"Concrete\",\n             STRUCTURE_KIND_043A_L == \"Steel continuous\" ~ \"Steel\",\n             STRUCTURE_KIND_043A_L == \"Prestressed concrete continuous\" ~ \"Prestressed concrete\",\n             TRUE ~ STRUCTURE_KIND_043A_L)) %>%\n  mutate(BRIDGE_IMP_COST_094 = BRIDGE_IMP_COST_094 * 1000)\n\nnbi_hampshire <- nbi_ma %>% filter(COUNTY_CODE_003_L == \"Hampshire\")\n\n\n\n\n\n\n\nExercise 1\n\n\n\nBelow, adjust the size to 2 and the alpha to 0.5, and the position to \"jitter\". Note how this changes the plot.\n\n\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(size = 1, alpha = 1, position = \"identity\") +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nIn the plot below, color the points on the plot with a qualitative palette by setting col= to the categorical variable ROUTE_PREFIX_005B_L. Remember that, in this case, color is an aesthetic, so this must be added inside of the aes() function. Add a label for the legend by setting col= in the labs() function to a phrase that describes the color variable.\n\n\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       col = \"ADD LEGED LABEL HERE\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\nCopy the plot from above below but swap out the variable your mapped to the color aesthetic with a numeric variable from the dataset. Be sure to also adjust the legend label. What kind of palette gets created? What new information do you gain about Hampshire County bridges from the plot?\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nYour Response\n\n\n\nFill response here.\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\nIn the plot below, assign the shape of the data on the plot by setting shape= to the categorical variable STRUCTURE_KIND_043A_L. Remember that, in this case, shape is an aesthetic, so this must be added inside of the aes() function. Add a label for the legend by setting shape= in the labs() function to a phrase that describes the shape variable.\n\n\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       shape = \"ADD LEGED LABEL HERE\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\nCopy one of your plots from above, and inside the aes() function, set size= to a numeric variable not already represented on the plot. Keep in mind that you’ll need to remove the size= attribute from geom_point() for the points to size correctly. Be sure to also adjust the legend labels. What new information do you gain about Hampshire County bridges from the plot?\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nYour Response\n\n\n\nFill response here.\n\n\n\nnbi_ma %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = BRIDGE_IMP_COST_094)) +\n  geom_point(size = 1, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Bridge Improvement Costs\") +\n  theme_minimal() + \n  scale_y_continuous(labels = scales::comma)\n\nWarning: Removed 1 rows containing missing values (geom_point).\n\n\n\n\n\n\n\n\n\n\n\nExercise 6\n\n\n\nCopy the plot that I created above, and change the y-axis scale from continuous to log10. You might reference the formula specified above or reference the ggplot cheatsheet for help. Note that your y-axis will appear in scientific notation. Convert it to comma notation by adding: labels = scales::comma as an argument in the scale function call.\n\n\n\n# Create plot here\n\n\n\n\n\n\n\nExercise 7\n\n\n\nThe Interstate Highway Act of 1956 (signed under Dwight D. Eisenhower) established the U.S.’s interstate highway system and is considered the country’s largest public works project. Adjust the x-axis scale of the plot below to help tell this story. Specifically, bin years into before 1954 (when Eisenhower was elected) and 1954-2021. Set the label at 1954 to “Eisenhower Elected” and the label at 2021 to “Today.”\n\n\n\nnbi_ma %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = ROUTE_PREFIX_005B_L)) +\n  geom_point(size = 1, alpha = 0.2, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Route Prefix\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nExercise 8\n\n\n\nAdd `+ scale_color_brewer()` to the ggplot() object below and assign a categorical palette. Refer to the ggplot cheatsheet or the help pages for help with formatting the function call.\n\n\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067,\n             col = ROUTE_PREFIX_005B_L)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, Massachusetts, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       col = \"Route Prefix\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\n\n\nExercise 9\n\n\n\nAdd `+ scale_color_distiller()` to the ggplot() object below and assign a sequential palette. By default the colors will be ordered from darkest to lightest. Add an argument to the function call to reverse the direction of the colors. Refer to the ggplot cheatsheet or the help pages for help with formatting the function call.\n\n\n\nnbi_hampshire %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067,\n             col = ADT_029)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       col = \"Average Daily Traffic\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nExercise 10\n\n\n\nFacet the plot below by STRUCTURE_KIND_043A_L to see the degree of structural deficiencies for different kinds of highway bridges of different ages in Massachusetts.\n\n\n\nnbi_ma %>%\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(alpha = 0.1, size = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA 2022\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal()"
  },
  {
    "objectID": "lab5-template.html",
    "href": "lab5-template.html",
    "title": "Lab 5: Exploratory Data Analysis",
    "section": "",
    "text": "library(tidyverse)\n\nsqf_url <- \"https://www1.nyc.gov/assets/nypd/downloads/zip/analysis_and_planning/stop-question-frisk/sqf-2011-csv.zip\"\ntemp <- tempfile()\ndownload.file(sqf_url, temp)\nsqf_zip <- unzip(temp, \"2011.csv\")\nsqf_2011 <- read.csv(sqf_zip, stringsAsFactors = FALSE) \nsqf_2011_race_cat <- read.csv(\"https://raw.githubusercontent.com/lindsaypoirier/STS-101/master/Data/SQF/sqf_race_categories.csv\", stringsAsFactors = FALSE) \nrm(sqf_url)\nrm(temp)\nrm(sqf_zip)\n\n\nsqf_2011 <- \n  sqf_2011 %>% \n  select(pct, race, age, frisked, pistol, riflshot, asltweap, knifcuti, machgun, othrweap, sumissue, arstmade) %>%\n  left_join(sqf_2011_race_cat, by = \"race\") %>%\n  mutate(across(frisked:arstmade, \n         ~ case_when(. == \"Y\" ~ 1, . == \"N\" ~ 0)))\nrm(sqf_2011_race_cat)\n\n\n\n\n\n\n\nExercise 1\n\n\n\nAdd new columns indicating 1) whether a weapon was found or 2) an arrest/summons was made.\n\n\n\nsqf_2011 <- \n  sqf_2011 %>%\n  #Add a variable for weapon found\n  _____(wpnfound = case_when(pistol == 1 |\n                               riflshot == 1 | \n                               asltweap == 1 |\n                               knifcuti == 1 | \n                               machgun == 1 | \n                               othrweap == 1 ~ 1,\n                             TRUE ~ 0))\nsqf_2011 <- \n  sqf_2011 %>%\n  #Add a variable for arrest made or summons issued\n  _____(arrestsumm = case_when(sumissue == 1 | \n                                arstmade == 1 ~ 1,\n                               TRUE ~ 0))\n\n\n\n\n\n\n\nExercise 2\n\n\n\nSubset the dataset to the six variables listed in the data dictionary above.\n\n\n\nsqf_2011 <-\n  sqf_2011 %>%\n  _____(pct, arrestsumm, _____, wpnfound, race_cat, _____)\n\n\n\n\n\n\n\nExercise 3\n\n\n\nCalculate the number of stops in 2011. Hint: Keep in mind that every row in the dataset represents one stop.\n\n\n\ntotal_stops <-\n  sqf_2011 %>%\n  summarize(Count = _____) %>%\n  pull()\n\ntotal_stops\n\n\n\n\n\n\n\nExercise 4\n\n\n\nHow many stops did not result in an arrest or summons in 2011? What percentage of stops did not result in an arrest or summons?\n\n\n\nsqf_2011 %>%\n  #Subset to rows where suspect innocent\n  _____(arrestsumm _____ 0) %>% \n  #Calculate number of observations\n  _____(total_innocent = n(), \n            percent_innocent = _____ / total_stops * 100)\n\n\n\n\n\n\n\nExercise 5\n\n\n\nIn how many stops were the individuals aged 14-24? In what percentage of stops were the individuals aged 14-24?\n\n\n\nsqf_2011 %>%\n  #Subset to rows where suspect age 14-24\n  _____(age _____ 14 & age _____ 24) %>% \n  #Calculate number of observations and percentage of observations\n  _____(total_14_24 = _____, \n            percent_14_24 = n() / total_stops * 100)\n\n\ntotal_stops_age_recorded <-\n  sqf_2011 %>%\n  #Subset to rows where age is not 999\n  _____(age _____ 999) %>% \n  summarize(Count = n()) %>%\n  pull()\n\nsqf_2011_sub %>%\n  filter(age >= 14 & age <= 24) %>%\n  summarize(total_14_24 = n(), \n            percent_14_24 = n() / total_stops_age_recorded * 100)\n\n\n\n\n\n\n\nExercise 6\n\n\n\nHow many stops were there per race in 2011? What percentage of stops per race in 2011? Arrange by number of stops in descending order.\n\n\n\ntotal_stops_race_recorded <-\n  sqf_2011 %>%\n  #Subset to rows where race_cat is not NA or \"OTHER\"\n  filter(_____(race_cat) & race_cat _____ \"OTHER\") %>% \n  summarize(Count = n()) %>%\n  pull()\n\nsqf_2011 %>%\n  #Subset to rows where race_cat is not NA or \"OTHER\"\n  _____(_____(race_cat) & race_cat _____ \"OTHER\") %>% \n  #Group by race\n  _____(race_cat) %>% \n  #Calculate number of observations\n  _____(stops = n(), \n            percent_stops = n() / total_stops_race_recorded * 100) %>%\n  #Sort by stops in descending order\n  _____(_____(stops)) \n\n\nsqf_2011 %>%\n  #Subset to rows where race_cat is \"WHITE-HISPANIC\" or \"BLACK-HISPANIC\"\n  _____(race_cat _____ c(\"WHITE-HISPANIC\", \"BLACK-HISPANIC\")) %>% \n  #Calculate number of observations\n  _____(stops_Latinx = _____, \n            percent_Latinx = n() / total_stops_race_recorded * 100)\n\n\n\n\n\n\n\nExercise 7\n\n\n\nWhat percentage of stops in 2011 resulted in a frisk per race? What percentage of stops in 2011 resulted in a weapon found per race? What percentage of stops in 2011 resulted in an arrest or summons per race? In your resulting data table, each row should be a race, and there should be columns for stops, percent_stops, percent_frisked, percent_wpnfound , and percent_arrestsum.\n\n\n\n# Write code here."
  }
]