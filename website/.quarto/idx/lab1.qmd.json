{"title":"Lab 1: Understanding Datasets","markdown":{"yaml":{"title":"Lab 1: Understanding Datasets","author":"Lindsay Poirier","format":"html","editor":"visual"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nThis lab is all about learning to understand the context and parts of a dataset by referencing and interpreting data dictionaries and technical data documentation.\n\n### Learning Goals\n\n-   Read a data dictionary\n\n-   Reference data documentation\n\n-   Identify unique observations in a dataset\n\n-   Understand different variable types\n\n-   Look up value codes and recode a variable\n\n-   Determine the number of missing values in a variable and why they are missing\n\n## Review of Key Terms\n\nRectangular Datasets\n\n:   datasets in which all rows are the same length, and all columns are the same length\n\nObservations\n\n:   rows in a dataset; represent discrete entities we observe in the world\n\nVariables\n\n:   columns in a dataset; describe something about an observation\n\nVector\n\n:   one-dimensional set of values that are all of the same type\n\nData Frame\n\n:   a list of vectors of equal lengths; typically organizes data into a two-dimensional table composed of columns (the vectors) and rows\n\n::: callout-note\nI use the letters `df` as a placeholder to refer to an arbitrary data frame in `R`. Any time that I refer to `df` in a lab, know that you can swap out `df` with any data frame in your environment.\n:::\n\nUnique Key\n\n:   variable (column) in the dataset that can be used to uniquely identify each row\n\nNominal categorical variables\n\n:   variables that identify something else; sometimes, numbers are considered nominal categorical variables (e.g. zip code)\n\nOrdinal categorical variables\n\n:   categorical variables that can be ranked or placed in a particular order (e.g. High, Medium, Low)\n\nDiscrete numeric variables\n\n:   numeric variables that represent something that is countable (e.g. the number of students in a classroom, the number pages in a book)\n\nContinuous numeric variables are variables\n\n:   variables in which it is always possible to measure the value more precisely (e.g. time can be measured with infinite amount of specificity - hours \\> minutes \\> seconds \\> milliseconds \\> microseconds \\> nanoseconds ...)\n\n## Scorecard Dataset\n\nIn his 2013 State of the Union Address, President Barack Obama announced his plans to create a \"college scorecard\" that would allow prospective students and parents to compare schools in terms of cost, offerings, diversity, completion rates, and post-graduate earnings. This data was first published in 2015 and since has undergone several improvements and revisions.\n\nThe College Scorecard dataset is *massive*. In fact, I thought long and hard about whether this was really the first dataset I wanted to introduce to you in a lab. It includes information about over 6500 institutions in the U.S., and has more than 3000 columns documenting information about those institutions. I chose this dataset for this lab because, if you can learn to read this data dictionary, you will be leaps and bounds ahead of the game in learning to read other data dictionaries. (It's also just a super cool dataset, and hint, hint: you will get a chance to dive into it in much more detail in a few weeks). While the full data is available online, we are only going to work with a small subset of the data today.\n\n## Setting Up Your Environment\n\n1.  Install the RScorecard package by entering the following into your Console: `install.packages(\"rscorecard\")`\n\n2.  Create a Scorecard API Key at [this link](https://api.data.gov/signup/). Shortly after you fill out the form, you will be emailed a key. Copy that key into code chunk below, replacing all of the following text in `sc_key()`: Sys.getenv(\"SCORECARD_KEY\"). Be sure to wrap the key in quotation marks.\n\n3.  Download the Scorecard Data Dictionary and Technical Documentation for Institution-Level Data Files [here](https://collegescorecard.ed.gov/data/documentation/).\n\n4.  Run the code below to the import 2018 Scorecard data for Massachusetts into `R`. Call me or one of the data assistants over if you get an error.\n\n```{r import}\n#| warning: false\n#| messages: false\nlibrary(tidyverse)\nlibrary(rscorecard)\nsc_key(Sys.getenv(\"SCORECARD_KEY\")) # Replace Sys.getenv(\"SCORECARD_KEY\") here with your API Key in quotation marks\n\nscorecard <- sc_init() %>%\n  sc_year(2018) %>%                 #Note how we are looking at only 2021 data here!\n  sc_filter(stabbr == \"MA\") %>%     #Note how we are looking at only Massachusetts data here!\n  sc_select(unitid, instnm, city, highdeg, control, ugds, adm_rate, costt4_a, costt4_p, pcip27, pctfloan, admcon7, wdraw_orig_yr2_rt, cdr3) %>%\n  sc_get()\n```\n\n## Glimpsing the Data\n\nWhen working with very large datasets, we need tools to help us get a sense of the dataset without having to load the entire data frame. For instance, we can view the first 6 rows of the dataset by calling `head()`.\n\n```{r}\nscorecard %>% head()\n```\n\n`str()` provides a great deal of information about the observations in the data frame, including the number of variables, the number of observations, the column names, their data types, and a list of observations.\n\n```{r}\nscorecard %>% str()\n```\n\nYou can also click on the name of your data frame in your Environment panel in RStudio, and it will open a new tab in RStudio that displays the data in a tabular format. Try clicking on `scorecard` in your Environment panel.\n\n::: callout-tip\nThis is the same as calling `df %>% View()` in your Console.\n:::\n\n## Getting to Know this Dataset\n\n### Observations (Rows)\n\nIn starting our data analysis, we need to have a good sense of what each observation in our dataset refers to - or its *observational unit*. Think of it this way. If you were to count the number rows in your dataset, what would that number refer to? A *unique key* is a variable (or set of variables) that uniquely identifies an observation in the dataset. Think of a unique key as a unique way to identify a row and all of the values in it. There should never be more than one row in the dataset with the same unique key. A unique key tells us what each row in the dataset refers to.\n\n::: callout-important\n## Exercise 1\n\nSee if you can identify a unique key for this dataset. Write some lines of code to determine whether the column you've identified can act as unique key for the data. Hint: You need to check whether the values in the column ever repeat.\n:::\n\n```{r}\n# Write code to check if values in column are all unique here.\n```\n\n::: callout-tip\nNote that NAME is typically not an appropriate variable to use as a unique key. Let me provide an example to demonstrate this. When I worked for BetaNYC, I was trying to build a map of vacant storefronts in NYC by mapping all commercially zoned properties in the city, and then filtering out those properties where a business was licensed or permitted. This way the map would only include properties where there wasn't a business operating. One set of businesses I was filtering out was restaurants. The only dataset that the city had made publicly available for restaurant permits was broken. It was operating on an automated process to update whenever there was a change in the permit; however, whenever a permit was updated, rather than updating the appropriate fields in the existing dataset, it was creating a new row in the dataset that only included the permit holder (the restaurant name), the permit type, and the updated fields. Notably the unique permit ID was not being included in this new row. We pointed this issue out to city officials, but fixing something like this can be slow and time-consuming, so in the meantime, we looked into whether we could clean the data ourselves by aggregating the rows that referred to the same restaurant. However, without the permit ID it was impossible to uniquely identify the restaurants in the dataset. Sure, we had the restaurant name, but do you know how many Wendy's there are in NYC?\n:::\n\nAnytime we count something in the world, we are not only engaging in a process of tabulation; we are also engaging in a process of defining. If I count the number of students in a class, I first have to define what counts as a student. If someone is auditing the class, do they count? If I, as the instructor, am learning from my students, do I count myself as a student? As I make decisions about how I'm going to define \"student,\" those decisions impact the numbers that I produce. When I change my definition of \"student,\" how I go about tabulating students also changes. Thus, as we prepare to count observations in a dataset, it is important to know how those observations are defined.\n\n::: callout-important\n## Exercise 2\n\nAt this point, you've probably figured out that each row in this dataset is a higher education institution. ...but there are many different ways that we can define higher education institutions, and that will impact what gets included and excluded in our data. Referencing the Technical Documentation, locate a definition for the unit of observation in this dataset. What institutions are included, and what institutions are excluded? Summarize a definition below.\n:::\n\n::: {.callout-note icon=\"false\"}\n#### Your Response\n\nFill response here.\n:::\n\n### Variables (Columns)\n\nNote the column names for this dataframe, and the kinds of values that appear in those columns. Some of them (like `city` and `year`) might make sense to you immediately. Others (like `pcip27` and `highdeg`) might be much more confusing. To figure out what we are looking out, we are going to need to refer to the dataset's data dictionary.\n\nOpen the data dictionary you downloaded in an earlier step. It will open as an Excel file. Click on the tab labeled \"Institution_Data_Dictionary\". There are thousands of variables in this dataset, falling into the broader categories of school, completion, admissions, cost, etc. Note how the file is organized, and specifically draw your attention to:\n\n-   Column 1 (NAME OF DATA ELEMENT): This is a long description of the variable and gives you clues as to what is represented in it.\n\n-   Column 6 (VARIABLE NAME): This is the column name for the variable. This is how you will reference the variable in `R`.\n\n-   Column 7 (VALUE): These are the possible values for that variable. Note that for many categorical variables, the values are numbers. We are going to have to associate the numbers with their corresponding labels.\n\n-   Column 8 (LABEL): These are the labels associated with the values recorded for the variable.\n\n-   Column 11 (NOTES): This provides notes about the variable, including whether it is currently in use and what missing values indicate.\n\n::: callout-important\n## Exercise 3\n\nFor each of the variable names in the `scorecard` data frame, look up the associated name in the data dictionary. You will need to search for the variable name in the sixth column of the data dictionary (I recommend using Ctrl-F to quickly locate the variable name in the spreadsheet.) Once you've found the variable name, reference column 1 to determine what this variable means, and reference columns 7 and 8 to see what possible values will appear in that column.\n\nIdentify one nominal variable, one ordinal variable, one discrete variable, and one continuous variable in `scorecard` and list their variable names below. Then uncomment the lines below and use the `typeof()` function to see how `R` determined their data types. Did any surprise you?\n:::\n\n::: {.callout-note icon=\"false\"}\n## Your Response\n\n-   Nominal variable: \\_\\_\\_\\_\\_\n\n-   Ordinal variable: \\_\\_\\_\\_\\_\n\n-   Discrete variable: \\_\\_\\_\\_\\_\n\n-   Continuous variable: \\_\\_\\_\\_\\_\n:::\n\n```{r}\n#typeof(scorecard$_____)\n#typeof(scorecard$_____)\n#typeof(scorecard$_____)\n#typeof(scorecard$_____)\n```\n\n### Values (Cells)\n\nYou may have noticed that several categorical variables are coded as numbers in the imported dataset. For instance, look at the column `control` which designates the institution's ownership. Running the code below, we see that the distinct values in that column are 1, 2, and 3.\n\n```{r}\nscorecard %>% distinct(control)\n```\n\nWhen we reference that column in the data dictionary (row 27), we see that a 1 in that column designates that the institution is Public, a 2 that the institution is Private nonprofit, and a 3 that the institution is Private for-profit. While I can always look that up, sometimes it is helpful to have that information in our dataset. For instance, let's say I create a bar plot that's supposed to show how many higher education institutions have each type of ownership in MA (which you will learn how to do soon!). The plot can be confusing when `control` is a series of numbers.\n\n```{r}\nggplot(scorecard, aes(x = control)) +\n  geom_bar()\n```\n\nWith this in mind, sometimes it can be helpful to *recode* the values in a column. Recoding data involves replacing the values in a vector according to criteria that we provide. Remember how all columns in a data frame are technically vectors? We can use the `recode()` function to recode all of the values in the `control` vector. We are going to store the recoded values in a new column in our dataset called `control_text`. Check out the code below to see how we do this. Reference the help pages for recode (i.e. `?recode`) to help you interpret the code.\n\n```{r}\nscorecard$control_text <-\n  recode(\n    scorecard$control, \n    \"1\" = \"Public\", \n    \"2\" = \"Private nonprofit\", \n    \"3\" = \"Private for-profit\",\n    .default = NA_character_\n  )\n```\n\nCheck out our barplot now!\n\n```{r}\nggplot(scorecard, aes(x = control_text)) +\n  geom_bar()\n```\n\n::: callout-important\n## Exercise 4\n\nWrite code below to recode the `admcon7` variable and store the results in a new variable in `scorecard` called `admcon7_text`. You'll need to look up the values in the data dictionary. If you've done this correctly, running this code should produce a barplot that displays multiple bars.\n:::\n\n```{r}\nscorecard$admcon7_text <-\n  recode(\n    scorecard$admcon7, \n    #Fill replacements here\n    .default = NA_character_\n  )\n\nggplot(scorecard, aes(x = admcon7_text)) +\n  geom_bar()\n```\n\n### Missing Values\n\nWhen we have missing values in a rectangular dataset, we have to provide a placeholder for the missing value in order for the dataset to remain rectangular. If we just skipped the value, then our dataset wouldn't necessarily have rows of all equal lengths and columns of all equal lengths. In `R`, `NA` serves as that placeholder. Before we start analyzing data, it can be important to note how many `NA` values we have in a column so that we can determine if the data is *representative*.\n\nThe function `is.na()` checks whether a value is an `NA` value and returns `TRUE` if it is and `FALSE` if it isn't. Providing a vector to `is.na()` will check this for every value in the vector and return a logical vector indicating `TRUE`/`FALSE` for every original value in the vector.\n\n```{r}\nis.na(scorecard$wdraw_orig_yr2_rt)\n```\n\nWhen we `sum()` across a logical vector, `R` will calculate the number of `TRUE` values in the vector.\n\n```{r}\nsum(is.na(scorecard$wdraw_orig_yr2_rt))\n```\n\n::: callout-important\n## Exercise 5\n\nCalculate the number of missing values in both the `costt4_a` and the `costt4_p` columns. Reference the NOTES column in the data dictionary to determine why there are so many `NA` values in these columns. Add a comment to the code chunk below, explaining the reason for missing values in these columns.\n:::\n\n```{r}\n# Calculate the number of NA values here\n\n# Add comment to explain missing values here\n```\n\n::: callout-caution\n## Ethical Considerations\n\nReferencing the College Scorecard data documentation, see if you can determine which students are included in calculations of earnings and debt. How might the data's coverage bias the values that get reported? What might be the social consequences of these biases? Share your ideas on our `sds-192-discussions` Slack channel.\n:::\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"lab1.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"0.9.342","theme":"flatly","toc_float":{"collapsed":true,"smooth_scroll":false},"highlight":"tango","lib_dir":"libs","title":"Lab 1: Understanding Datasets","author":"Lindsay Poirier","editor":"visual"},"extensions":{"book":{"multiFile":true}}}}}